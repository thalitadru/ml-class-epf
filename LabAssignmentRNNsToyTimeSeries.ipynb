{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LabAssignmentRNNsToyTimeSeries.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNjHAU3uc5GGNHGdzeEil3I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thalitadru/ml-class-epf/blob/main/LabAssignmentRNNsToyTimeSeries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdFq6io3oWcH"
      },
      "source": [
        "# Processing Sequences Using RNNs (and CNNs)\n",
        "\n",
        "*Credits:* Based on code written by [A. GÃ©ron](https://colab.research.google.com/github/ageron/handson-ml2/blob/master/15_processing_sequences_using_rnns_and_cnns.ipynb#scrollTo=AiINDLJHVNep) for his \"Hands-on ML\" book. Code realeased under [Apache 2.0 license](https://github.com/ageron/handson-ml2/blob/master/LICENSE)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbgX6-aMHanl"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyooZ0K4pCpC"
      },
      "source": [
        "In this activity we are going to explore different ways of modelling sequential data. We will try out simple baseline models as well as 1D CNNs and different RNNs to **forecast the next step in a time series**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR4QqhoDihkx"
      },
      "source": [
        "## Generating sample data\n",
        "\n",
        "To experiment with different models, we'll use some randomly generated univariate time series.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_bQfrh_PGdz"
      },
      "source": [
        "def generate_time_series(batch_size, n_steps):\n",
        "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
        "    time = np.linspace(0, 1, n_steps)\n",
        "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  #   wave 1\n",
        "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2\n",
        "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # + noise\n",
        "    return series[..., np.newaxis].astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSZCqVimq9Sz"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jALDZroTkupK"
      },
      "source": [
        "Notice the shape of the series array:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj6Fw0yjrJ4_"
      },
      "source": [
        "series.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd9b9zf7p9Xt"
      },
      "source": [
        "The meaning of each dimension is `[samples, time steps, sequence element size]`. \n",
        "Here each series element is a single scalar because we are making a univariate prediction.\n",
        "If we were modeling mutiple time series at once (eg. temperature and humidity) this would be a multi-variate prediction and our sequence element size would be 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWUj5-Zaqi1D"
      },
      "source": [
        "### Creating predicion targets\n",
        "We want to predict one step further into the series. To do so , we can take the last time step of a series as the regression target, while using all the previous time steps as inputs.\n",
        "\n",
        "We also want to split our data into train, validation and test sets as usual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjSOnWaRVNer"
      },
      "source": [
        "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
        "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
        "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YItC9egeVNer"
      },
      "source": [
        "X_train.shape, y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjIjQ4W23MZ-"
      },
      "source": [
        "### Visualizing the data\n",
        "This function plots the first series samples from a given batch, along with the next predicted value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y1Wg-eXVNer"
      },
      "source": [
        "def plot_series(series, y=None, y_pred=None, x_label=\"$t$\", y_label=\"$x(t)$\", legend=True):\n",
        "    plt.plot(series, \".-\")\n",
        "    if y is not None:\n",
        "        plt.plot(n_steps, y, \"bo\", label=\"Target\")\n",
        "    if y_pred is not None:\n",
        "        plt.plot(n_steps, y_pred, \"rx\", markersize=10, label=\"Prediction\")\n",
        "    plt.grid(True)\n",
        "    if x_label:\n",
        "        plt.xlabel(x_label, fontsize=16)\n",
        "    if y_label:\n",
        "        plt.ylabel(y_label, fontsize=16, rotation=0)\n",
        "    plt.hlines(0, 0, 100, linewidth=1)\n",
        "    plt.axis([0, n_steps + 1, -1, 1])\n",
        "    if legend and (y or y_pred):\n",
        "        plt.legend(fontsize=14, loc=\"upper left\")\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3, sharey=True, figsize=(12, 4))\n",
        "for col in range(3):\n",
        "    plt.sca(axes[col])\n",
        "    plot_series(X_valid[col, :, 0], y_valid[col, 0],\n",
        "                y_label=(\"$x(t)$\" if col==0 else None),\n",
        "                legend=(col == 0))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78-2AbhOsvrX"
      },
      "source": [
        "## Prediction task 1 : forecast one time step\n",
        "\n",
        "We want to predict a single continuous value for each input sequence. This can be seen as a sequence-to-vector regression task. As seen in class, a suitable loss function is the `keras.losses.mean_squared_error` (as derived by MLE under a Gaussian assumption on the prediciton targets).\n",
        "\n",
        "\n",
        "As we try out different models, keep the models and their validation performances. In the end you will compare their performances in validation to pick the best model (which will be applied to the test set)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq8h_fRJu2cb"
      },
      "source": [
        "\n",
        "### Simple baseline models\n",
        "\n",
        "Before trying complex models, it is important to set up a baseline performance obtained with a simple model. This way we will only be interested in the more complex models if they do better than our baseline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH8ELltMrrmg"
      },
      "source": [
        "#### Baseline 1: Naive baseline\n",
        "\n",
        "Our first baseline will follow a very simple rule: predict the series will not change. That means the output will be a repetition of the series value at the last observed time step.\n",
        "\n",
        "This is very simple to implement: our predictions `y_pred` will simply be equal to the last time step in the `X` array.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRxCh_Cfwf9K"
      },
      "source": [
        "\n",
        "##### TODO: write the naive baseline code bellow\n",
        "Compute the naive predctions on the validation set and save them to a `y_pred` array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0F_BH9uqUTL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7EtFfee3k3H"
      },
      "source": [
        "Here is the first series in the validation set with its predicted value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqrIG3bXVNet"
      },
      "source": [
        "plot_series(X_valid[0, :, 0], y_valid[0, 0], y_pred[0, 0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJJoJTwqsiCr"
      },
      "source": [
        "#### Baseline 2: linear baseline\n",
        "\n",
        "Our second basline model will be a linear regression. We can implement it using keras `Sequential` API by simply using a `Dense` layer with no activation functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO4mpDFUwhiw"
      },
      "source": [
        "\n",
        "##### TODO: write a linear regression model using a Dense layer\n",
        "- Don't forget to use `Flatten` and define the expected input shape using the keyword argument `input_shape`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UoHY76NuIps"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c5GO6XVuJR_"
      },
      "source": [
        "This model has parameters to train: we will call `fit` to learn them.\n",
        "\n",
        "##### TODO: train the linear regression model\n",
        " - Use the Adam optimizer with the default learning rate\n",
        " - Train it it for 20 epochs\n",
        " - Don't forget to include your validation data\n",
        " - Check the learning curves and the performance on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIps0Re0uxeL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUtD75qcxWNj"
      },
      "source": [
        "### Recurrent models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7lXYg0luxzb"
      },
      "source": [
        "#### Model 1: Simple RNN\n",
        "Our first recurrent model will use simple recurrent units. You can implement it using the built-in `keras.layers.SimpleRNN`.\n",
        "\n",
        "Here again we will specify the expected input shape (ignoring the batch dimension). We want the time-steps dimension to be variable: thus we set it to `None`. The sequence element dimension continues to be 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD0qhLwLvfAL"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQrlBTNQvnnj"
      },
      "source": [
        "##### TODO: train the simple RNN model\n",
        " - Use the Adam optimizer with the a learning rate of 1e-5\n",
        " - Train it it for 20 epochs\n",
        " - Don't forget to include your validation data\n",
        " - Check the learning curves and the performance on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNEebRjExJbL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EEjkFk-yKGK"
      },
      "source": [
        "#### Model 2: Deep simple RNN\n",
        "This time we will use multiple recurrent layers in our model. The final output will be computed with a dense layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YTNDegf3rTt"
      },
      "source": [
        "##### Chaining two RNN layers\n",
        "Notice that while recurrent layers expect sequence inputs with 3 dimensions (samples, time, element size), they output by default 2 D data as in `[samples, predicted element size]`. \n",
        "\n",
        "If you chain two RNN layers, the first one needs yield sequence-shaped outputs (with the 3 dimensions) so that it is compatible with the second layer. In this case, the layer needs to be declared with the keywork argument `return_sequences=True`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doNNziYp0F6F"
      },
      "source": [
        "##### TODO: Create a Deep RNN with 2 hidden recurrent layers\n",
        "- Use two simple RNN layers with 20 units\n",
        "- Compute the final output doing a linear read-out (use a Dense layer with no activation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32b1oHTQytQ1"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuJzOArd0dzQ"
      },
      "source": [
        "##### TODO: train the Deep RNN model\n",
        " - Use the Adam optimizer with the default learning rate\n",
        " - Train it it for 20 epochs\n",
        " - Don't forget to include your validation data\n",
        " - Check the learning curves and the performance on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RxDc74T0mRL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPpWzXdw1OuQ"
      },
      "source": [
        "### TODO: Final comparison and model choice\n",
        "Compare the compare performances of different models in validation to pick the best model. \n",
        "Apply it to the test set and evaluate it: is the performance close to what you got in the validation set?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0kJDf-a1fNf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7_2OCdW2HHR"
      },
      "source": [
        "## Prediction task 2: forecast multiple time steps\n",
        "\n",
        "This task can be formulated as a sequence-to-sequence task if we predict one output timstep at a time. But we can also formulate it as a sequence-to-vector task, in which we try to predict all time steps at once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0Rpk0d023_f"
      },
      "source": [
        "### Method 1: Predicting 1 step at a time (seq2vec)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfNZx94JFCjK"
      },
      "source": [
        "We'll use the previous model to predict the next 10 values. We first need to regenerate the sequences with 9 more time steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdEYgOTyFCjL"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 10)\n",
        "X_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\n",
        "X_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]\n",
        "X_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPa-XFggVNey"
      },
      "source": [
        "Now let's predict the next 10 values one by one. We will keep using tha previous output as new input until we predict 10 values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOYi2JpgVNey"
      },
      "source": [
        "X = X_valid\n",
        "for step_ahead in range(10):\n",
        "    y_pred_one = model.predict(X)[:, np.newaxis, :]\n",
        "    X = np.concatenate([X, y_pred_one], axis=1)\n",
        "\n",
        "Y_pred = X[:, n_steps:, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfaioULUVNez"
      },
      "source": [
        "Y_pred.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oENU05Zzxqf5"
      },
      "source": [
        "#### TODO: check the loss value (MSE) for this method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98U87VyPETqC"
      },
      "source": [
        "#### Plot multiple forecasts\n",
        "Use this plotting function to visualize your input series and the output predictions.\n",
        "\n",
        "**Attention**: Reshape the outputs of your models acordingly:\n",
        "If you model outputs 2D arrays with shape [batch size, time steps], you can use `Y[..., np.newaxis]` or `np.expand_dims(Y, shape=-1) ` to make them compatible with this function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4omE9niVVNey"
      },
      "source": [
        "def plot_multiple_forecasts(X, Y, Y_pred):\n",
        "    # X should be a rank 3 array with shapes [batch size, sequence length, 1]\n",
        "    # Y and y_pred should also be rank 3 with shape [0, sequence lenght, 1]\n",
        "    n_steps = X.shape[1]\n",
        "    ahead = Y.shape[1]\n",
        "    plot_series(X[0, :, 0])\n",
        "    plt.plot(np.arange(n_steps, n_steps + ahead), Y[0, :, 0], \"bo-\", label=\"Actual\")\n",
        "    plt.plot(np.arange(n_steps, n_steps + ahead), Y_pred[0, :, 0], \"rx-\", label=\"Forecast\", markersize=10)\n",
        "    plt.axis([0, n_steps + ahead, -1, 1])\n",
        "    plt.legend(fontsize=14)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2H51keT2osf"
      },
      "source": [
        "#### TODO: Baselines: naive and linear prediction\n",
        "\n",
        "Implement the naive and the linear baselines folowing this method to predict the next 10 steps. Compare their performances to the previous RNN model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyuiOUFx4gvf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzFdKKHG_2fn"
      },
      "source": [
        "### Method 2A: Predicting 10 steps at once at the end (seq2vec)\n",
        "\n",
        "In this section we wil predict all 10 steps at once as a vector output at the end of the input sequence processing. Intermediate network outputs will be ignored."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rdtEMSv4y17"
      },
      "source": [
        "#### Model 1: Deep simple RNN with output prediction at the end\n",
        "\n",
        "Let's create an RNN that predicts all 10 next values at once. To do that, all you need is to change the size of the final `Dense` layer to 10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWRGy4zhVNez"
      },
      "source": [
        "##### TODO: build the new RNN model with output size = 10\n",
        "- Use two simple RNN layers with 20 units\n",
        "- Compute the final output doing a linear read-out (use a Dense layer with no activation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYn3ggzg41dV"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63luki0pBm6g"
      },
      "source": [
        "##### TODO: train the Deep RNN model\n",
        " - Use the Adam optimizer with the default learning rate\n",
        " - Train it it for 20 epochs\n",
        " - Don't forget to include your validation data\n",
        " - Check the learning curves and the performance on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJgMX2LE9va9"
      },
      "source": [
        "# Training the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-QP6QRpAOTx"
      },
      "source": [
        "### Method 2B: Predicting 10 steps at once at every time step (seq2seq)\n",
        "\n",
        "Now let's create RNN models that predict the next 10 steps at each time step. \n",
        "\n",
        "This means that at t=0, the model predicts a vector with time steps t=1 to 1=10. Then as t=1, it predcits another vector with t=2 to t=11, and so on, until the end of the series. \n",
        "\n",
        "A crucial difference here is that for each time step we expect a prediction, while before we only cared about the last output. This implies that the loss will contain a term for each time step, not just the last. This way, during backpropagation, each time-step will contribute with a gradient term flowing back, not only though time but also within the same time step. In practice, this is likely to stabilize AND speed-up training.\n",
        "\n",
        "\n",
        "**Note:** In this model and all others used here, predictions for time $T$ only take into account past series values, i.e. where $t < T$. This defines them as a *causal* models. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxWM2UTTGnQb"
      },
      "source": [
        "#### Generating appropriate data and prediciton targets\n",
        "\n",
        "For this method we'll \"collect\" more data, generating longer series with extra 10 steps.\n",
        "\n",
        "We also need to generate the corresponting target array, which should contain 10-D vectors for all time steps $t>0$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jqkemjPVNe0"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "n_steps = 50\n",
        "# generate data with extra 10 steps\n",
        "series = generate_time_series(10000, n_steps + 10)\n",
        "\n",
        "# prep input arrays\n",
        "X_train = series[:7000, :n_steps]\n",
        "X_valid = series[7000:9000, :n_steps]\n",
        "X_test = series[9000:, :n_steps]\n",
        "\n",
        "# generate target arrays using the last 10 steps\n",
        "Y = np.empty((10000, n_steps, 10))\n",
        "for step_ahead in range(1, 10 + 1):\n",
        "    Y[..., step_ahead - 1] = series[..., step_ahead:step_ahead + n_steps, 0]\n",
        "Y_train = Y[:7000]\n",
        "Y_valid = Y[7000:9000]\n",
        "Y_test = Y[9000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bfUkIg7VNe0"
      },
      "source": [
        "X_train.shape, Y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EKy30KcChpp"
      },
      "source": [
        "#### Custom loss function\n",
        "\n",
        "All outputs are needed during training, but only the output at the last time step is useful at inference time for predictions and evaluation.\n",
        "So eve though we will rely on the MSE over all the outputs for training, we will use a custom metric for evaluation, to compute MSE only over the output at the last time step:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5xQ6L8yDFIz"
      },
      "source": [
        "def last_time_step_mse(Y_true, Y_pred):\n",
        "    return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVqbmKMrAOTy"
      },
      "source": [
        "#### TODO Model 2: Deep simple RNN with output prediction at every timestep\n",
        "- Use two simple RNN layers with 20 units\n",
        "- Compute the final output doing a linear read-out (use a Dense layer with no activation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbzEv7m6DHiW"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdyAq1bSKLKX"
      },
      "source": [
        "##### TODO: train the Deep RNN model\n",
        " - Use the Adam optimizer with the default learning rate\n",
        " - Train it it for 20 epochs\n",
        " - Don't forget to include your validation data\n",
        " - Check the learning curves and the performance on the validation set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psYQ8rPe43_s"
      },
      "source": [
        "#### TODO Model 3: Deep LSTM with output prediction at every timestep\n",
        "\n",
        "- Use two LSTM layers with 20 units\n",
        "- Compute the final output doing a linear read-out (use a Dense layer with no activation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUjDSmje5ktJ"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KZhGpAjBo5F"
      },
      "source": [
        "##### TODO: train the Deep RNN model\n",
        " - Use the Adam optimizer with the default learning rate\n",
        " - Train it it for 20 epochs\n",
        " - Don't forget to include your validation data\n",
        " - Check the learning curves and the performance on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd_1bYjZ9zns"
      },
      "source": [
        "# Training the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cY2whzZD5l47"
      },
      "source": [
        "#### TODO Model 4: Deep GRU with output prediction at every timestep\n",
        "- Use two GRU layers with 20 units\n",
        "- Compute the final output doing a linear read-out (use a Dense layer with no activation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b6nqVxO5lX1"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2PXnUNeBptl"
      },
      "source": [
        "##### TODO: train the Deep RNN model\n",
        " - Use the Adam optimizer with the default learning rate\n",
        " - Train it it for 20 epochs\n",
        " - Don't forget to include your validation data\n",
        " - Check the learning curves and the performance on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezffRGX290Qr"
      },
      "source": [
        "# Training the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUm5-9itHWX0"
      },
      "source": [
        "### TODO: Model comparison for prediction task 2\n",
        "\n",
        "Create a summary table with the validation performaces of all models used in this secont task. Choose the best of them and evaluate it on the test set. Did it generalize as well as predicted by the validation score?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UC2w7btHTep"
      },
      "source": [
        "## Extra models\n",
        "\n",
        "Sequences can be treated with convolutional layers as we'll see in class soon. Here are two example models applied to the time series data we have used in this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bb3-HpgkIRBJ"
      },
      "source": [
        "#### Generating appropriate data and prediciton targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb_8S82_IRBK"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 10)\n",
        "X_train = series[:7000, :n_steps]\n",
        "X_valid = series[7000:9000, :n_steps]\n",
        "X_test = series[9000:, :n_steps]\n",
        "Y = np.empty((10000, n_steps, 10))\n",
        "for step_ahead in range(1, 10 + 1):\n",
        "    Y[..., step_ahead - 1] = series[..., step_ahead:step_ahead + n_steps, 0]\n",
        "Y_train = Y[:7000]\n",
        "Y_valid = Y[7000:9000]\n",
        "Y_test = Y[9000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUviOPLDIRBL"
      },
      "source": [
        "X_train.shape, Y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfVfbrac8N-F"
      },
      "source": [
        "#### Model 5: 1D convolution + GRUs\n",
        "**Attention** Due to using a convolution stride of 2, the convolutional layer subsamples the input series by a factor of 2. That means that the predicted outputs will be related to the prediction targets with a similar subsampling factor.\n",
        "\n",
        "Additionally, the kernel size of 4 in combination with using a valid convolution makes that the first predicted output corresponds to the 4th time-steps.\n",
        "\n",
        "This means you should compare your predictions to a cropped and subsampled version of the targets array, as follows:\n",
        "```\n",
        "subsampled_target = target[:, 3::2]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ9VYQJ28Nl0"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv1D(filters=20, kernel_size=4, strides=2, padding=\"valid\",\n",
        "                        input_shape=[None, 1]),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3JoEZ5DBrtO"
      },
      "source": [
        "##### TODO: train the conv + GRU model\n",
        " - Use the Adam optimizer with the default learning rate\n",
        " - Train it it for 20 epochs\n",
        " - Don't forget to include your validation data\n",
        " - Check the learning curves and the performance on the validation set\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dcu6mqzBsfT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Tiilsvt8UD8"
      },
      "source": [
        "#### Model 6: WaveNet-like model\n",
        "![Wavenet diagram](http://benanne.github.io/images/wavenet.png)\n",
        "[From Oord et al., 2016](https://arxiv.org/abs/1609.03499)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_ZlhU-mVNe5"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.InputLayer(input_shape=[None, 1]))\n",
        "for rate in (1, 2, 4, 8) * 2:\n",
        "    model.add(keras.layers.Conv1D(filters=20, kernel_size=2, padding=\"causal\",\n",
        "                                  activation=\"relu\", dilation_rate=rate))\n",
        "model.add(keras.layers.Conv1D(filters=10, kernel_size=1))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUAOLoDdIIxt"
      },
      "source": [
        "##### TODO: train the WaveNet model\n",
        " - Use the Adam optimizer with the default learning rate\n",
        " - Train it it for 20 epochs\n",
        " - Don't forget to include your validation data\n",
        " - Check the learning curves and the performance on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox-NYduk8Ul2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}