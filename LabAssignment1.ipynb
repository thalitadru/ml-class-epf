{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6a42fbd-3785-4090-ab90-04970d6e7152",
   "metadata": {},
   "source": [
    "# Lab assignment 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf49a68b-7e00-4f52-8c48-10712fe55072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.width\", 500)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.notebook_repr_html\", True)\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298fbcc6-7551-4d11-9077-addbc6130d17",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Iris dataset\n",
    "\n",
    "We are going to use the clasical Iris dataset do demonstrate the influence of different design choices on learned weight matrix norm and the resulting decision function. We restrain these experiments to use 2 features only in order to allow 2D visualisation of the decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e27d3c-7d53-43bb-8af4-85610dc7013d",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d938065-145a-42cb-b53c-668ee004d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4948a9-8e6e-41d7-87d8-4690ec8dee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601fd5b9-2cfb-4391-b0ae-d241eacc23f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228011f0-9019-434f-929c-208c7aed6f11",
   "metadata": {},
   "source": [
    "### Select 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7d7b40-e53f-4065-85b5-5b6647c37cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = iris[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a842ed5-cba9-437c-8b71-027d6efcd240",
   "metadata": {},
   "source": [
    "## Train a multinomial logistic regression with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2db38b-81ab-49bd-bceb-ddc07d583622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "softmax_reg = LogisticRegression(\n",
    "    multi_class=\"multinomial\", solver=\"lbfgs\", C=10, random_state=42\n",
    ")\n",
    "softmax_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5a0cd7-e35d-48fc-970f-748d0886d18f",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Useful code snipets\n",
    "#### Plotting the decision boundaries\n",
    "Code for contour plots is taken from [A. GÃ©ron's notebooks](https://github.com/ageron/handson-ml2) for his book [\"Hands-on machine learning with Scikit-learn, Keras & Tensorflow\"](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ecce4d-d067-4f6e-8d67-309d249f4838",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_id = 1\n",
    "lim_x0 = 0.8, 8\n",
    "lim_x1 = 0, 4\n",
    "x0, x1 = np.meshgrid(\n",
    "    np.linspace(*lim_x0, 500).reshape(-1, 1),\n",
    "    np.linspace(*lim_x1, 200).reshape(-1, 1),\n",
    ")\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "\n",
    "y_proba = softmax_reg.predict_proba(X_new)\n",
    "y_predict = softmax_reg.predict(X_new)\n",
    "\n",
    "zz1 = y_proba[:, class_id].reshape(x0.shape)\n",
    "zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y == 2, 0], X[y == 2, 1], \"g^\", label=\"Iris virginica\")\n",
    "plt.plot(X[y == 1, 0], X[y == 1, 1], \"bs\", label=\"Iris versicolor\")\n",
    "plt.plot(X[y == 0, 0], X[y == 0, 1], \"yo\", label=\"Iris setosa\")\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "custom_cmap = ListedColormap([\"#fafab0\", \"#9898ff\", \"#a0faa0\"])\n",
    "\n",
    "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
    "contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)\n",
    "plt.clabel(contour, inline=1, fontsize=12)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"center left\", fontsize=14)\n",
    "plt.axis([*lim_x0, *lim_x1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109e1f3a-46f0-4876-a61a-3c86661f7d5f",
   "metadata": {},
   "source": [
    "**Suggestion**: create a function to reproduce this contour plot easily. You will need it often thougout this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70fd43d-1304-4aef-ba49-26cb0e3c65e4",
   "metadata": {},
   "source": [
    "#### Frobenius norm of the weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8e9e61-464f-4cab-8e3b-545e3793b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(softmax_reg.coef_ ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0386e6d-1c6e-47c5-807f-34d7a03c9e82",
   "metadata": {},
   "source": [
    "#### L2 norm of the weight vector per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4167db5f-4f20-4d3b-955d-7f109f4c6bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(softmax_reg.coef_ ** 2).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64919f8-69a0-438a-8da3-7d9d32e8ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_function(X, y, classifier, class_id=1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f78ac9-2f49-4207-8633-40a8d02a83cf",
   "metadata": {},
   "source": [
    "### Exercise: qualitative analysis of regularization\n",
    "Study the decision boundary of logistic regression under different regularization strengths.\n",
    "1. Check the documentation for [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) on sklearn's documentation.\n",
    "1. Identify the parameter corresponding to the regularization.\n",
    "    1. Is it directly or inversely related to regularization strentgh?\n",
    "    1. What is the possible range for this hyperparameter?\n",
    "1. Try different values (at least 3) of regularization and observe the effect on the norm of the weights and the decision function.\n",
    "    1. What is the observed relationship between weight norm and regularization?\n",
    "    1. Is the weight norm directly correlated to sharper or smoother decision functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96edec6-0528-4c9a-aee3-612ffe78981c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0abca893-0060-4c69-8377-ae2fd66cc710",
   "metadata": {},
   "source": [
    "## Alternative to multinomial: one-versus-rest logistic regression\n",
    "Whe using the multinomial regression, we treat the multi-class problem by modeling all classes together.\n",
    "Another approach to tackle multi-class classification is to learn a binary classifier for each class. For the final prediction, whichever classifiers predicts the highest confidence \"wins\". This approach is known as one-versus-all or one-versus-rest.\n",
    "\n",
    "You can implement this strategy by setting `multi_class=\"ovr\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0c68d2-8ac9-4308-8c02-02af194287d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", C=10, random_state=42)\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ea7b70-9760-4a24-9fd5-cae0c526f89d",
   "metadata": {},
   "source": [
    "### Exercise: decision boundaries for OVR multi-class logistic regression\n",
    "1. Plot the contours of the decision function for each of the 3 classes and compare to the multinomial classifier. \n",
    "1. What main difference do you observe in the contours of the predicted probabilities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec27eaf-752d-4009-8a62-8fd2a3299ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a7d8c9f-1fd8-4116-923e-7769a55e3d2f",
   "metadata": {},
   "source": [
    "## Implementing OVR logistic regression using SGDClassifier\n",
    "The `SGDClassifier` module can be used to implement logistic regression by setting its loss function accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f32fde-55e6-42a7-8c37-57a850c6f0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_cls = SGDClassifier(\n",
    "    loss=\"log\",\n",
    "    max_iter=1000,\n",
    "    tol=1e-3,\n",
    "    learning_rate=\"constant\",\n",
    "    penalty=None,\n",
    "    eta0=0.1,\n",
    "    random_state=42,\n",
    ")\n",
    "sgd_cls.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff476af-348e-48c1-94e2-dbce3970a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_function(X, y, sgd_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aff395-06bb-4e08-9e3b-c224f13d6e04",
   "metadata": {},
   "source": [
    "### Exercise: Regularization and SGD\n",
    "1. Implement SGDClassifier the same way as before but adding L2 regularization. \n",
    "    1. Identify the regularization parameter by reading the doc of [SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)\n",
    "    1. Which is the convergence criteria used?\n",
    "    1. Which parameters are involved in adjusting the learning rate?\n",
    "1. Test different values for the regularization parameter while observing:\n",
    "    1. Decision function\n",
    "    1. Weight matrix norm\n",
    "    1. Number of iterations until convergence\n",
    "    1. Miscassified samples\n",
    "1. Which relationships do you observe between these metrics and regularization strenght? \n",
    "    1. Can you formulate hypothesis to explain your observations? In other words, can you imagine possible reasons that justify the observed behavior?\n",
    "    1. Can you think of other metrics you could observe to enforce or invalidate your hypothesis?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138595ef-aff7-4f0e-8a85-6b924857bacc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1f57c54-4d1c-4554-b9ba-120c1547f9be",
   "metadata": {},
   "source": [
    "### Exercise: feature standardization and regularized SGD convergence\n",
    "1. Build a pipeline including feature standardization prior to SGD training\n",
    "    - Tip: Use `make_pipeline` from `sklearn.pipeline`\n",
    "1. Observe the number of iterations taken to convergence and compare it to the previous non-standardized case.\n",
    "    - Tip: use the pipeline's `named_steps` property to navigate to SGDClassifier's `n_iter_` attribute.\n",
    "1. Repeat your experiment for different starting points \n",
    "    - Tip: weights are initialized randomly, so you need to change the source of randomness...\n",
    "    1. Did your initial observation generalize to other initial points?\n",
    "    1. Repeat the experiment removing regularization. Do you observe the same difference in convergence?\n",
    "**Suggestion**: you can use violin plots (`plt.violinplot` or `sns.violinplot`) to visually compare the distributions of your observations under each condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5cde4b-934e-4564-a189-48a55d47cac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml-latest]",
   "language": "python",
   "name": "conda-env-ml-latest-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
