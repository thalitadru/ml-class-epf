{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "-PqDEowZvL_4",
        "d1AgkZhaxIq_"
      ],
      "authorship_tag": "ABX9TyPXiL03n0LmjKSYUkVxljpk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thalitadru/ml-class-epf/blob/main/LabTextRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text classification with RNNs\n",
        "## Preamble: installing and importing packages"
      ],
      "metadata": {
        "id": "IiykT_dnzrmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import datasets\n",
        "except ModuleNotFoundError:\n",
        "    !pip install datasets\n",
        "    import datasets"
      ],
      "metadata": {
        "id": "4fNgXNXpU3jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from unidecode import unidecode\n",
        "except ModuleNotFoundError:\n",
        "    !pip install unidecode\n",
        "    from unidecode import unidecode"
      ],
      "metadata": {
        "id": "grvNo4BdOARu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RfXPJl7UPBp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED=34"
      ],
      "metadata": {
        "id": "QLxUTStFhPoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load training dataset\n",
        "\n",
        "We are going to work with a [dataset of movie reviews in french collected by the AlloCine website](https://huggingface.co/datasets/allocine). \n",
        "This dataset can be retreived using the [`datasets` library from the company HuggingFace](https://huggingface.co/docs/datasets/index).\n",
        "\n",
        "The next cells load some information on the dataset:"
      ],
      "metadata": {
        "id": "PoDMRKLxVTfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_HANDLE = \"allocine\""
      ],
      "metadata": {
        "id": "HM3HwhXZV18E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset_builder\n",
        "ds_builder = load_dataset_builder(DATA_HANDLE)"
      ],
      "metadata": {
        "id": "1RToVk78UT6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the dataset description, we can see it is intended to be used for binary sentiment analysis:"
      ],
      "metadata": {
        "id": "L-dalSef8ymi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_builder.info.description"
      ],
      "metadata": {
        "id": "kifPSXJgui5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each element in the dataset has two features: the review text itself, and the associated label:"
      ],
      "metadata": {
        "id": "K2AtiJ_V8-ID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_builder.info.features"
      ],
      "metadata": {
        "id": "MaySYK6_uh9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to load the training data:"
      ],
      "metadata": {
        "id": "7gXsjnAXushx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "train_ds = load_dataset(DATA_HANDLE, split=\"train\")"
      ],
      "metadata": {
        "id": "2Z4ROj35VyJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As seen in `ds_builder.info.features`, each data sample has two fields: the `review` text and the `label` string. Here is the review text for one particular sample"
      ],
      "metadata": {
        "id": "IJSUzpAJuyUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds[10]['review']"
      ],
      "metadata": {
        "id": "ox_tUOUZkcj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalizing characters\n",
        "Some of the tools we'll be using later cannot flawlessly handle all unicode characters. To avoid problems, we will normalize all characters to their closest ASCII equivalent using the function `unidecode` (imported from [`unidecode` package](https://pypi.org/project/Unidecode/)).\n",
        "\n",
        "The function basically replaces all characters bearing [diacritic signs](https://en.wikipedia.org/wiki/Diacritic) with their corresponding plain character, as well as any symbols with close ASCII equivalents. The result is a text with no accents, cedillas, no ‚Ç¨ symbol, etc."
      ],
      "metadata": {
        "id": "-PqDEowZvL_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unidecode(train_ds[10]['review'])"
      ],
      "metadata": {
        "id": "crqN6BSWNhQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the method `map` to apply this transformation to all `review` texts"
      ],
      "metadata": {
        "id": "u4HT4PxVxTZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.map(lambda sample: {'review': unidecode(sample['review']), 'label': sample['label']})"
      ],
      "metadata": {
        "id": "QF4_4TcXkPq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a TF dataset\n",
        "\n",
        "The current dataset object is not in the format recognized by TensorFlow.\n",
        "The `datasets` library provides a method to convert individual samples to the tensorflow format:"
      ],
      "metadata": {
        "id": "d1AgkZhaxIq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.with_format(\"tf\")[10]"
      ],
      "metadata": {
        "id": "kQud27KUVmLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is also possible to convert the entire object into a batched `tf.Dataset`:"
      ],
      "metadata": {
        "id": "VhTUu2JZx4LB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE=80"
      ],
      "metadata": {
        "id": "YCedwF4ZZmXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.set_random_seed(SEED)\n",
        "train_tfds = train_ds.to_tf_dataset(\n",
        "            columns=[\"review\"],\n",
        "            label_cols=[\"label\"],\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=True\n",
        "            )"
      ],
      "metadata": {
        "id": "s1e5jdgwfg1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for example, label in train_tfds.take(1):\n",
        "    print('Example batch shape: ', example.shape)\n",
        "    print('Label batch shape: ', label.shape)\n",
        "    print('text: ', example.numpy()[10,...])\n",
        "    print('label: ', label.numpy()[10,...])"
      ],
      "metadata": {
        "id": "MNGJ6WOwd1q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check here how many batches there are in the dataset with the method `cardinality()`:"
      ],
      "metadata": {
        "id": "SfKiQYVlyJ6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_tfds.cardinality()"
      ],
      "metadata": {
        "id": "PzM4DD7wZc6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO Loading validation data\n",
        "\n",
        "We now load the validation data:"
      ],
      "metadata": {
        "id": "HRXVBaLTxaq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO set the split to validation\n",
        "val_ds = ..."
      ],
      "metadata": {
        "id": "6muQwvWeVdSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We must repeat the same pre-treatment steps applied to the training set:"
      ],
      "metadata": {
        "id": "UfMsApNbyvLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO apply the same character normalization operation you applied to the training set\n",
        "val_ds = ..."
      ],
      "metadata": {
        "id": "R59GkqKQgCY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.set_random_seed(SEED)\n",
        "# TODO convert the val_ds into a tf dataset\n",
        "val_tfds = ..."
      ],
      "metadata": {
        "id": "dGcoZGuHYQD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check how many validation batches we have:"
      ],
      "metadata": {
        "id": "jzyCueVwy1en"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Use the method cardinality on the tf.Dataset object\n",
        "val_tfds ..."
      ],
      "metadata": {
        "id": "NtSvJnXfhlrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text encoding layers\n"
      ],
      "metadata": {
        "id": "zATUxiE1WhnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text vectorization"
      ],
      "metadata": {
        "id": "MH_1YQZZ_J_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simplest way to process text for training is using the [`TextVectorization` layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization). This layer has many capabilities, but in this notebook we stick to the default behavior.\n"
      ],
      "metadata": {
        "id": "vSsRZz_NmxwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TODO Creating and fitting the vectorizer\n",
        "\n",
        "First we create the layer, with default parameters. We need to inform an upper limit to the vocabulary size, using the keyword argument `max_tokens`:\n"
      ],
      "metadata": {
        "id": "HCtdH_3QZviC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 1000\n",
        "# TODO set the max_tokens argument to VOCAB_SIZE\n",
        "encoder = tf.keras.layers.TextVectorization(\n",
        "    # TODO your code here\n",
        "    ...\n",
        "    )\n",
        "\n",
        "encoder"
      ],
      "metadata": {
        "id": "XVoiNNqgfygx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then need to train `encoder` (our vectorizer) on our training texts. This encoder is fitted in an **unsupervised** manner: we only use the texts, not the labels. Moreover, this encoder needs to be fully fitted prior to training of any subsequent NN models (since it defines the vector space on which NN models will work).\n",
        "In keras, this type of training uses a different method: `.adapt` (instead of `fit`). \n",
        "\n",
        "`.adapt` must receive a different version of the dataset, that only contains the review text and does not contain any labels. We can do this transformation using the method `.map`:\n",
        "\n"
      ],
      "metadata": {
        "id": "Yg-aFDid3HYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_tfds_txt = train_tfds.map(lambda text, label: text)"
      ],
      "metadata": {
        "id": "P5SY5LvZ37N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can pass the text only dataset to the layer's `.adapt` method:"
      ],
      "metadata": {
        "id": "OCiSB3NR4GSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO adapt the encoder to the texts in the training dataset\n",
        "encoder..."
      ],
      "metadata": {
        "id": "O8YbnsS-22Od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Checking the vocabulary\n",
        "The `.adapt` method sets the layer's **vocabulary**. Here are the first 50 tokens. \n",
        "- the first is an empty string token, corresponding to zero-padded sequence positions\n",
        "- the second `[UNK]` stands for any unkknown tokens, all encoded with value 1.\n",
        "- the remaining tokens are words sorted by frequency of appearence in the text corpus"
      ],
      "metadata": {
        "id": "G17HtARCan8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = np.array(encoder.get_vocabulary())\n",
        "vocab[:50]"
      ],
      "metadata": {
        "id": "e6Og1g_OhCV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjId5pua3jHQ"
      },
      "source": [
        "Once the vocabulary is set, the layer can encode text into indices (following the vocabulary order). That is, `de` is encoded as 2, `et` encoded as 3, `le` encoded as 4, `a` encoded as 5, and so on.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example of encoded sequence"
      ],
      "metadata": {
        "id": "BV5kFv-XajYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Let us look back into the 2 first samples in the batch of training samples loaded previously:\n"
      ],
      "metadata": {
        "id": "4yCsOg0SbWqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example[:2]"
      ],
      "metadata": {
        "id": "JAQvKRvY5O3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "After encoding, the tensors of indexes are 0-padded to the longest sequence in the batch (unless you set a fixed `output_sequence_length`):"
      ],
      "metadata": {
        "id": "uxHs9C4t5JNU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGc7C9WiwRWs"
      },
      "outputs": [],
      "source": [
        "encoded_example = encoder(example).numpy()\n",
        "encoded_example[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dealing with variable sequence lenghts"
      ],
      "metadata": {
        "id": "nGY4yGc7iAsY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF-PsCk1LwjY"
      },
      "source": [
        "You have just seen the `encoder` layer pads sequence endding so that all sequences in a batch have the same lenght.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlS0iaUIWLpI"
      },
      "source": [
        "To see an example, lets compute the encoding for a short review. Since the batch contain only this review, no padding needs to be done:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O41gw3KfWHus"
      },
      "outputs": [],
      "source": [
        "short_review = \"rien a redire\"\n",
        "\n",
        "batch = np.array([short_review])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can confirm this by thecking the encoder output for this batch:"
      ],
      "metadata": {
        "id": "FNe5vImRFO_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder(batch)"
      ],
      "metadata": {
        "id": "hVe2FVxnE36u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that each one of the three words is represented by the corresponding vocabulary index. No zeros are added."
      ],
      "metadata": {
        "id": "A06rSO-acR4I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYg8mGfEr7Qx"
      },
      "source": [
        "Now see how the encoder pads the end of shorter sequences with zeros.\n",
        "First we include a long review in the batch. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Mg-30Wnr7Qx"
      },
      "outputs": [],
      "source": [
        "long_review = (\"un tres bon film qui vaut au moins 3 etoile car le casting est\"\n",
        "            \" superbe avec notamment Rachel Hurd-Wood qui est exeptionnelle\")\n",
        "\n",
        "batch = np.array([short_review,\n",
        "                 long_review])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `encoder` will need to pad the short sentence so it matches the lenght of the longest one in the batch:\n"
      ],
      "metadata": {
        "id": "cGmxRyxUr7Qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder(batch)"
      ],
      "metadata": {
        "id": "Gb1Fiw_6r7Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first 3 values are the same, the rest of the sequence is filled with zeros."
      ],
      "metadata": {
        "id": "4Z7NM8mesK5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decoding an encoded sequence\n",
        "Let's focus on the beggining of the first batch sample:"
      ],
      "metadata": {
        "id": "-Sz4HX0t6pkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Example (first 50 chars): \", example[0].numpy()[:50])\n",
        "print(\"Encoded example (first 10 words):\", encoded_example[0][:10])\n",
        "print(\"Decoded with vocabulary (first 10 words): \", \" \".join(vocab[encoded_example[0][:10]]))\n"
      ],
      "metadata": {
        "id": "ydAlhfCI6oTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5cjz0bS39IN"
      },
      "source": [
        "With the default settings, the process is not completely reversible. There are two main reasons for that:\n",
        "\n",
        "1. The default value for `preprocessing.TextVectorization`'s `standardize` argument is `\"lower_and_strip_punctuation\"`: punctuation and uppercase information is lost\n",
        "2. The limited vocabulary size: any infrequent words which did not make it up into the top list (here top-1000) will be assigned the code `1` corresponding to the `[UNK]` unknown token.\n",
        "\n",
        "Here we compare original text and encoded-decoded text for some batch samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_tD0QY5wXaK"
      },
      "outputs": [],
      "source": [
        "for n in range(3):\n",
        "  print(\"Original: \", example[n].numpy())\n",
        "  print(\"Encoded-decoded: \", \" \".join(vocab[encoded_example[n]]))\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word embedding\n",
        "An embedding layer stores one vector per word. When called, it converts the sequences of word indices to sequences of vectors.\n",
        "\n",
        "\n",
        "\n",
        "In theory, this operation is equivalent to one-hot encoding each word in a sequence, then passing the sequence through a `tf.keras.layers.Dense` layer. This implementation however avoids explicit one-hot encoding and works with index-lookup to be more computationally efficient.\n",
        "\n",
        "Here is an example of some one-hot encoded words with a 5-word vocabulary:\n",
        "![one hot encoded](https://www.tensorflow.org/static/text/guide/images/one-hot.png)\n",
        "\n",
        "After a projection to $\\mathbb{R}^4$, these same words get represented in 4-D:\n",
        "![Embedding example](https://www.tensorflow.org/static/text/guide/images/embedding2.png)\n",
        "\n",
        "\n",
        "Despite not explicitly using one, an `Embedding` layer has trainable weights just like a `Dense` layer. These weights project all vocabulary words into a common vectorspace.  After training (on enough data), words with similar meanings often have similar vectors. \n",
        "    \n"
      ],
      "metadata": {
        "id": "0DipiZ8kxZnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TODO Creating the layer\n",
        "The code bellow creates a [`tf.keras.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer that represents words with 64-dimensional vectors (as set via the argument `output_dim`):\n"
      ],
      "metadata": {
        "id": "tas6VRTs3Ggl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed for reproducibility\n",
        "tf.keras.utils.set_random_seed(SEED)\n",
        "\n",
        "# TODO check the documentation and complete the call\n",
        "embedding = tf.keras.layers.Embedding(\n",
        "        # TODO set the input dimension to be the length of the encoder vocabulary\n",
        "        ...\n",
        "        # TODO set the output dimension to 64\n",
        "        ...\n",
        "        # Use masking to handle the variable sequence lengths\n",
        "        mask_zero=True)"
      ],
      "metadata": {
        "id": "OqAGcK3xZDkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dealing with variable sequence lenghts\n",
        "The embedding layer uses [masking](https://www.tensorflow.org/guide/keras/masking_and_padding) to handle the varying sequence-lengths. \n",
        "\n",
        "Masking allows the layer to **ignore the portions that got zero-padded** by the `encoder` layer. \n",
        "\n",
        "\n",
        "\n",
        "We have activated it by declaring the layer with the keyweord argument `mask_zero=True`.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8YjjMhVyqc1q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87a8-CwfKebw"
      },
      "outputs": [],
      "source": [
        "print(embedding.supports_masking)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will include both the `encoding` and `embedding` layer in the following models."
      ],
      "metadata": {
        "id": "ZL1-LlGzzUZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM Model"
      ],
      "metadata": {
        "id": "ljHnJuhjWmf2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7zsmInBOCPO"
      },
      "source": [
        "### Description\n",
        "![A drawing of the information flow in the model](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/bidirectional.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgs6nnSTGw-t"
      },
      "source": [
        "Above is a diagram of the model. This model can be build as a `tf.keras.Sequential`.\n",
        "\n",
        "2. The first layer is the `encoder`, which converts the text to a sequence of token indices.\n",
        "\n",
        "3. After the encoder is an `embedding` layer, that converts the sequences of word indices to sequences of vectors.\n",
        "\n",
        "4. A recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input on the next timestep. \n",
        "Here we use a recurrent layer of the [`LSTM`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) type.\n",
        "\n",
        "5. Additionally, we use the [`tf.keras.layers.Bidirectional`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional) wrapper. This propagates the input forward and backwards through the RNN layer and then concatenates the final output. It is often used with text sequences (but should not be used with time series since it breaks causality).\n",
        "\n",
        "  * The main advantage of a bidirectional RNN is that the signal from the beginning of the input doesn't need to be processed all the way through every timestep to affect the output.  \n",
        "\n",
        "  * The main disadvantage of a bidirectional RNN is that you can't efficiently compute online predictions for a word stream, since new words keep getting added at the end of the sequence.\n",
        "\n",
        "6. After the RNN has converted the sequence to a single vector the two `layers.Dense` do some final processing, and convert from this vector representation to a single logit as the classification output. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO Declaration\n"
      ],
      "metadata": {
        "id": "gLfgoajXhv_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed for reproducibility\n",
        "tf.keras.utils.set_random_seed(SEED)\n",
        "\n",
        "# TODO complete the call bellow\n",
        "model = tf.keras.Sequential([\n",
        "    # We reuse the encoder and embedding layers previously created\n",
        "    encoder,\n",
        "    embedding,\n",
        "    # TODO add a bidirectional LSTM layer with 64 units\n",
        "    ...,\n",
        "    # TODO add a dense layer with 64 units and relu activation\n",
        "    ...,\n",
        "    # TODO add an output layer\n",
        "    ...\n",
        "])"
      ],
      "metadata": {
        "id": "kfnpXotsRPqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe the output shapes in the model summary. Note that the `encoder` layer has a 2D output shape `(None, None)`:\n",
        "- The first dimension `None`, as usual, is a placeholder for the batch dimension\n",
        "- the second dimension `None` is a placeholder for the sequence lenght dimension. Since each batch of sequences has variable lenght, this dimension does not have a fixed size.\n",
        "\n"
      ],
      "metadata": {
        "id": "1LoOvPMjpAee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "14dX_SE5c_eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that since we are using bidirectional LSTM, each unit has two outputs, leading to an output shape of `2*64=128`."
      ],
      "metadata": {
        "id": "QmhiGdr3A8B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A note on masking padded sequences"
      ],
      "metadata": {
        "id": "EBvNT_hJrJC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "All the layers after the `Embedding` support masking, meaning they all ignore padding in short sequences:\n",
        "\n"
      ],
      "metadata": {
        "id": "i2b3SHDaqmGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print([(layer.name, layer.supports_masking) for layer in model.layers])"
      ],
      "metadata": {
        "id": "bSKINJ7eBcLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This means that **predictions for a given sample should ramain the same, regardless zero-padding**.\n",
        "\n",
        "To see an example, let's compute predictions for a short review. \n"
      ],
      "metadata": {
        "id": "IYgZDnA4rfcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(short_review)"
      ],
      "metadata": {
        "id": "USr9ZWxsseTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Since the batch contain only this review, no padding needs to be done.\n",
        "\n",
        "Applying the model to this batch will give the following prediction:"
      ],
      "metadata": {
        "id": "C51vMBR2sifK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict on a sample text without padding.\n",
        "batch = np.array([short_review])\n",
        "\n",
        "predictions = model.predict(batch)\n",
        "print(predictions[0])"
      ],
      "metadata": {
        "id": "lBR0joGoFGV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we check that even if the sentence needs pading, its corresponding model output remains the same. First we include a long review in the batch. As seen before, we know the encoder will pad the shorter sequence up to the lenght of the longest sequence."
      ],
      "metadata": {
        "id": "OGpcl2r2FYc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(long_review)"
      ],
      "metadata": {
        "id": "l4QtcNn7sntA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIgpuTeFNDzq"
      },
      "outputs": [],
      "source": [
        "batch = np.array([short_review,\n",
        "                 long_review])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we compute predictions in the new batch. The prediction for the short review should be identical:"
      ],
      "metadata": {
        "id": "j2Eyv-hQE3YB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict on a sample text with padding\n",
        "batch = np.array([short_review,\n",
        "                 long_review])\n",
        "\n",
        "\n",
        "predictions = model.predict(batch)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "33ATm5c2FpN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO Compile"
      ],
      "metadata": {
        "id": "XGN4owDKhzLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO complete the compile call\n",
        "model.compile(...)"
      ],
      "metadata": {
        "id": "Me3xfCIKTIJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calbacks and logs\n"
      ],
      "metadata": {
        "id": "Zpbt-KQ-H67A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dictionary to keep history output from fit calls\n",
        "logs = {}\n",
        "\n",
        "# directory in which model checkpoints and logs are saved\n",
        "LOG_DIR = 'logs'\n",
        "\n",
        "def best_model_path(model_name):\n",
        "    base_dir  = os.path.join(LOG_DIR, model_name)\n",
        "    return os.path.join(base_dir, 'best_val_accuracy.ckpt')\n",
        "\n",
        "def callback_list(model_name):\n",
        "    base_dir  = os.path.join(LOG_DIR, model_name)\n",
        "    tb_cb = tf.keras.callbacks.TensorBoard(base_dir)\n",
        "    ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
        "         best_model_path(model_name),\n",
        "         monitor='val_accuracy',\n",
        "         mode='max', \n",
        "         verbose=0,\n",
        "         save_best_only=True)\n",
        "    backup_dir = os.path.join(base_dir, 'backup_checkpoint')\n",
        "    bkp = tf.keras.callbacks.BackupAndRestore(\n",
        "        backup_dir)\n",
        "    return [tb_cb, ckpt, bkp]"
      ],
      "metadata": {
        "id": "mvccjj8VZ620"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensorboard"
      ],
      "metadata": {
        "id": "ZcUAdvCmIIAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO laod tensorboard extension\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "hUIBsiMdIKRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO call tensorboard on your log directory\n",
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "RaHds6jzIMdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fit\n",
        "Complete the fit call with training and validation data. Train the model for 10 epochs. With Colab's GPU backend, this should take you around 20 minutes. In the mean-time, **go back to Moodle and check this week's quiz ‚úî‚úç üòÄ**"
      ],
      "metadata": {
        "id": "O8wWADoPh1UB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'LSTM'\n",
        "logs[MODEL_NAME] = model.fit(\n",
        "    # complete the fit call\n",
        "    ...,\n",
        "    callbacks=callback_list(MODEL_NAME)\n",
        "    )"
      ],
      "metadata": {
        "id": "JYiPE4NOYDkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stacking 2 LSTM layers"
      ],
      "metadata": {
        "id": "XFMHz-gvWqAI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g1evcaRpTKm"
      },
      "source": [
        "When using the output of a recurrent layer as input to another recurrent layer, the second should process not only the final output, but acctualy all the intermediate results (corresponding to each position in the sequence). To do that we need to tell the keras layer to return intermediate results. This behavior is controlled by the `return_sequences` constructor argument:\n",
        "\n",
        "* If `False` it returns only the last output for each input sequence (a 2D tensor of shape (batch_size, output_features)). This is the default, used in the previous model.\n",
        "\n",
        "* If `True` the full sequences of successive outputs for each timestep is returned (a 3D tensor of shape `(batch_size, timesteps, output_features)`).\n",
        "\n",
        "Here is what the flow of information looks like with `return_sequences=True`:\n",
        "\n",
        "![layered_bidirectional](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/layered_bidirectional.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO Declaration"
      ],
      "metadata": {
        "id": "Y1hgNQFjC1QR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed for reproducibility\n",
        "tf.keras.utils.set_random_seed(SEED)\n",
        "\n",
        "# TODO complete the model declaration \n",
        "model2 = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    embedding,\n",
        "    # TODO set the LSTM layer to return sequences\n",
        "    ...\n",
        "    # TODO add another Bidirectional LSTM layer with 32 units\n",
        "    ...\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "YBFt719yWv6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbSClCrG1z8l"
      },
      "source": [
        "**Note:**\n",
        "- because we set `return_sequences=True`, the output for the first LSTM layer still has 3-dimensions, like its input, so it can be passed to another recurrent layer.\n",
        "- The second LSTM layer behaves as in the previous model, with a 2D output \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.summary()"
      ],
      "metadata": {
        "id": "ezkeceQ5ibU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO Compile and fit"
      ],
      "metadata": {
        "id": "ZuTSACA5iy7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO complete the compile call\n",
        "model2.compile(...)"
      ],
      "metadata": {
        "id": "7rj4iYoeW3WG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model takes longer to train because there are more forward and backward computations to be done with the addition of the extra RNN layer. 10 epochs should take about 35 min on colab with GPU backend. To limit the time spent, we will **train for 5 epochs only.**"
      ],
      "metadata": {
        "id": "xQa73N45F8Ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'Stack2LSTM'\n",
        "logs[MODEL_NAME] = model2.fit(\n",
        "    # TODO complete the fit call\n",
        "    ...,\n",
        "    callbacks=callback_list(MODEL_NAME)\n",
        "    )"
      ],
      "metadata": {
        "id": "8HkIqQcBYC95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comments\n",
        "\n",
        "\n",
        "If we use a randomly initialized embedding, this model does not outperform the previous one even after 10 (long) epochs. We can however reuse the embedding that was trained with the previous model, in hopes it gives us a head start (and this was the strategy used in the code above). Reusing the trained embedding let us achieve and surpass the previous model's performance after 3 epochs.\n",
        "\n",
        "Nonethelees, keep in mind it is possible that simply training the previous model for extra 10 epochs would lead to similar improvements, though it remains to be tested. If that were the case, the computational overhead of a second recurrent layer could not be as easily justified."
      ],
      "metadata": {
        "id": "2wGzc9s2CHvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: save all logs and checkpoints to a compressed archive you can download\n",
        "#!tar -czf logs.tgz logs"
      ],
      "metadata": {
        "id": "gbh4gj3PMl1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO Test time!\n",
        "1. Load the test split for this dataset\n",
        "2. Apply the same pre-processing steps used for training and validation\n",
        "3. Load your best model from the corresponding model checkpoint and evaluate it on the test set. What was your accuracy?\n",
        "4. Write your own fake movie review (positive or negative) and process it through your model. Did it get correctly classified?"
      ],
      "metadata": {
        "id": "rJ7PXGyb9t7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO your code here"
      ],
      "metadata": {
        "id": "v3Uo0GMnD6yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "This notebook is based on the following tutorials:\n",
        "- [Text classification with an RNN |Tensorflow documentation](https://www.tensorflow.org/text/tutorials/text_classification_rnn) \n",
        "- [Word embeddings |Tensorflow documentation](https://www.tensorflow.org/text/guide/word_embeddings)\n",
        "\n",
        "Tensorflow documentation is release under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/) license, with code samples under [Apache license 2.0](https://www.apache.org/licenses/LICENSE-2.0).\n",
        "\n",
        "\n",
        "Additional references/sources are:\n",
        "\n",
        "- [Load a dataset from the Hub | Huggingface Datasets documentation](https://huggingface.co/docs/datasets/load_hub)\n",
        "- [Using Datasets with TensorFlow | Huggingface Datasets documentation](https://)huggingface.co/docs/datasets/use_with_tensorflow\n",
        "- [Masking and padding |Tensorflow documentation](https://www.tensorflow.org/guide/keras/masking_and_padding)\n"
      ],
      "metadata": {
        "id": "2MyUp2Q6WS-N"
      }
    }
  ]
}