{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNovM6yfTLUmNoI61/oDSTQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thalitadru/ml-class-epf/blob/main/TutorialAutoDiff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic differentiation software (Auto Diff SW)\n",
        "*Credits*: compilation of Tensorflow and PyTorch tutorials:\n",
        "- https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\n",
        "- https://www.tensorflow.org/guide/autodiff"
      ],
      "metadata": {
        "id": "-JNy7nQ0DgGX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "Sx3ley5ou2Ag"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To differentiate, AutoDiff SW needs to keep a \"track record\" of the order in which operations are applyed to variables. The SW package then uses this \"track record\" to compute the gradients of the \"recorded\" computations using [reverse mode differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation). This record takes the form of a **computational graph**.\n",
        "\n",
        "\n",
        "In order for the entire chain of operations to be differentiable, each operator implemented needs to have a *forward mode*, that gets called to compute the normal operation, along with a *backward mode*, that can be called to compute its derivative.\n",
        "\n",
        "\n",
        "Using these operators, we only need to explicitly declare the **forward pass**, that is, the computations leading to the expression we want to differentiate (typically a cost function). AutoDiff SW will be able to follow operations backwards and compute gradients for the **backward pass**. \n"
      ],
      "metadata": {
        "id": "Es13QFDSu-1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: derivative of y = xÂ² with respect to x"
      ],
      "metadata": {
        "id": "0LgEOhv8vutS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensorflow"
      ],
      "metadata": {
        "id": "ls4kY7DjvfRM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CLWJl0QliB0"
      },
      "source": [
        "\n",
        "TensorFlow provides the `tf.GradientTape` API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually `tf.Variable`s.\n",
        "TensorFlow \"records\" relevant operations executed inside the context of a `tf.GradientTape` onto a \"tape\". \n",
        "\n",
        "Here is a simple example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "Xq9GgTCP7a4A"
      },
      "outputs": [],
      "source": [
        "x = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x**2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR9tFAP_7cra"
      },
      "source": [
        "Once you've recorded some operations, use `GradientTape.gradient(target, sources)` to calculate the gradient of some target (often a loss) relative to some source (often the model's variables):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "LsvrwF6bHroC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fc1caff-e0d1-4ef4-8992-7d94ee9443ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=6.0>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "# dy = 2x * dx\n",
        "dy_dx = tape.gradient(y, x)\n",
        "dy_dx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can call `.numpy()` to cast the tensor into a numpy array."
      ],
      "metadata": {
        "id": "p8B94hox3E7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dy_dx.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMPj7STr3EJm",
        "outputId": "ff54d0d9-32fc-4c75-b4d3-31dcdaecf51b"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.0"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch\n",
        "PyTorch has a built-in differentiation engine called `torch.autograd`. It supports automatic computation of gradient for any computational graph.\n",
        "\n",
        "In this exemple, we want to be able to compute the gradients of y with respect to x. In order to do that, we set the `requires_grad` property of the tensor containing the dependent variable x."
      ],
      "metadata": {
        "id": "WJ98WShrvjek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "y = x**2"
      ],
      "metadata": {
        "id": "v4wiX_7PvddC"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: You can set the value of `requires_grad` when creating a tensor, or later by using `x.requires_grad_(True)` method."
      ],
      "metadata": {
        "id": "WBfPow031paP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A function that we apply to tensors during the forward pass is in fact an object of class [`Function`](https://pytorch.org/docs/stable/autograd.html#function). This object knows how to compute the function in the *forward* direction, and also how to compute its derivative during the *backward* propagation step. A reference to the backward propagation function is stored in `grad_fn` property of a tensor:"
      ],
      "metadata": {
        "id": "l5GuTET01HHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Gradient function for y = {y.grad_fn}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQWAP2nov4vf",
        "outputId": "db061190-add6-48dd-bcdd-bb2f39c7c9a3"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient function for y = <PowBackward0 object at 0x7fad16946dd0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compute derivatives of y, we call `y.backward()`:"
      ],
      "metadata": {
        "id": "9ICjFUcK2MAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# computes gradients of y with respect to any dependent variables\n",
        "# having requires_grad=True\n",
        "y.backward()"
      ],
      "metadata": {
        "id": "RoXose3dwCEV"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we retrieve the value of $\\partial y/ \\partial x$ in `x.grad`"
      ],
      "metadata": {
        "id": "DLKkcQcV2kSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dy/dx\n",
        "dy_dx = x.grad\n",
        "dy_dx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2TOZUtlwPT2",
        "outputId": "e75a2ced-afb1-4597-eb64-6ac91a4aebde"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like in Tensorflow, we can call `.numpy()` to cast the tensor into a numpy array."
      ],
      "metadata": {
        "id": "bqSDoXaV2wJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dy_dx.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxEVCRmiw5QC",
        "outputId": "de55a9fd-aed1-4087-d942-bc4ae84d2fe4"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(6., dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: cost derivative for a layer of neurons\n",
        "\n",
        "In this example we compute forward and backward passes for a single layer of neurons with softmax activation (in other words, we are applying multinomial logistic regression).\n",
        "\n",
        "In this model, w and b are parameters, which we need to optimize. Thus, we need to be able to compute the gradients of loss function with respect to those variables. \n",
        "\n",
        "The following sections show how to implement this graph in Tensorflow and PyTorch. The forward operations are:\n",
        "$$logits = wx + b$$\n",
        "$$loss = \\mathtt{CrossEntropyFromLogits}(logits, y)$$\n",
        "\n",
        "Here `CrossEntropyFromLogits` is a function that computes Cross-entropy between target propabilities $y$ and predicted $logits$ (the values predicted prior to aplying sigmoid or softmax).\n",
        "\n",
        "\n",
        "\n",
        "Here is the forward computational graph for this chain of operations:\n",
        "\n",
        "\n",
        "\n",
        "![computational graph](https://pytorch.org/tutorials/_images/comp-graph.png)"
      ],
      "metadata": {
        "id": "oZTj2E8zx85m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is some example data to test the code: 2 samples with 4 features each, and predicted probabilities for 3 classes"
      ],
      "metadata": {
        "id": "Op30GKyEBxSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = [[-2., 2., 2., -2.], [ 2.1, 1., 1.5, -1.]] # input\n",
        "y = [[1., 0., 0.], [0., 1., 0]] # expected output"
      ],
      "metadata": {
        "id": "L5IxzI3yAjhv"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This implies we need $w$ to be shape=(4,3) and b to be shape=(3,)."
      ],
      "metadata": {
        "id": "OLEBD-uaPSuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensorflow\n",
        "The previous example uses scalars, but `tf.GradientTape` works as easily on any tensor. \n",
        "\n",
        "Here we use the built-in cost function [`tf.nn.softmax_cross_entropy_with_logits`](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits) which expects not the predicted probabilities, but the logits (prior to the application of softmax):"
      ],
      "metadata": {
        "id": "n8U3eNw2yJPA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "vacZ3-Ws5VdV"
      },
      "outputs": [],
      "source": [
        "# parameters\n",
        "w = tf.Variable(tf.random.normal((4, 3)), name='w')\n",
        "b = tf.Variable(tf.zeros(3, dtype=tf.float32), name='b')\n",
        "\n",
        "# Computations we want to track get done with the GradientTape context\n",
        "with tf.GradientTape() as tape:\n",
        "    # NOTE tensorflow is implcitly casting x and y to tf.Tensor\n",
        "    logits = x @ w + b\n",
        "    loss = tf.nn.softmax_cross_entropy_with_logits(y,logits)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4eXOkrQ-9Pb"
      },
      "source": [
        "To get the gradient of `loss` with respect to both variables, you can pass both as sources to the `gradient` method. The tape is flexible about how sources are passed and will accept any nested combination of lists or dictionaries and return the gradient structured the same way (see `tf.nest`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "luOtK1Da_BR0"
      },
      "outputs": [],
      "source": [
        "[dl_dw, dl_db] = tape.gradient(loss, [w, b])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei4iVXi6qgM7"
      },
      "source": [
        "The gradient with respect to each source has the shape of the source:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "aYbWRFPZqk4U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef86c0cf-aced-45d4-a29a-c55ff3594d0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 3)\n",
            "(4, 3)\n"
          ]
        }
      ],
      "source": [
        "print(w.shape)\n",
        "print(dl_dw.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A `GradientTape` may be used only once to compute gradients. After that the computational graph is erased. "
      ],
      "metadata": {
        "id": "zufA3wMPFtLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This should raise an error\n",
        "[dl_dw, dl_db] = tape.gradient(loss, [w, b])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "IYmfsGjAFhE6",
        "outputId": "486936f8-3ada-41a3-f619-ac30f78ee337"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-76fa2c4df080>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This should raise an error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mdl_dw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_db\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \"\"\"\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m       raise RuntimeError(\"A non-persistent GradientTape can only be used to \"\n\u001b[0m\u001b[1;32m   1030\u001b[0m                          \"compute one set of gradients (or jacobians)\")\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recording\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be able to repeat the gradient computations, you need to set `persistent=True` when instantiating `GradientTape`:"
      ],
      "metadata": {
        "id": "JQudY7BJGHzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to regriate the forwad computations, this time with persistent gradient tape\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "    # NOTE tensorflow is implcitly casting x and y to tf.Tensor\n",
        "    logits = x @ w + b\n",
        "    loss = tf.nn.softmax_cross_entropy_with_logits(y,logits)\n",
        "\n",
        "# Now gradients can be computed multiple times\n",
        "[dl_dw, dl_db] = tape.gradient(loss, [w, b])"
      ],
      "metadata": {
        "id": "-2QNYorNGQ7b"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI_SzxHsvao1"
      },
      "source": [
        "Here is the gradient calculation again, this time passing a dictionary of variables. In this case gradients are returned in a dictionnary with the same indexed as the input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d73cY6NOuaMd"
      },
      "outputs": [],
      "source": [
        "my_vars = {\n",
        "    'w': w,\n",
        "    'b': b\n",
        "}\n",
        "\n",
        "grad = tape.gradient(loss, my_vars)\n",
        "grad['b']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch\n",
        "PyTorch will also compute the backward pass for arbitrary-sized tensors. \n",
        "It is more strict about the type of inputs and targets, that must be cast to `torch.tensor` explicitly prior to manipulation.\n",
        "\n",
        "Here we use the built-in cost function [`torch.nn.functional.cross_entropy`](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html#torch.nn.functional.cross_entropy) which expects not the predicted probabilities, but the logits (prior to the application of softmax)"
      ],
      "metadata": {
        "id": "Jh0dpkmYyWFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "w = torch.randn(4, 3, requires_grad=True)\n",
        "b = torch.zeros(3, requires_grad=True)\n",
        "\n",
        "# NOTE in Pytorch we need to cast x and y to tensors explicitly\n",
        "logits = torch.matmul(torch.tensor(x), w) + b\n",
        "\n",
        "loss = torch.nn.functional.cross_entropy(logits, torch.tensor(y))"
      ],
      "metadata": {
        "id": "GqV1LaThyfjp"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We call `loss.backward()` to compute the derivatives with respect to all dependent variables (w and b):"
      ],
      "metadata": {
        "id": "OiEWIPzpEX63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "xqQ2MG9N7gEg"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then gradients can be retrieved under `w.grad` and `b.grad`:"
      ],
      "metadata": {
        "id": "3C7LEYj_EerR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[dl_dw, dl_db] = w.grad, b.grad"
      ],
      "metadata": {
        "id": "U_vkwxP3CI2X"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMVPzYjdCkol"
      },
      "source": [
        "As expected, the gradient with respect to each source has the shape of the source:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f53ea2b-962f-48bf-fe5d-cf84524c3343",
        "id": "bSKUsqq-Ckoq"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 3])\n",
            "torch.Size([4, 3])\n"
          ]
        }
      ],
      "source": [
        "print(w.shape)\n",
        "print(dl_dw.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can only perform gradient calculations using backward once on a given graph, for performance reasons. If we need to do several backward calls on the same graph, we need to pass `retain_graph=True` to the backward call."
      ],
      "metadata": {
        "id": "VaL4L7wqE2fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this will raise an error\n",
        "loss.backward()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "Qq2JT5epCxJO",
        "outputId": "1b65e7cb-72a2-4115-a439-8dd0a6d76b70"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-5a592879b21f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# this will raise an error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# need to recriate the forward graph and use retain_graph=True in backward call\n",
        "logits = torch.matmul(torch.tensor(x), w) + b\n",
        "\n",
        "loss = torch.nn.functional.cross_entropy(logits, torch.tensor(y))\n",
        "\n",
        "loss.backward(retain_graph=True)"
      ],
      "metadata": {
        "id": "9Ur6UnKDFGBY"
      },
      "execution_count": 100,
      "outputs": []
    }
  ]
}