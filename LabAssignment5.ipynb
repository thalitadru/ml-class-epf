{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LabAssignment5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNDsgFxA5XhpDrVkRZILeFw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thalitadru/ml-class-epf/blob/main/LabAssignment5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPLM2m6Ph9NR"
      },
      "source": [
        "*Credits:* Exercises 1 through 5 are based on notebooks written by [A. GÃ©ron](https://colab.research.google.com/github/ageron/handson-ml2/) for his \"Hands-on ML\" book. Code realeased under [Apache 2.0 license](https://github.com/ageron/handson-ml2/blob/master/LICENSE)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sXwnYZQGRgso"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import scipy as sp\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import skimage.io\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "import os\n",
        "import urllib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Color quantization\n",
        "\n",
        "In this exercise we'll user k-means to reduce the number of colors in an image. We'll work with the following example image:"
      ],
      "metadata": {
        "id": "OsXgJ2L3TaAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the ladybug image\n",
        "images_path = \"./images\"\n",
        "os.makedirs(images_path, exist_ok=True)\n",
        "url = \"https://github.com/ageron/handson-ml2/raw/master/images/unsupervised_learning/ladybug.png\"\n",
        "filename = url.split('/')[-1]\n",
        "urllib.request.urlretrieve(url, os.path.join(images_path, filename))"
      ],
      "metadata": {
        "id": "gqVG8ahvTaTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = skimage.io.imread(os.path.join(images_path, filename))\n",
        "image.shape"
      ],
      "metadata": {
        "id": "nklOhUyATni5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(image)"
      ],
      "metadata": {
        "id": "mi39KGL2Tw1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our goal is to represent this same image using a reduced pallete of colors.\n",
        "To do so, we want to find K colors that should summarize all the others. Try this procedure for K = {10,8,6,4,2} colors.\n",
        "  1. Apply K-means to the image pixels. The centroids of each cluster will correspond to the color of that cluster\n",
        "  2. Create a copy of the image mapping all its pixel values to that of their corresponding cluster centroid\n",
        "  3. Display the new color-reduced images. Which pallete size looks best?"
      ],
      "metadata": {
        "id": "hqzRbl1XVR6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-ZesTAL3UvFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2**: Clustering as a preprocessing step for classification\n",
        "\n",
        "In this exercise, you are going to use clustering to improve the classification performance of logistic regression model on a digits dataset similar to MNIST."
      ],
      "metadata": {
        "id": "8LEy6FC0WmSD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwZb2xunAMd_"
      },
      "source": [
        "from sklearn.datasets import load_digits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzXZEiyEAMd_"
      },
      "source": [
        "X_digits, y_digits = load_digits(return_X_y=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_id=0\n",
        "plt.imshow(X_digits[sample_id].reshape([8,8]), cmap='binary', interpolation='bilinear')\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "I_m5EmcfXynR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  1. Split this dataset into a training and a test set.\n",
        "  2. Fit a logistic regression model and evaluate it on the test set. This should give you a baseline accuracy of about 96-97%\n",
        "  3. Now we'll use clustering as a preprocessing step by including it in a `Pipeline`. Create a `Pipeline` (e.g. using `make_pipeline`) chaining K-means then Logistic Regression. Start trying out 50 clusters for K-means\n",
        "  4. Evaluate you pipeline on the test set. Did you get a better score than before? By how much?\n",
        "  5. We did choose the number of clusters in a completely arbitrary fashion. Use a hyperparameter search method (e.g. `GridSearchCV`) to test some other values for `n_clusters`.\n",
        "  6. Compare your results before and after hyperparameters tuning. Could you improve the model?"
      ],
      "metadata": {
        "id": "VV7ovlEXYIE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XrEpf64FX4tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3**: Using clustering for semi-supervised learning\n",
        "\n",
        "When we have little labeled samples but lots of unlabeled ones, clustering can help us profit from the information the unlabeled samples can gives us.\n",
        "To demonstrate this idea, well re-use the previous digits dataset, but restrict ourselves to using only a handful of labels:\n"
      ],
      "metadata": {
        "id": "_ywdaQRJZ9ad"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzBXwDKLAMeI"
      },
      "source": [
        "n_labeled = 50\n",
        "X_train_labeled, y_train_labeled = X_train[:n_labeled], y_train[:n_labeled]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use clustering to find representative samples\n",
        "Now its up to you:\n",
        "1. Train a logistic regression model on the reduced labeled training set. How lower is the accuracy compared to the previous experiment?\n",
        "2. Now let's use all the samples to train a K-means clustering model. Start using 50 clusters. \n",
        "3. K-means will have found us 50 centroids that don't correspond directly to dataset images. We'll take the closest image to each centroid to represent that cluster. To do so, compute distances between images in the training set and the centroids found by k-means. *Tip*: You can use the method [`transform(X)`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html?highlight=kmeans#sklearn.cluster.KMeans.transform) from your trained k-means estimator to give you distances between all samples in `X` and the cluster centers.\n",
        "4. Plot the 50 representative images and label them \"manually\" (since we have the true labels here, you can take the labels from the y_train array).\n",
        "5. By now you have created a new pool of 50 labeled samples, but carefully chosen (via clustering) to represent well the samples in the full unlabeled dataset. Train a new logistic regression on these 50 representative samples. Does your performance get any better?\n",
        "\n"
      ],
      "metadata": {
        "id": "S8YKLh2Bb3O5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8dXRDQ95fmHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Propagate labels within the clusters\n",
        "Up to this point we've used clustering to select a handful of representative samples that, once labeled, could represent the full data much better than random samples, thus improving classification results. \n",
        "We'll now go a step further and label the remaining unlabeled samples by propagating the labels of the cluster centroids:\n",
        "\n",
        "1. Create a new label array for all the training samples by giving them the same label of their cluster centroid.\n",
        "2. Train a logistic regression model on this new set of labeled samples. Could you improve its accuracy any further? It is probabily not the same as having a fully labeled dataset, but it is lileky better than using only 50 labeled samples!\n",
        "3. Label propagation helps the most when the propagated labels are accurate. Since we have the true labels for this samples, check the accuracy of the propagated labels array. It being accurate explains the accuracy improvement observed just before.\n",
        "\n"
      ],
      "metadata": {
        "id": "RB8LK_BHfk4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OXwJDZS9btEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4: k-means on Olivetti faces dataset\n",
        "The classic Olivetti faces dataset contains 400 grayscale 64 Ã 64âpixel images of faces. Each image is flattened to a 1D vector of size 4,096. 40 different people were photographed (10 times each), and the usual task is to train a model that can predict which person is represented in each picture. \n",
        "\n",
        "1. Load the dataset using the `sklearn.datasets.fetch_olivetti_faces()` function, then split it into a training set, a validation set, and a test set (note that the dataset is already scaled between 0 and 1). \n",
        "Since the dataset is quite small, you probably want to use stratified sampling to ensure that there are the same number of images per person in each set.\n",
        "\n",
        "2. Next, cluster the images using K-Means, and ensure that you have a good number of clusters (using one of the techniques discussed in the k-means tutorial). \n",
        "\n",
        "3. Visualize the clusters: do you see similar faces in each cluster?"
      ],
      "metadata": {
        "id": "pITglRet1z-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_ukWK2AE3Ccl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 5: GMM on Olivetti faces dataset\n",
        "\n",
        "1. Train a Gaussian mixture model on the Olivetti faces dataset. To speed up the algorithm, you should probably reduce the datasetâs dimensionality (e.g., use PCA, preserving 99% of the variance). \n",
        "2. Use the model to generate some new faces (using the sample() method), and visualize them (if you used PCA, you will need to use its inverse_transform() method). \n",
        "3. Try to modify some images (e.g., rotate, flip, darken) and see if the model can detect the anomalies (i.e., compare the output of the score_samples() method for normal images and for anomalies)."
      ],
      "metadata": {
        "id": "V8VBxuuY27-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DbU165-z36wE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 6: Mall customer segmentation\n",
        "Download the following dataset about mall customers, then try to segment these customers into a few groups corresponding to typical spending profiles."
      ],
      "metadata": {
        "id": "Tuf7QX7837UD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://github.com/SteffiPeTaffy/machineLearningAZ/raw/master/Machine%20Learning%20A-Z%20Template%20Folder/Part%204%20-%20Clustering/Section%2025%20-%20Hierarchical%20Clustering/Mall_Customers.csv\""
      ],
      "metadata": {
        "id": "rFIuHeiI-w_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Check which features look most relevant to segmenting this customer base into groups. You can use the seaborn `pairplot` function to visualize all features plotted against each other.\n",
        "2. Apply k-means clustering and hierarchical clustering to the features you selected in the previous step. \n",
        "Do both methods find similar groups? \n",
        "3. What are the main characteristics of the clusters found by each method? Try to visualize the clusters with respect to age, income and spending score.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PhB1IyhAEcZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note*: To plot a dendrogram for hierarchical clustering, you will need to build a linkage matrix and then call scipy's `sp.cluster.hierarchy.dendrogram` function on it. Check the documentation for more details on how to customize your plot.\n",
        "\n",
        "Here is a function that builds the linkage matrix given an `AgglomerativeClustering` model (adapted from [scikit-learn's documentation](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html))."
      ],
      "metadata": {
        "id": "1cJB0qOBKIES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_linkage_matrix(model):\n",
        "    # create the counts of samples under each node\n",
        "    counts = np.zeros(model.children_.shape[0])\n",
        "    n_samples = len(model.labels_)\n",
        "    for i, merge in enumerate(model.children_):\n",
        "        current_count = 0\n",
        "        for child_idx in merge:\n",
        "            if child_idx < n_samples:\n",
        "                current_count += 1  # leaf node\n",
        "            else:\n",
        "                current_count += counts[child_idx - n_samples]\n",
        "        counts[i] = current_count\n",
        "\n",
        "    linkage_matrix = np.column_stack(\n",
        "        [model.children_, model.distances_, counts]\n",
        "    ).astype(float)\n",
        "    return linkage_matrix"
      ],
      "metadata": {
        "id": "Q3tJj9UyELwb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}