{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thalitadru/ml-class-epf/blob/main/LabAssignmentTreesEnsembles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOJg0qAE0-Ql"
      },
      "source": [
        "# Exercices\n",
        "You have 4 exercises in this lab assignment. First two are about ensemble models. Third is about decision trees and the last one, about creating a random forest from individual decision trees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPLM2m6Ph9NR"
      },
      "source": [
        "*Credits:* Based on [code written by A. Géron](https://github.com/ageron/handson-ml2) for his book \"\"Hands-on ML with scikit-learn, keras and tensorflow.\", 2nd edition 2019, O'Reilly Media. Code realeased under [Apache-2.0 License](https://github.com/ageron/handson-ml2/blob/master/LICENSE)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h13UMb-Dh9rl"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import scipy as sp\n",
        "from scipy import stats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXmHwioGiHMB"
      },
      "source": [
        "## Voting Classifier on MNIST\n",
        "### Data\n",
        "This exercise uses the MNIST dataset: a set of 28x28 images containing hadnwritten 0-9 digits. It can be loaded using `sklearn` functions as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayAS552Dui88"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Load data from https://www.openml.org/d/554\n",
        "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgUdHMaUui88"
      },
      "source": [
        "Here the feaures in X are simply all the 784 pixels of any given image, in vectorized form. To visualize the orignal image, the vector must be reshaped back to a 2D array as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9onb8CZui89"
      },
      "outputs": [],
      "source": [
        "plt.imshow(X[0, :].reshape([28, 28]), cmap='Greys')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjxqgeFOui8-"
      },
      "source": [
        "**The goal** is to correctly predict the digit from the image pixels. This is what you have in the y array:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rJOzgxhui8-"
      },
      "outputs": [],
      "source": [
        "y[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w47zu-fWui8_"
      },
      "source": [
        "### Feture range and scaling\n",
        "In general, that pixel values here range in 0-255. However, since digits are always cented, pixels in the center of the image tend to have a larger variance than those at the border. To visualize this, observe the plotting of the stddev for each pixel across all images. See how pixels at the border have near 0 variance while  those at the center have a much large one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEgJQifsui9A"
      },
      "outputs": [],
      "source": [
        "plt.imshow(X.std(axis=0).reshape([28,28]))\n",
        "plt.colorbar()\n",
        "plt.title(\"Standard deviation per image location\\n (i.e. per feature)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJuyEUgLui9A"
      },
      "source": [
        "Remember models trained with continuous optimization (especially 1st order methods) benefit from features having a similar range (because this leads to better conditioned cost functions). For these methods **it is recommended that you apply some form of feature scaling**. For example, after applying standard scaling, variances become mostly equal to 1, meaning all features lie in a similar range:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_4K3U5eui9B"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "plt.imshow(X_scaled.std(axis=0).reshape([28,28]))\n",
        "plt.colorbar()\n",
        "plt.title(\"Standard deviation per image location\\n (i.e. per feature)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYIN8X9Yui9B"
      },
      "source": [
        "### Exercise \n",
        "1. Load the MNIST data and split it into a training set, a validation set, and a test set (e.g., use 50,000 instances for training, 10,000 for validation, and 10,000 for testing). \n",
        "\n",
        "1. Then train various classifiers, such as a Random Forest classifier, an Extra-Trees classifier, and a LinearSVM classifier. \n",
        "\n",
        "1. Next, try to combine them into an ensemble that outperforms each individual classifier on the validation set, using soft or hard voting.\n",
        "\n",
        "1. Once you have found one, try it on the test set. \n",
        " \n",
        "1. How much better does it perform compared to the individual classifiers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8-72W2ViHMQ"
      },
      "source": [
        "## Stacking Ensemble on MNIST\n",
        "1. Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training set with the resulting predictions: each training instance is a vector containing the set of predictions from all your classifiers for an image, and the target is the image’s class.\n",
        "1. Train a classifier on this new training set. Congratulations, you have just trained a blender, and together with the classifiers it forms a stacking ensemble! \n",
        "1. Now evaluate the ensemble on the test set.  For each image in the test set, make predictions with all your classifiers, then feed the predictions to the blender to get the ensemble’s predictions.\n",
        "1. How does it compare to the voting classifier you trained earlier?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSeZPqw_Qv0I"
      },
      "source": [
        "## (Optional) Train and fine-tune a decision tree\n",
        "Train and fine-tune a Decision Tree for the moons dataset by following these steps:  \n",
        "  1. Use `make_moons(n_samples=10000, noise=0.4)` to generate a moons dataset.   \n",
        "  1. Use `train_test_split()` to split the dataset into a training set and a test set.\n",
        "  1. Use grid search with cross-validation (with the help of the `GridSearchCV` class) to find good hyperparameter values for a `DecisionTreeClassifier`. Hint: try various values for `max_leaf_nodes`.\n",
        "  1. Train it on the full training set using these hyperparameters, and measure your model's performance on the test set. You should get roughly 85% to 87% accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4wJJZPrRCzo"
      },
      "source": [
        "## (Optional) Grow your own random forest\n",
        "Grow a forest by following these steps:  \n",
        "  \n",
        "  1. Continuing the previous exercise, generate 1,000 subsets of the training set, each containing 100 instances selected randomly. *Hint*: you can use ScikitLearn’s `ShuffleSplit` class for this.  \n",
        "  1. Train one Decision Tree on each subset, using the best hyperparameter values found in the previous exercise. Evaluate these 1,000 Decision Trees on the test set. Since they were trained on smaller sets, these Decision Trees will likely perform worse than the first Decision Tree, achieving only about 80% accuracy.  \n",
        "  1. Now comes the magic. For each test set instance, generate the predictions of the 1,000 Decision Trees, and keep only the most frequent prediction (you can use SciPy’s `mode()` function for this). This approach gives you majority-vote predictions over the test set.  \n",
        "  1. Evaluate these predictions on the test set: you should obtain a slightly higher accuracy than your first model (about 0.5 to 1.5% higher). Congratulations, you have trained a Random Forest classifier!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "LabAssignment4.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('ml-latest')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "1bede9cc09fadb76754e231ea17b3d1b4d36d88785eed308e26382b97c73c356"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}