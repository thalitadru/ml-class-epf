{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thalitadru/ml-class-epf/blob/main/TutorialEnsembles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEbnaYDJaIyP"
      },
      "source": [
        "# Tutorial on ensemble learning with scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPLM2m6Ph9NR"
      },
      "source": [
        "*Credits:* Based on [code written by A. GÃ©ron](https://github.com/ageron/handson-ml2) for his book \"\"Hands-on ML with scikit-learn, keras and tensorflow.\", 2nd edition 2019, O'Reilly Media. Code realeased under [Apache-2.0 License](https://github.com/ageron/handson-ml2/blob/master/LICENSE)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h13UMb-Dh9rl"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import scipy as sp\n",
        "from scipy import stats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJeMZ8BuLzu5"
      },
      "source": [
        "## Some useful plotting functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlDIc5nliHL1"
      },
      "outputs": [],
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "def plot_data(X, y, ax=None, axes=[-1.5, 2.45, -1, 1.5], alpha=0.5,y_pred=None):\n",
        "    ax = ax or plt.gcf().gca() \n",
        "    # plot data points\n",
        "    ax.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", alpha=alpha)\n",
        "    ax.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", alpha=alpha)\n",
        "    if y_pred is not None:  # plot crosses over misclassified samples\n",
        "        ax.plot(X[:, 0][y != y_pred], X[:, 1][y != y_pred], \"kx\", alpha=alpha,\n",
        "                label='mistakes')\n",
        "    ax.axis(axes)\n",
        "    ax.set_xlabel(r\"$x_1$\", fontsize=18)\n",
        "    ax.set_ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n",
        "\n",
        "def plot_decision_boundary(clf, X, y, ax=None, axes=[-1.5, 2.45, -1, 1.5], alpha=0.5, contour=True):\n",
        "    if ax is None:\n",
        "        ax = plt.gcf().gca()\n",
        "    x1s = np.linspace(axes[0], axes[1], 100)\n",
        "    x2s = np.linspace(axes[2], axes[3], 100)\n",
        "    x1, x2 = np.meshgrid(x1s, x2s)\n",
        "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
        "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
        "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
        "    ax.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
        "    if contour:\n",
        "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
        "        ax.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
        "    # plot data points\n",
        "    y_pred = clf.predict(X)\n",
        "    plot_data(X, y, ax=ax, axes=axes, alpha=alpha, y_pred=y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql_-5ShoJmAs"
      },
      "source": [
        "# A few experiments with ensembles on toy data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8-39XFOiHLv"
      },
      "source": [
        "Let's use the moons dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbiODJ8piHLw"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoidC89aJqed"
      },
      "outputs": [],
      "source": [
        "plot_data(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05_um2JriHLx"
      },
      "source": [
        "## Voting ensemble classifier\n",
        "\n",
        "First we import and instantiate the induvisual classifiers constituting our ensemble:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY2vXhCtiHLx"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
        "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "svm_clf = SVC(gamma=\"scale\", random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmlp7SeJKrcy"
      },
      "source": [
        "Then, we can use [`VotingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier) to combine the individual estimators:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJEH7XKoKhuk"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='hard')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiUbgPfOK5Yw"
      },
      "source": [
        "Calling fit on the voting classifier will train the individual models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FwTrUd2iHLx"
      },
      "outputs": [],
      "source": [
        "voting_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r1lESBvyppR"
      },
      "source": [
        "We can access the individual estimators trained for the ensamble through the attribute `estimators_`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3_DUy23xGs5"
      },
      "outputs": [],
      "source": [
        "voting_clf.estimators_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWWLBlJdiHLy"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "fig, axs = plt.subplots(1, 4, figsize=(20,4))\n",
        "for i, clf in enumerate((log_clf, rnd_clf, svm_clf, voting_clf)):\n",
        "    # fit the model\n",
        "    clf.fit(X_train, y_train)\n",
        "    # compute predicitons on test set\n",
        "    y_pred = clf.predict(X_test)\n",
        "    # calculate accuracy on test set\n",
        "    test_acc = accuracy_score(y_test, y_pred)\n",
        "    # plot decision boundaries and accuracy score for each model\n",
        "    ax = axs[i]\n",
        "    plot_decision_boundary(clf, X_train, y_train, ax=ax)\n",
        "    ax.set_title(clf.__class__.__name__ + \n",
        "                 f\"\\ntest accuracy: {test_acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fe1aM-1yrBm"
      },
      "source": [
        "### Diversity within the ensemble\n",
        "The crosses at the previous plots mark misclassified samples. Notice how the models make different mistakes.\n",
        "\n",
        "The code bellow shows the ids of the misclassified samples for each model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgZL0lEuxJ9n"
      },
      "outputs": [],
      "source": [
        "individual_miss = set()\n",
        "for model in voting_clf.estimators_:\n",
        "    y_pred = model.predict(X_train)\n",
        "    miss_ids = np.arange(X_train.shape[0])[y_pred != y_train]\n",
        "    individual_miss = individual_miss.union(set(miss_ids))\n",
        "    print(f\"Mistakes on training set for model {model.__class__.__name__}: \\nsamples {miss_ids}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Odngmjv0g9v"
      },
      "source": [
        "The ensemble model allows to combine stengths and compensate weakeness of the composing models, finally making less mistakes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce4l-YTyy9ax"
      },
      "outputs": [],
      "source": [
        "y_pred = voting_clf.predict(X_train)\n",
        "miss_ids = np.arange(X_train.shape[0])[y_pred != y_train]\n",
        "ensemble_miss = set(miss_ids)\n",
        "\n",
        "print(f'union of individual mistakes: {len(individual_miss)} samples\\n {sorted(individual_miss)}')\n",
        "print(f'ensemble mistakes: {len(ensemble_miss)} samples\\n {sorted(ensemble_miss)}' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHfGOx_h5qyp"
      },
      "source": [
        "Here you can see the predictions of each individual model for all the training samples over which one of them made mistakes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzrH2w4a1aLr"
      },
      "outputs": [],
      "source": [
        "ids = np.array(sorted(individual_miss))\n",
        "df = pd.DataFrame(index=ids)\n",
        "df.rename_axis(\"sample_id\", inplace=True)\n",
        "for model in voting_clf.estimators_:\n",
        "    y_pred = model.predict(X_train[ids])\n",
        "    df[model.__class__.__name__] = y_pred\n",
        "df[voting_clf.__class__.__name__] = voting_clf.predict(X_train[ids])\n",
        "df[\"true class\"] = y_train[ids]\n",
        "fig, ax = plt.subplots(1,1, figsize=(16,3))\n",
        "sns.heatmap(df.T, cmap='coolwarm', ax=ax);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQLDjL2h6GHa"
      },
      "source": [
        "The prediction output by the voting ensamble is computed by simple majority voting, as verified by the assert statement bellow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wEIrSv72QYF"
      },
      "outputs": [],
      "source": [
        "df['Majority'] = df.aggregate(func=lambda x: stats.mode(x)[0], axis=1)\n",
        "assert((df['VotingClassifier'] == df['Majority']).all())\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Elq-ZTzNLMXx"
      },
      "source": [
        "### Soft-Voting \n",
        "The same class `VotingClassifier` can be used to combine estimators through a weighted voting method. \n",
        "In this case, the voting is done by averaging the predicted class distributions of each individual classifier.\n",
        "\n",
        "Do achieve it, just pass the argument `voting='soft'` to the constructor: \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ge9fAgBp4DYS"
      },
      "outputs": [],
      "source": [
        "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
        "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "svm_clf = SVC(gamma=\"scale\", probability=True, random_state=42)\n",
        "\n",
        "voting_clf_soft = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='soft')\n",
        "voting_clf_soft.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lufFDn8TiHLz"
      },
      "outputs": [],
      "source": [
        "voting_clf_soft.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "fig, axs = plt.subplots(1, 5, figsize=(25,4))\n",
        "for i, clf in enumerate((log_clf, rnd_clf, svm_clf, voting_clf, voting_clf_soft)):\n",
        "    # fit the model\n",
        "    clf.fit(X_train, y_train)\n",
        "    # compute predicitons on test set\n",
        "    y_pred = clf.predict(X_test)\n",
        "    # calculate accuracy on test set\n",
        "    test_acc = accuracy_score(y_test, y_pred)\n",
        "    # plot decision boundaries and accuracy score for each model\n",
        "    ax = axs[i]\n",
        "    plot_decision_boundary(clf, X_train, y_train, ax=ax)\n",
        "    ax.set_title(clf.__class__.__name__ + \n",
        "                 f\"\\ntest accuracy: {test_acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-F7z0jFAQrJ"
      },
      "source": [
        "Here are some plots to visualize the predicted probabilities for each indivisual classifier. We plot the predicted probas of 20 samples misclassified by at least one of the models. \n",
        "\n",
        "Note that if we average them manually, we obtain the same response as the `VotingClassifier` we just trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BBT8IPV01Zv"
      },
      "outputs": [],
      "source": [
        "# Gathering samples misclassified by the base models\n",
        "individual_miss = set()\n",
        "for model in voting_clf_soft.estimators_:\n",
        "    y_pred = model.predict(X_train)\n",
        "    miss_ids = np.arange(X_train.shape[0])[y_pred != y_train]\n",
        "    individual_miss = individual_miss.union(set(miss_ids))\n",
        "\n",
        "# We'll focus on the first 20 samples for this visualization\n",
        "ids = np.array(sorted(individual_miss))[:20]\n",
        "\n",
        "# Here we collect all predicted probas for each model and the ensemble\n",
        "pred_probas_df = pd.DataFrame(index=ids).rename_axis('sample id')\n",
        "for i, model in enumerate(voting_clf_soft.estimators_+[voting_clf_soft]):\n",
        "    # Here we'll manually compute the predicted probabilities \n",
        "    # and save them to y_pred_agg\n",
        "    y_pred_proba = model.predict_proba(X_train[ids])\n",
        "    pred_probas_df[str(model.__class__.__name__)] = y_pred_proba[:,1]\n",
        "\n",
        "# We inclde manually averaged probas into the dataframe\n",
        "pred_probas_df[\"Average probas\"] = pred_probas_df.iloc[:,0:3].mean(axis=1)\n",
        "pred_probas_df[\"true class\"] = y_train[ids]\n",
        "\n",
        "# now we plot it all\n",
        "fig, ax = plt.subplots(1,1, figsize=(15,4))\n",
        "sns.heatmap(pred_probas_df.T, vmin=0, vmax=1, cmap='coolwarm', \n",
        "            annot=True, ax=ax);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlGcPwV7pUL5"
      },
      "source": [
        "## Stacking ensemble\n",
        "Stacking ensembles or model stacking is the practice of learning multiple estimators and combine them in a smarter way. Instead of simply doing voting or averaging the predictions, we train a model to combine this estimators on separate hold-out set. This model is sometimes called a *blender* model or a *meta-learner*.\n",
        "\n",
        "Here is an example using a linear model as a blender. First we need to separate a hold-out split from our training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGhGhS4crRnT"
      },
      "outputs": [],
      "source": [
        "X_train_train, X_train_blend, y_train_train, y_train_blend  = train_test_split(X_train, y_train, test_size=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLcWKyWgscqw"
      },
      "source": [
        "Now we train the estimators composing our ensemble on the training subset **without the held-out split**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XDKtWkhrvdR"
      },
      "outputs": [],
      "source": [
        "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
        "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "svm_clf = SVC(gamma=\"scale\", random_state=42)\n",
        "\n",
        "for clf in (log_clf, rnd_clf, svm_clf):\n",
        "    clf.fit(X_train_train, y_train_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ8V32Ipsrm-"
      },
      "source": [
        "Now we are going to use the hold-out split predictions of each individual estimator on the held-out set as training data for oure blending model.\n",
        "\n",
        "First, let us gather this data by making predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgKKcOFprvWA"
      },
      "outputs": [],
      "source": [
        "preds = {}\n",
        "for clf in (log_clf, rnd_clf, svm_clf):\n",
        "    preds[clf.__class__.__name__] = clf.predict(X_train_blend)\n",
        "df = pd.DataFrame(preds)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58rg3yxDuR7z"
      },
      "source": [
        "This data can now be used to train our linear blender (we will use a logistic regression since we are doing classification). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSMfAmjEuRvk"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "blender = LogisticRegression(random_state=42)\n",
        "blender.fit(df, y_train_blend)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVHdmOxdvNJe"
      },
      "source": [
        "After fitting, the model has learned intercept and coeficients to combine the predictions of composing estimators:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-kC_wKTuGJ7"
      },
      "outputs": [],
      "source": [
        "blender.coef_, blender.intercept_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEdwG_iawfyE"
      },
      "source": [
        "Finally, we can combine models and blender to compute predictions and the accuracy score on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XF2zMlA1v9cG"
      },
      "outputs": [],
      "source": [
        "preds = {}\n",
        "for clf in (log_clf, rnd_clf, svm_clf):\n",
        "    preds[clf.__class__.__name__] = clf.predict(X_test)\n",
        "df = pd.DataFrame(preds)\n",
        "blender.score(df, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNmeZNGkwuWZ"
      },
      "source": [
        "This performance is not realy better than that of the voting ensemble. There are some factors that could be at cause:\n",
        "- stacking classifiers works better when combining the predictied probabilities instead of class labels. \n",
        "- the size of the held-out dataset may not be big enough to generate sufficient samples for training the blender. \n",
        "- the individual estimators were trained on a smaller training set. \n",
        "- the linear model is not complex enough to learn a good blending function\n",
        "\n",
        "We could train the estimators on the full training set, and do the hold-out split directly on the prediction data which is used to train the final model. \n",
        "More over, we can use cross-validation to generate multiple hold-out splits and train the blender with better confidence.\n",
        "We can also try to use a more complex model as a blender."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcTylUjRvfal"
      },
      "source": [
        "All of this can be achieved using the classes [`Stackingclassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html) or [`StackingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html). They will train the estimators on the full training data and train the final blender using cross validation.\n",
        "\n",
        "The default blender for classification is `LogisticRegression``. Notice how we get a similar score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diPTbis4vcZL"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "stack = StackingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    cv=5)\n",
        "stack.fit(X_train, y_train)\n",
        "stack.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql6qryjFy45a"
      },
      "outputs": [],
      "source": [
        "stack.final_estimator_.coef_, stack.final_estimator_.intercept_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlSbBRGA2nyg"
      },
      "source": [
        "Now let's  try again with a more complex blender model: `RandomForestClassifier`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykSYfo7S19Yg"
      },
      "outputs": [],
      "source": [
        "stack = StackingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    final_estimator=RandomForestClassifier(n_estimators=100, random_state=42), cv=5)\n",
        "stack.fit(X_train, y_train)\n",
        "stack.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7L9apAg6PmHJ"
      },
      "source": [
        "## Bagging\n",
        "\n",
        "Bagging classifiers can be created with `BaggingClassifier`. We need to pass another base estimator that will be fit by the constructor. Here is an example using decision trees:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPFrlP0TiHL0"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(), n_estimators=500,\n",
        "    max_samples=100, bootstrap=True, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNh01Mu2P_6u"
      },
      "source": [
        "When we call `fit` on the `BaggingClassifier` object, it will fit `n_estimator=500` trees on boostrapped subsets of the training data :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZEJ_uXZP82A"
      },
      "outputs": [],
      "source": [
        "bag_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ5p8DVJRWLy"
      },
      "source": [
        "Now we can use the bagging ensemble to make predictions on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xGzto9xiHL0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "print(\"Test accuracy for the bagged ensemble of decision trees\", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohHEF106u8TN"
      },
      "source": [
        "We can access the individual estimators trained for the ensamble through the attribute `estimators_`. This box plot sumarizes the test performance of the 500 individual trees. As a reference, the green line marks the test performance of the bagged ensemble of these same trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rma7B-qvERJ"
      },
      "outputs": [],
      "source": [
        "test_acc = []\n",
        "for estim in bag_clf.estimators_:\n",
        "    test_acc.append(estim.score(X_test, y_test))\n",
        "plt.boxplot(test_acc);\n",
        "plt.xticks([1],['accuracy of individual trees'])\n",
        "plt.hlines(bag_clf.score(X_test, y_test), 0.5, 1.5, color='g', label='Accuracy of\\nbagged ensemble')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNKxRNZ8Rt_Q"
      },
      "source": [
        "Here is an example comparison of a single decision tree against the bagged ensemble we just trained:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A28q9CZciHL0"
      },
      "outputs": [],
      "source": [
        "tree_clf = DecisionTreeClassifier(random_state=42)\n",
        "tree_clf.fit(X_train, y_train)\n",
        "y_pred_tree = tree_clf.predict(X_test)\n",
        "print(\"Test accuracy for a single decision tree\", accuracy_score(y_test, y_pred_tree))\n",
        "\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "print(\"Test accuracy for the bagged ensemble of decision trees\", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuRcP_XjiHL1",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "fix, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True)\n",
        "plt.sca(axes[0])\n",
        "plot_decision_boundary(tree_clf, X, y)\n",
        "plt.title(\"Decision Tree\", fontsize=14)\n",
        "plt.sca(axes[1])\n",
        "plot_decision_boundary(bag_clf, X, y)\n",
        "plt.title(\"Decision Trees with Bagging\", fontsize=14)\n",
        "plt.ylabel(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luQ0eErAS1FD"
      },
      "source": [
        "### Out-of-bag evaluation\n",
        "\n",
        "Notice that in bagging, since each classifier is trained on a subsample fo the training set, there are always some unused training samples, that we call *out-of-bag*.\n",
        "These samples can be used to evaluate the generalization of that individual classifier: this is called the *out-of-bag score* (or oob score) .\n",
        "\n",
        "Averaging out-of-bag scores for all estimators can be a shortcut to estimating test performance without having to hold out and evaluate on separate validation set.\n",
        "\n",
        "We can ask Scikit-learn to compute this average oob score as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPDy0x4QiHL2"
      },
      "outputs": [],
      "source": [
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(), n_estimators=500,\n",
        "    bootstrap=True, oob_score=True, random_state=40)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "bag_clf.oob_score_.round(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtNliYULVB--"
      },
      "source": [
        "Notice it is not too far from the test performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WUuQEqBVCbR"
      },
      "outputs": [],
      "source": [
        "bag_clf.score(X_test, y_test).round(3)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNPDjZUP1nWPGb3wyEVm4Uc",
      "include_colab_link": true,
      "name": "LabSession4.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('ml-latest')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "1bede9cc09fadb76754e231ea17b3d1b4d36d88785eed308e26382b97c73c356"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
