{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thalitadru/ml-class-epf/blob/main/TutorialBoosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPLM2m6Ph9NR"
      },
      "source": [
        "*Credits:* Based on [code written by A. GÃ©ron](https://github.com/ageron/handson-ml2) for his book \"\"Hands-on ML with scikit-learn, keras and tensorflow.\", 2nd edition 2019, O'Reilly Media. Code realeased under [Apache-2.0 License](https://github.com/ageron/handson-ml2/blob/master/LICENSE)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzutZ7nBiHL6"
      },
      "source": [
        "\n",
        "# Boosting Tutorial\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h13UMb-Dh9rl"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import scipy as sp\n",
        "from scipy import stats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6xo4twnHb6K"
      },
      "source": [
        "## AdaBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJeMZ8BuLzu5"
      },
      "source": [
        "### Some useful plotting functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlDIc5nliHL1"
      },
      "outputs": [],
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "def plot_data(X, y, ax=None, axes=[-1.5, 2.45, -1, 1.5], alpha=0.5,):\n",
        "    if ax is None:\n",
        "        ax = plt.gcf().gca()\n",
        "    ax.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", alpha=alpha)\n",
        "    ax.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", alpha=alpha)\n",
        "    ax.axis(axes)\n",
        "    ax.set_xlabel(r\"$x_1$\", fontsize=18)\n",
        "    ax.set_ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n",
        "\n",
        "def plot_decision_boundary(clf, X, y, ax=None, axes=[-1.5, 2.45, -1, 1.5], alpha=0.5, contour=True):\n",
        "    if ax is None:\n",
        "        ax = plt.gcf().gca()\n",
        "    x1s = np.linspace(axes[0], axes[1], 100)\n",
        "    x2s = np.linspace(axes[2], axes[3], 100)\n",
        "    x1, x2 = np.meshgrid(x1s, x2s)\n",
        "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
        "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
        "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
        "    ax.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
        "    if contour:\n",
        "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
        "        ax.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
        "    plot_data(X, y, ax=ax, axes=axes, alpha=alpha)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql_-5ShoJmAs"
      },
      "source": [
        "### Some toy data to demonstrate the algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8-39XFOiHLv"
      },
      "source": [
        "Let's use the moons dataset for binary classification:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbiODJ8piHLw"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoidC89aJqed"
      },
      "outputs": [],
      "source": [
        "plot_data(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivXBzXzzwzAq"
      },
      "source": [
        "### AdaBoost using SVM as a base model\n",
        "\n",
        "This example implements AdaBoost manually, using SVM classifiers as a base model.\n",
        "\n",
        "At the end, we plot the decision function corrsponding to the final mode, along with the decision boundaries of each of the 5 models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWUK3OMvw-sw"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "m = X_train.shape[0] # n samples\n",
        "\n",
        "# Initialize sample weights\n",
        "sample_weights = np.ones(m) / m\n",
        "\n",
        "for i in range(5):\n",
        "    # Let us fit a new learner Fm\n",
        "    svm_clf = SVC(kernel=\"rbf\", C=0.2, gamma=0.6, random_state=42)\n",
        "    # We can pass the weight samples to the fit call so that\n",
        "    # they get applied in the loss function\n",
        "    svm_clf.fit(X_train, y_train, sample_weight=sample_weights * m)\n",
        "    y_pred = svm_clf.predict(X_train)\n",
        "\n",
        "    # Once Fm is fit, we compute its normalized error\n",
        "    r = sample_weights[y_pred != y_train].sum() / sample_weights.sum()\n",
        "    # Then we compute the model weight alpha\n",
        "    alpha = np.log((1 - r) / r)\n",
        "    # Now we update the sample wheights only for misclassified samples\n",
        "    sample_weights[y_pred != y_train] *= np.exp(alpha) \n",
        "    # Before we continue, we renormalize the weights\n",
        "    sample_weights /= sample_weights.sum() # normalization step\n",
        "\n",
        "    plot_decision_boundary(svm_clf, X, y, alpha=0.2)\n",
        "\n",
        "plt.text(-0.75, -0.95, \"1\", fontsize=14)\n",
        "plt.text(-1.05, -0.95, \"2\", fontsize=14)\n",
        "plt.text(1.0, -0.95, \"3\", fontsize=14)\n",
        "plt.text(-1.45, -0.5, \"4\", fontsize=14)\n",
        "plt.text(1.36,  -0.95, \"5\", fontsize=14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNADgN5L1exh"
      },
      "source": [
        "#### Adding a learning rate\n",
        "As in any boosting algorithm, AdaBoost combines models additively. Each model has an associated weight $\\alpha_m$ that can be adjusted according to its mistakes on the training set. \n",
        "$$f_m(x) = f_{m-1}(x)+\\alpha_m F_m(x)$$\n",
        "\n",
        "From the decision function plot we observe that not all boundaries are very helpful in defining an accurate decision function.\n",
        "In particular, the boundaries learned by the later models (3, 4 and 5) are quite distant from where the ideal boundary should be. This is normal and is due to these later models focusing more on the few samples misclassified by the previous models. \n",
        "\n",
        "However, it would be better to give these models less weight in the model combination. This can be achieved by including a learning rate hyperparameter, as follows.\n",
        "Along the model weight $\\alpha_m$, the algorithm can adjusted to include a learning rate hyperparameter $\\nu$ that further regulates how much each new model $F_m$ will impact the final decision function:\n",
        "\n",
        "$$f_m(x) = f_{m-1}(x)+\\nu\\alpha_m F_m(x)$$\n",
        "\n",
        "\n",
        "When this value is small (typically lower than 1), it is also known as *shrinkage factor*, as it reduces the influence of new models $F_m$ being learned.\n",
        "\n",
        "\n",
        "This next example implements the same algorithm, but tries out a learning rate of 0.5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBvFdw5_iHL7"
      },
      "outputs": [],
      "source": [
        "m = len(X_train)\n",
        "\n",
        "fix, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True)\n",
        "for subplot, learning_rate in ((0, 1), (1, 0.5)):\n",
        "    sample_weights = np.ones(m) / m\n",
        "    plt.sca(axes[subplot])\n",
        "    for i in range(5):\n",
        "        # Let us fit a new learner Fm\n",
        "        svm_clf = SVC(kernel=\"rbf\", C=0.2, gamma=0.6, random_state=42)\n",
        "        svm_clf.fit(X_train, y_train, sample_weight=sample_weights * m)\n",
        "        y_pred = svm_clf.predict(X_train)\n",
        "\n",
        "        # Once Fm is fit, we compute its normalized error\n",
        "        r = sample_weights[y_pred != y_train].sum() / sample_weights.sum()\n",
        "        # Then we compute the model weight alpha\n",
        "        alpha = learning_rate * np.log((1 - r) / r)\n",
        "        # Now we update the sample wheights only for misclassified samples\n",
        "        sample_weights[y_pred != y_train] *= np.exp(alpha) \n",
        "        # Before we continue, we renormalize the weights\n",
        "        sample_weights /= sample_weights.sum() # normalization step\n",
        "\n",
        "        plot_decision_boundary(svm_clf, X, y, alpha=0.2)\n",
        "        plt.title(\"learning_rate = {}\".format(learning_rate), fontsize=16)\n",
        "    if subplot == 0:\n",
        "        plt.text(-0.75, -0.95, \"1\", fontsize=14)\n",
        "        plt.text(-1.05, -0.95, \"2\", fontsize=14)\n",
        "        plt.text(1.0, -0.95, \"3\", fontsize=14)\n",
        "        plt.text(-1.45, -0.5, \"4\", fontsize=14)\n",
        "        plt.text(1.36,  -0.95, \"5\", fontsize=14)\n",
        "    else:\n",
        "        plt.ylabel(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv1erVF7-jO4"
      },
      "source": [
        "Notice how the decision boundaries are now less deviant from the class frontier and how the final decision function is sharper around it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsJ5zgvuI7Wj"
      },
      "source": [
        "### AdaBoost with decision trees on Scikit-Learn\n",
        "\n",
        "A common version of AdaBoost is to use very short decision trees as a base model, with only one level bellow the root node. These short trees are cften called *decision stumps*. \n",
        "\n",
        "Here is an example implementing AdaBoost with trees of depth 1 with Scikit-learn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTm5Ypp1iHL6"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1), n_estimators=5,\n",
        "    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42)\n",
        "ada_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NT8068GXiHL7"
      },
      "outputs": [],
      "source": [
        "plot_decision_boundary(ada_clf, X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB49iJO-_aOH"
      },
      "source": [
        "#### About the number of estimators\n",
        "Since the base models are much weaker than SVM, it is normal that we should add many more to the ensemble to get a more accurate classification. Let's try again with more trees:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9t6FZl3_qKd"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1,4,figsize=(20,4))\n",
        "for i, M in enumerate([5, 50, 200, 500]):\n",
        "    ada_clf = AdaBoostClassifier(\n",
        "        DecisionTreeClassifier(max_depth=1), n_estimators=M,\n",
        "        algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42)\n",
        "    ada_clf.fit(X_train, y_train)\n",
        "    train_score = ada_clf.score(X_train, y_train)\n",
        "    test_score = ada_clf.score(X_test, y_test)\n",
        "    plot_decision_boundary(ada_clf, X, y, ax=axs[i])\n",
        "    plt.sca(axs[i])\n",
        "    plt.title(f'n_estimators={M}\\ntrain accuracy={train_score:0.2f}\\ntest accuracy={test_score:0.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AstgOnMOBDTl"
      },
      "source": [
        "Notice how increasing the number of estimators can make the model more accurate, but a too large number increases the risk of overfitting (see tha case with 500 estimators)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb2prcUfso5S"
      },
      "source": [
        "#### About the algorithm\n",
        "The algorithm SAMME is a generalization of the orginal AdaBoost for multi-class problems. It is equivalent to Binary AdaBoost when the number of classes is 2 (as is the case here).\n",
        "SAMME.R stands for SAMME Real and is a variant that combines the predicted probabilities for all classes, instead of just combining the final predictions.\n",
        "\n",
        "Here is the same comparison as before, but using simple SAMME as the fitting algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-A6nRoj3HsoA"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1,4,figsize=(20,4))\n",
        "for i, M in enumerate([5, 50, 200, 500]):\n",
        "    ada_clf = AdaBoostClassifier(\n",
        "        DecisionTreeClassifier(max_depth=1), n_estimators=M,\n",
        "        algorithm=\"SAMME\", learning_rate=0.5, random_state=42)\n",
        "    ada_clf.fit(X_train, y_train)\n",
        "    train_score = ada_clf.score(X_train, y_train)\n",
        "    test_score = ada_clf.score(X_test, y_test)\n",
        "    plot_decision_boundary(ada_clf, X, y, ax=axs[i])\n",
        "    plt.sca(axs[i])\n",
        "    plt.title(f'n_estimators={M}\\ntrain accuracy={train_score:0.2f}\\ntest accuracy={test_score:0.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhBXQZAdiHL7"
      },
      "source": [
        "## Gradient Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeIXG2JTiHL8"
      },
      "source": [
        "### A simple example of Gradient Boosted Regression Trees\n",
        "We will start demonstrating gradient boosting in a toy regression problem. \n",
        "Let's create a simple quadratic dataset with a single feature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9OEI3HiiHL8"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) - 0.5\n",
        "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)\n",
        "\n",
        "plt.scatter(X,y)\n",
        "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
        "plt.xlabel(\"$x_1$\", fontsize=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HscMM8XHiHL8"
      },
      "source": [
        "Now let's train a decision tree regressor on this dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_mvhyntiHL8"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
        "tree_reg1.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u733Qs5DCQ1N"
      },
      "source": [
        "In gradient boosting we fit the learners $F_m$ to the residuals. That is, we fit:\n",
        "$$F_m = \\text{argmin}_F (r_m-F)^2$$\n",
        "\n",
        "From theory, the residuals should be the negative of the gradient of $\\mathcal{L}$ wrt $f$. For the squared error loss, we have seen that the negative gradient is \n",
        "$$r_m = \\frac{\\partial \\mathcal{L}}{\\partial f(x)} = y-f(x)$$\n",
        "\n",
        "The current $f(x)=F_1(x)$ is implemented by `tree_reg1`, so that we can compute the residuals by $$r_2 = y - F_1(x)$$ and then fit $F_2$ as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8Tfi9J7iHL8"
      },
      "outputs": [],
      "source": [
        "r2 = y - tree_reg1.predict(X)\n",
        "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
        "tree_reg2.fit(X, r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPem2mvYFm_j"
      },
      "source": [
        "Now we fit another model $F_3$ to the residuals of the current model \n",
        "$$f_2(x) = F_1(x) + F_2(x)$$\n",
        "\n",
        "We can compute $r_3$ as a function of $r_2$ as follows:\n",
        "$$r_3 = y - f_2(x) = y - F_1(x) - F_2(x) = r_2 - F_2(x)$$\n",
        "\n",
        "Then we can fit $F_3$ to approximate it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVrX-aFbiHL9"
      },
      "outputs": [],
      "source": [
        "r3 = r2 - tree_reg2.predict(X)\n",
        "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
        "tree_reg3.fit(X, r3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMOTQ82pGcg-"
      },
      "source": [
        "To compute the final model, we sum the predictions of all 3 trees. Here is an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLVtS_KbiHL9"
      },
      "outputs": [],
      "source": [
        "X_new = np.array([[0.8]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ge5aCX3-iHL9"
      },
      "outputs": [],
      "source": [
        "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NNQErmxiHL9"
      },
      "outputs": [],
      "source": [
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5qsadmbiHL9"
      },
      "source": [
        "The following plots show models learned on each iteration, alongside the ensamble model at that iteration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S49cnChniHL9"
      },
      "outputs": [],
      "source": [
        "def plot_predictions(regressors, X, y, axes, label=None, style=\"r-\", data_style=\"b.\", data_label=None):\n",
        "    x1 = np.linspace(axes[0], axes[1], 500)\n",
        "    y_pred = sum(regressor.predict(x1.reshape(-1, 1)) for regressor in regressors)\n",
        "    plt.plot(X[:, 0], y, data_style, label=data_label)\n",
        "    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n",
        "    if label or data_label:\n",
        "        plt.legend(loc=\"upper center\", fontsize=16)\n",
        "    plt.axis(axes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oBC54syiHL-"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(11,11))\n",
        "\n",
        "plt.subplot(321)\n",
        "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$F_1(x_1)$\", style=\"g-\", data_label=\"Training set\")\n",
        "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
        "plt.title(\"Residuals and tree predictions\", fontsize=16)\n",
        "\n",
        "plt.subplot(322)\n",
        "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$f(x_1) = F_1(x_1)$\", data_label=\"Training set\")\n",
        "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
        "plt.title(\"Ensemble predictions\", fontsize=16)\n",
        "\n",
        "plt.subplot(323)\n",
        "plot_predictions([tree_reg2], X, r2, axes=[-0.5, 0.5, -0.5, 0.5], label=\"$F_2(x_1)$\", style=\"g-\", data_style=\"k+\", data_label=\"Residuals\")\n",
        "plt.ylabel(\"$y - F_1(x_1)$\", fontsize=16)\n",
        "\n",
        "plt.subplot(324)\n",
        "plot_predictions([tree_reg1, tree_reg2], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$f(x_1) = F_1(x_1) + F_2(x_1)$\")\n",
        "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
        "\n",
        "plt.subplot(325)\n",
        "plot_predictions([tree_reg3], X, r3, axes=[-0.5, 0.5, -0.5, 0.5], label=\"$F_3(x_1)$\", style=\"g-\", data_style=\"k+\")\n",
        "plt.ylabel(\"$y - F_1(x_1) - F_2(x_1)$\", fontsize=16)\n",
        "plt.xlabel(\"$x_1$\", fontsize=16)\n",
        "\n",
        "plt.subplot(326)\n",
        "plot_predictions([tree_reg1, tree_reg2, tree_reg3], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$f(x_1) = F_1(x_1) + F_2(x_1) + F_3(x_1)$\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=16)\n",
        "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf90gk4XiHL-"
      },
      "source": [
        "### Using scikit-learn to implement Gradient Boosted Trees\n",
        "\n",
        "We can use scikit-learn's model [`GradientBoostingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) to implement the same models more easily.\n",
        "\n",
        "Here is an example that implements the same model we just did before, using 3 trees with max-depth of 2. Note that we have combined all models with equal weight. Implicitly, we have used a learning rate of 1.0 to combine them. We can also inform this as a hyperparameter to the model constructor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFXHLnRziHL-"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)\n",
        "gbrt.fit(X, y)\n",
        "\n",
        "plot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"Ensemble predictions\")\n",
        "plt.title(\"learning_rate={}, n_estimators={}\".format(gbrt.learning_rate, gbrt.n_estimators), fontsize=14)\n",
        "plt.xlabel(\"$x_1$\", fontsize=16)\n",
        "plt.ylabel(\"$y$\", fontsize=16, rotation=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbMsnFlELJa2"
      },
      "source": [
        "#### About the number of estimators and the learning rate\n",
        "\n",
        "Again, as in AdaBoost, the number of estimators is an important hyperparameter to balance under and overfitting. To few and the ensemble underfits, to many and the ensemble model can overfit.\n",
        "\n",
        "Here is an example on the same regression data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjfmZUplL5qe"
      },
      "outputs": [],
      "source": [
        "gbrt_many = GradientBoostingRegressor(max_depth=2, n_estimators=200, learning_rate=1.0, random_state=42)\n",
        "gbrt_many.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHQGxUtoL_B7"
      },
      "outputs": [],
      "source": [
        "fix, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True)\n",
        "\n",
        "plt.sca(axes[0])\n",
        "plot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"Ensemble predictions\")\n",
        "plt.title(\"learning_rate={}, n_estimators={}\".format(gbrt.learning_rate, gbrt.n_estimators), fontsize=14)\n",
        "plt.xlabel(\"$x_1$\", fontsize=16)\n",
        "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
        "\n",
        "plt.sca(axes[1])\n",
        "plot_predictions([gbrt_many], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n",
        "plt.title(\"learning_rate={}, n_estimators={}\".format(gbrt_many.learning_rate, gbrt_many.n_estimators), fontsize=14)\n",
        "plt.xlabel(\"$x_1$\", fontsize=16)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPCMvBdXMIZI"
      },
      "source": [
        "When using many estimators, reducing the learning rate can be an effective way of regularizing the ensemble model by limiting the influence of each individual learner in the final prediction.\n",
        "\n",
        "Here is the same example seen before, but with the learning rate dropped to 0.5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wA6lsfHAiHL-"
      },
      "outputs": [],
      "source": [
        "gbrt_slow = GradientBoostingRegressor(max_depth=2, n_estimators=200, learning_rate=0.1, random_state=42)\n",
        "gbrt_slow.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoDVBIvOiHL-"
      },
      "outputs": [],
      "source": [
        "fix, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True)\n",
        "\n",
        "plt.sca(axes[0])\n",
        "plot_predictions([gbrt_many], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"Ensemble predictions\")\n",
        "plt.title(\"learning_rate={}, n_estimators={}\".format(gbrt_many.learning_rate, gbrt_many.n_estimators), fontsize=14)\n",
        "plt.xlabel(\"$x_1$\", fontsize=16)\n",
        "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
        "\n",
        "plt.sca(axes[1])\n",
        "plot_predictions([gbrt_slow], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n",
        "plt.title(\"learning_rate={}, n_estimators={}\".format(gbrt_slow.learning_rate, gbrt_slow.n_estimators), fontsize=14)\n",
        "plt.xlabel(\"$x_1$\", fontsize=16)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO3UxzFuM0g9"
      },
      "source": [
        "#### Limiting the number of estimators with early stopping\n",
        "\n",
        "Remember that in boosting, models are fit sequentially. Therefore one possible way to limit the number of estimators is to use a validation set to estimate generalization, and piking the number of estimators that minimizes validation error.\n",
        "\n",
        "Here is an example using this method to choose a good number of estimators. First we train a `GradientBoostingRegressor` with many estimators.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oddyk2vRiHL_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=49)\n",
        "\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)\n",
        "gbrt.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiY4V8-wOEF8"
      },
      "source": [
        "Then we can compute predictions at each stage of the boosting process by using the method `staged_predict`. This will give us predictions at each of the 119 boosting iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSwR9OLgODN4"
      },
      "outputs": [],
      "source": [
        "errors = [mean_squared_error(y_val, y_pred)\n",
        "          for y_pred in gbrt.staged_predict(X_val)]\n",
        "print(\"errors computed:\", len(errors))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ech66M7NPoyS"
      },
      "source": [
        "We take the number of estimators corresponding to the minimum of this error array (plus 1 since the first iteration starts at 2 models already)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpMMJdrLPn63"
      },
      "outputs": [],
      "source": [
        "bst_n_estimators = np.argmin(errors) + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZgEnee0P5oV"
      },
      "source": [
        "Here is a plot of this validation error curve with the minimum value indicated by a black line:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYtkOodrPkxW"
      },
      "outputs": [],
      "source": [
        "min_error = np.min(errors)\n",
        "plt.plot(errors, \"b.-\")\n",
        "plt.plot([bst_n_estimators, bst_n_estimators], [0, min_error], \"k--\")\n",
        "plt.plot([0, 120], [min_error, min_error], \"k--\")\n",
        "plt.plot(bst_n_estimators, min_error, \"ko\")\n",
        "plt.text(bst_n_estimators, min_error*1.2, \"Minimum\", ha=\"center\", fontsize=14)\n",
        "plt.axis([0, 120, 0, 0.01])\n",
        "plt.xlabel(\"Number of trees\")\n",
        "plt.ylabel(\"Error\", fontsize=16)\n",
        "plt.title(\"Validation error\", fontsize=14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1cJu2eSQYLr"
      },
      "source": [
        "Now we can train a gradient boosted model with the best number of estimators:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VEYD4KSPSRB"
      },
      "outputs": [],
      "source": [
        "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators, random_state=42)\n",
        "gbrt_best.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpXxKAzgiHL_"
      },
      "outputs": [],
      "source": [
        "plot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n",
        "plt.title(\"Best model (%d trees)\" % bst_n_estimators, fontsize=14)\n",
        "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
        "plt.xlabel(\"$x_1$\", fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vABUw0mOiHL_"
      },
      "source": [
        "#### Early stopping with incremental fit\n",
        "\n",
        "In fact it is not necessary to fit the boosting all the way through to only then evaluate validation errors. We can iteratively add more estimators and continue fitting from the previous model state by using the parameter `warm_start=True`.\n",
        "\n",
        "This way we can loop through an increasing series of values for the `n_estimators` parameter, calling fit and evaluating performances on the validation set.\n",
        "\n",
        "Since we won't be trying values all the way until clear overfitting, it is intereting to include a patience mechanism into our early stopping. \n",
        "That is, once validation error stops improving, we wait a certain number of iterations (say, 5 ) before we stop training.\n",
        "\n",
        "Here is an example implementing incremental fit and early stopping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vvYJKvgiHMA"
      },
      "outputs": [],
      "source": [
        "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)\n",
        "\n",
        "min_val_error = float(\"inf\")\n",
        "error_going_up = 0\n",
        "for n_estimators in range(1, 120):\n",
        "    # Increase the number of estimators\n",
        "    gbrt.n_estimators = n_estimators\n",
        "    # Fit will be incremental due to warm_start=True\n",
        "    gbrt.fit(X_train, y_train)\n",
        "    # Now compute validation error and check early stopping criteria\n",
        "    y_pred = gbrt.predict(X_val)\n",
        "    val_error = mean_squared_error(y_val, y_pred)\n",
        "    if val_error < min_val_error:\n",
        "        min_val_error = val_error\n",
        "        error_going_up = 0\n",
        "    else:\n",
        "        error_going_up += 1\n",
        "        if error_going_up == 5:\n",
        "            break  # early stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quEhUxsAiHMA"
      },
      "outputs": [],
      "source": [
        "print(gbrt.n_estimators)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBeGsnK3S0mG"
      },
      "outputs": [],
      "source": [
        "plot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n",
        "plt.title(\"Best model (%d trees)\" % gbrt.n_estimators, fontsize=14)\n",
        "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
        "plt.xlabel(\"$x_1$\", fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZOI63zAiHMA"
      },
      "outputs": [],
      "source": [
        "print(\"Minimum validation MSE:\", min_val_error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpHGZ2W2iHMA"
      },
      "source": [
        "### Using XGBoost\n",
        "\n",
        "XGBoost is a popular version of gradient boosted trees that includes many optmizations and adds many regularization options to the basic Gradient Boosted Trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnyfYQ7-iHMA"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import xgboost\n",
        "except ModuleNotFoundError:\n",
        "    !pip install xgboost\n",
        "    import xgboost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04yblp3WVslp"
      },
      "source": [
        "It has a scikit-learn-like API, as you can check in the documentation for `XGBRegressor`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPgmj9klWvmr"
      },
      "outputs": [],
      "source": [
        "?xgboost.XGBRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUyMDwWUW-_3"
      },
      "source": [
        "Full documentation for all parameters can be found [here](https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst).\n",
        "\n",
        "\n",
        "Here is an example usgin XGBoost to solve the same regression problem we have been solving with scikit-learn's GBRT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrCPQyTviHMA"
      },
      "outputs": [],
      "source": [
        "xgb_reg = xgboost.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "xgb_reg.fit(X_train, y_train)\n",
        "y_pred = xgb_reg.predict(X_val)\n",
        "val_error = mean_squared_error(y_val, y_pred)\n",
        "print(\"Validation MSE:\", val_error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anhw_mTEWe_1"
      },
      "source": [
        "#### Early stopping on XGBoost\n",
        "In XGBoost you can inform a validation set to the `fit` function in order to implement early stopping.\n",
        "\n",
        "The patience parameter is called `early_stopping_rounds`.\n",
        "\n",
        "Here is an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thkIBsmPiHMB"
      },
      "outputs": [],
      "source": [
        "xgb_reg.fit(X_train, y_train,\n",
        "            eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
        "y_pred = xgb_reg.predict(X_val)\n",
        "val_error = mean_squared_error(y_val, y_pred)\n",
        "print(\"Validation MSE:\", val_error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4kQpci8XWN6"
      },
      "source": [
        "#### Computational optimizations\n",
        "XGBoost implementations are optimized for faster performance and can profit from hardware acceleration.\n",
        "\n",
        "Here we use the Ipython's `%timeit` magic to compare average running times for both  XGBoost's and scikit-learn's implementations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vW_4jCFRiHMB"
      },
      "outputs": [],
      "source": [
        "%timeit xgboost.XGBRegressor(objective='reg:squarederror').fit(X_train, y_train) if xgboost is not None else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPu9UqpniHMB"
      },
      "outputs": [],
      "source": [
        "%timeit GradientBoostingRegressor().fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6B4umAMX97M"
      },
      "source": [
        "On a standard Colab runtime, XGBoost runs around 3x faster than scikit-learn."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPhg6UkOvpF8WjJ24IeYJow",
      "collapsed_sections": [
        "bJeMZ8BuLzu5"
      ],
      "include_colab_link": true,
      "name": "TutorialBoosting.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('ml-latest')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "1bede9cc09fadb76754e231ea17b3d1b4d36d88785eed308e26382b97c73c356"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
