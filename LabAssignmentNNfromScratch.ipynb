{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thalitadru/ml-class-epf/blob/main/LabAssignmentNNfromScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4bce1a6-aa4f-4177-9047-cde18118d5b6",
      "metadata": {
        "id": "b4bce1a6-aa4f-4177-9047-cde18118d5b6"
      },
      "source": [
        "# Neural Networks From Scratch\n",
        "\n",
        "_Credits_: activity adapted from:\n",
        "\n",
        "- [Part 1 of notebook by Sondak et al](https://harvard-iacs.github.io/2019-CS109A/labs/lab11x/)\n",
        "- [Tutorial by C. Hansen](https://mlfromscratch.com/neural-network-tutorial/#/). Made for and explained at [mlfromscratch.com/neural-networks-tutorial/](https://mlfromscratch.com/neural-network-tutorial/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0272c16-e700-4f70-8b6b-22f538e0299e",
      "metadata": {
        "id": "f0272c16-e700-4f70-8b6b-22f538e0299e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11a51ecf-17d7-419f-9350-cb7efba2e29e",
      "metadata": {
        "id": "11a51ecf-17d7-419f-9350-cb7efba2e29e"
      },
      "source": [
        "## Learning Goals\n",
        "\n",
        "By the end of this lab, you should:\n",
        "- understand how a simple neural network works and code its functions from scratch.\n",
        "- implement forward and backard pass for a network with multiple layers in Numpy.\n",
        "\n",
        "## Part 1: Single neuron\n",
        "\n",
        "The simplest way to describe a neural network is that we have some inputs $x$, which get combined into an auxilliary variable $a$. The auxilliary variable is passed through the activation function $\\sigma\\left(a\\right)$ and the result is the output.\n",
        "Inputs are linearly combined according to some weights $w$ and a bias $b$. This transformation is also sometimes called an **affine** transformation. The perceptron transforms the weighted inputs according to the rule of the activation function. For a single perceptron, the output $y$ is just the output from the perceptron. The linear transformation and activation of the neuron occurs within a single **layer** of the network (shown in the dotted box).\n",
        "Let's see what the single-layer, single neuron network give us. We have a couple of choices to make:\n",
        "\n",
        "1. We must choose some weights and some biases\n",
        "2. We must choose an activation function\n",
        "\n",
        "For now, we will manually specify the weights and biases.\n",
        "\n",
        "We choose a _sigmoid_ activation function\n",
        "$$\\sigma\\left(a\\right) = \\dfrac{1}{1 + e^{-a}}=\\frac{e^a}{e^a+1}$$\n",
        "\n",
        "What are the limits $\\displaystyle\\lim_{a\\to\\infty}\\sigma\\left(a\\right)$ and $\\displaystyle\\lim_{a\\to-\\infty}\\sigma\\left(a\\right)$?\n",
        "\n",
        "_Note_: Actually, the sigmoid we have here is called the _logistic_ function. Sigmoids are really a family of functions and the logistic function is just one member in that family.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ad0a01f-c093-4e12-9c2e-52890a6c4c22",
      "metadata": {
        "id": "8ad0a01f-c093-4e12-9c2e-52890a6c4c22"
      },
      "source": [
        "**TODO** Plot the sigmoid\n",
        "\n",
        "Define a sigmoid function and plot it for an array of 500 points from -5 to 5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5555a5ca-d863-4075-8a7c-615d75f1ff72",
      "metadata": {
        "id": "5555a5ca-d863-4075-8a7c-615d75f1ff72"
      },
      "outputs": [],
      "source": [
        "# TODO your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f7ce18d-31a8-4a64-8c78-c8c01e240cc0",
      "metadata": {
        "id": "1f7ce18d-31a8-4a64-8c78-c8c01e240cc0"
      },
      "source": [
        "**TODO** Generate a list of 500 $x$ points from -5 to 5 (using `np.linspace`) and plot both the sigmoid and the tanh (for tanh you may use `np.tanh`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e33a5fe-d954-4622-b13a-a646af9ff08a",
      "metadata": {
        "id": "2e33a5fe-d954-4622-b13a-a646af9ff08a"
      },
      "outputs": [],
      "source": [
        "# TODO your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "989eb27b-5bc3-4de1-97f6-4f2ff8cc68a6",
      "metadata": {
        "id": "989eb27b-5bc3-4de1-97f6-4f2ff8cc68a6"
      },
      "source": [
        "Note the difference in **range** between both functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba83ac78-214c-4a03-b95e-34e42233721c",
      "metadata": {
        "id": "ba83ac78-214c-4a03-b95e-34e42233721c"
      },
      "source": [
        "##### Comments\n",
        "\n",
        "- We say that the activation occurs when $\\sigma(a) = \\dfrac{1}{2}$ (at $a=0$). When $a= wx+b$, we can show that this corresponds to $x = -\\dfrac{b}{w}$.\n",
        "- The \"steepness\" of the sigmoid is controlled by $w$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fc4df34-48ca-43a8-b30d-d755b947ec46",
      "metadata": {
        "id": "9fc4df34-48ca-43a8-b30d-d755b947ec46"
      },
      "source": [
        "### Our task: approximate a Gaussian function using a node\n",
        "\n",
        "The task is to approximate (or learn) a function $f\\left(x\\right)$ given some input $x$. For demonstration purposes, the function we will try to learn is a Gaussian function\n",
        "\\begin{align}\n",
        "f\\left(x\\right) = e^{-x^{2}}\n",
        "\\textrm{}\n",
        "\\end{align}\n",
        "\n",
        "Even though we represent the input $x$ as a vector on the computer, you should think of it as a single input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e26289b1",
      "metadata": {
        "id": "e26289b1"
      },
      "source": [
        "**TODO** Start by plotting the above function using the $x$ dataset you created earlier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61b5f872-daf4-431a-97da-ad300184ada9",
      "metadata": {
        "id": "61b5f872-daf4-431a-97da-ad300184ada9"
      },
      "outputs": [],
      "source": [
        "# TODO your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab22d6f9-8f4d-4a6a-a88c-a16a0b600f36",
      "metadata": {
        "id": "ab22d6f9-8f4d-4a6a-a88c-a16a0b600f36"
      },
      "source": [
        "Now, let's code the single node as per the image above.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e97d4171",
      "metadata": {
        "id": "e97d4171"
      },
      "source": [
        "**TODO** Write a function named `affine` that does the linear transformation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7560e67-b4c6-4a4e-ad54-7908eaa6cc50",
      "metadata": {
        "id": "d7560e67-b4c6-4a4e-ad54-7908eaa6cc50"
      },
      "outputs": [],
      "source": [
        "# TODO your code here\n",
        "def affine(x, w, b):\n",
        "    \"\"\"Return affine transformation of x\n",
        "\n",
        "    INPUTS\n",
        "    ======\n",
        "    x: A numpy array of points in x\n",
        "    w: A float representing the weight of the perceptron\n",
        "    b: A float representing the bias of the perceptron\n",
        "\n",
        "    RETURN\n",
        "    ======\n",
        "    a: A numpy array of points after the affine transformation\n",
        "        a = wx + b\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO your code goes here\n",
        "   \n",
        "    return a\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16d6e454-9576-4df3-b070-c9c47fa87fe3",
      "metadata": {
        "id": "16d6e454-9576-4df3-b070-c9c47fa87fe3"
      },
      "source": [
        "Test your function with the following call:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1f1080e-6746-465c-9f74-cc88dce8f98c",
      "metadata": {
        "id": "e1f1080e-6746-465c-9f74-cc88dce8f98c"
      },
      "outputs": [],
      "source": [
        "# TODO your code here\n",
        "w = -4.5\n",
        "b = 4.0\n",
        "\n",
        "z = sigmoid(affine(x, w, b))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d1e21e2-b80e-4d0b-bece-febab6bd807f",
      "metadata": {
        "id": "3d1e21e2-b80e-4d0b-bece-febab6bd807f"
      },
      "source": [
        "And now we plot the activation function and the true function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7678ee4b-e8a3-4d9a-84c6-015b2b20239f",
      "metadata": {
        "id": "7678ee4b-e8a3-4d9a-84c6-015b2b20239f"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(8, 4))  # create axes object\n",
        "\n",
        "SIZE = 16\n",
        "# Plot\n",
        "ax.plot(x, f, ls='-.', lw=4, label=r'True function')\n",
        "ax.plot(x, z, lw=4, label=r'Single Neuron')\n",
        "\n",
        "# Create labels (very important!)\n",
        "# Notice we make the labels big enough to read\n",
        "ax.set_xlabel('$x$', fontsize=SIZE)\n",
        "ax.set_ylabel('$y$', fontsize=SIZE)\n",
        "\n",
        "ax.tick_params(labelsize=SIZE)  # Make the tick labels big enough to read\n",
        "\n",
        "# Create a legend and make it big enough to read\n",
        "ax.legend(fontsize=SIZE, loc=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5e42a69-9598-4852-b520-4fcc7a9fffff",
      "metadata": {
        "id": "f5e42a69-9598-4852-b520-4fcc7a9fffff"
      },
      "source": [
        "The single perceptron simply turns the output on and off at some point, but that's about it. We see that the neuron is on until about $x=0$ at which point it abruptly turns off. It's able to get \"close\" to the true function. Otherwise, it has nothing in common with the true function.\n",
        "\n",
        "What do you think will happen if you change $w$ and $b$? Try it out with the next cell of code. Here we have 3 sets of weights and biases to produce 3 sets of predicted curves. All of them apper in the folloing plot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a966a57-a90c-4609-a643-c4f2b21a6b0e",
      "metadata": {
        "id": "1a966a57-a90c-4609-a643-c4f2b21a6b0e"
      },
      "outputs": [],
      "source": [
        "w = [-5.0, 0.1, 5.0]  # Create a list of weights\n",
        "b = [0.0, -1.0, 1.0]  # Create a list of biases\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
        "ax.plot(x, f, lw=4, ls='-.', label='True function')\n",
        "for wi, bi in zip(w, b):\n",
        "    h = sigmoid(affine(x, wi, bi))\n",
        "    ax.plot(x, h, lw=4, label=r'$w = {0}$, $b = {1}$'.format(wi, bi))\n",
        "ax.set_title('Single neuron network', fontsize=SIZE)\n",
        "\n",
        "# Create labels (very important!)\n",
        "# Notice we make the labels big enough to read\n",
        "ax.set_xlabel('$x$', fontsize=SIZE)\n",
        "ax.set_ylabel('$y$', fontsize=SIZE)\n",
        "\n",
        "ax.tick_params(labelsize=SIZE)  # Make the tick labels big enough to read\n",
        "\n",
        "# Create a legend and make it big enough to read\n",
        "ax.legend(fontsize=SIZE, loc='best')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8221737-9849-495d-b322-75961b021601",
      "metadata": {
        "id": "a8221737-9849-495d-b322-75961b021601"
      },
      "source": [
        "We didn't do an exhaustive search of the weights and biases, but it sure looks like this single perceptron is never going to match the actual function. Again, we shouldn't be suprised about this. The output layer of the network is simply the logistic function, which can only have so much flexibility.\n",
        "\n",
        "Let's try to make our network more flexible by using **more neurons**!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e311b38-74e6-486b-a527-c24c1493e4cd",
      "metadata": {
        "id": "5e311b38-74e6-486b-a527-c24c1493e4cd"
      },
      "source": [
        "## Part 2: Multiple neurons\n",
        "\n",
        "It appears that a single neuron is somewhat limited in what it can accomplish. What if we expand the number of nodes/neurons in our network? We have two obvious choices here. One option is to add depth to the network by putting layers next to each other. The other option is to stack neurons on top of each other in the same layer. Now the network has some width, but is still only one layer deep.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b8736b9-598a-40ac-9e15-26f6691cd66e",
      "metadata": {
        "id": "0b8736b9-598a-40ac-9e15-26f6691cd66e"
      },
      "source": [
        "**Some observations**\n",
        "\n",
        "1. We still have a single input in this case. Note that this is not necessary in general. We're just keeping things simple with a single input for now. If we have more inputs we will have a matrix for $X$.\n",
        "2. Each node (or neuron) has a weight and bias associated with it. An affine transformation is performed for each node.\n",
        "3. Both nodes use the same activation function form $\\sigma\\left(\\cdot\\right)$ on their respective inputs.\n",
        "4. The outputs of the nodes must be combined to give the overall output of the network. There are a variety of ways of accomplishing this. In the current example, we just take a linear combination of the node outputs to produce the actual prediction. Notice that now we have weights and biases at the output too.\n",
        "   Let's see what happens in this case. First, we just compute the outputs of each neuron.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3941211b-ea02-4826-845b-745aac032db1",
      "metadata": {
        "id": "3941211b-ea02-4826-845b-745aac032db1"
      },
      "outputs": [],
      "source": [
        "x = np.linspace(-5.0, 5.0, 500)  # 500 input points\n",
        "f = np.exp(-x*x)  # target data\n",
        "\n",
        "w = np.array([3.5, -3.5])  # weight vector\n",
        "b = 3.5  # bias term\n",
        "\n",
        "# Affine transformations\n",
        "a1 = w[0] * x + b\n",
        "a2 = w[1] * x + b\n",
        "\n",
        "# Node outputs\n",
        "z1 = 1.0 / (1.0 + np.exp(-a1))\n",
        "z2 = 1.0 / (1.0 + np.exp(-a2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e4b757a",
      "metadata": {
        "id": "9e4b757a"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
        "\n",
        "ax.plot(x, f, lw=4, ls='-.', label='True function')\n",
        "ax.plot(x, z1, lw=4, label='First neuron')\n",
        "ax.plot(x, z2, lw=4, label='Second neuron')\n",
        "\n",
        "# Set title\n",
        "ax.set_title('Comparison of Neuron Outputs', fontsize=SIZE)\n",
        "\n",
        "# Create labels (very important!)\n",
        "# Notice we make the labels big enough to read\n",
        "ax.set_xlabel('$x$', fontsize=SIZE)\n",
        "ax.set_ylabel('$y$', fontsize=SIZE)\n",
        "\n",
        "ax.tick_params(labelsize=SIZE)  # Make the tick labels big enough to read\n",
        "\n",
        "# Create a legend and make it big enough to read\n",
        "ax.legend(fontsize=SIZE, loc='best')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4539284-b88a-4ea5-94df-cfb5ab2cfc23",
      "metadata": {
        "id": "a4539284-b88a-4ea5-94df-cfb5ab2cfc23"
      },
      "source": [
        "Just as we expected, we obtain one sigmoid for each neuron.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VHzwJ8EiIpK2",
      "metadata": {
        "id": "VHzwJ8EiIpK2"
      },
      "source": [
        "### Adding another layer\n",
        "\n",
        "Of course, to get the network prediction we must combine these two sigmoid curves somehow. We can apply again an afine transformation to $z_{1}$ and $z_{2}$:\n",
        "\n",
        "$$ y_{out} = \\vec{w}_{out} \\cdot [z_1 \\quad z_2] + b_{out} $$\n",
        "\n",
        "**Note**: \n",
        "We are **not** doing classification here, but regression. We are trying to predict a real-valued function. This is why the output neuron will not have sigmoid activation. The sigmoid activation is convenient when doing classification because you need outputs to range between $0$ and $1$. However, when learning a real-valued function, we don't have as good of a reason to choose a sigmoid.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83590711-4da9-4727-bb8f-de46667022d3",
      "metadata": {
        "id": "83590711-4da9-4727-bb8f-de46667022d3"
      },
      "outputs": [],
      "source": [
        "# output neuron weights and bias\n",
        "wout = np.array([1.2, 1.2])\n",
        "bout = -1.2\n",
        "\n",
        "# computing the affine transformation with two inputs\n",
        "yout = wout @ np.array([z1, z2]) + bout\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5ea755c-b5ec-4b14-9cdf-1101c30d3c1e",
      "metadata": {
        "id": "e5ea755c-b5ec-4b14-9cdf-1101c30d3c1e"
      },
      "source": [
        "And plot to check your result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eecc65cc-eede-4923-8933-d391bfa47dba",
      "metadata": {
        "id": "eecc65cc-eede-4923-8933-d391bfa47dba"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
        "\n",
        "ax.plot(x, f, lw=4, ls='-.', label='True function f(x)')\n",
        "ax.plot(x, yout, lw=4, label=r'$y_{out} = w_{1}z_{1} + w_{2}z_{2} + b$')\n",
        "ax.plot (x, yout-f, label='$error = y_{out}-f$')\n",
        "\n",
        "# Create labels (very important!)\n",
        "# Notice we make the labels big enough to read\n",
        "ax.set_xlabel('$x$', fontsize=SIZE)\n",
        "ax.set_ylabel('$y$', fontsize=SIZE)\n",
        "\n",
        "ax.tick_params(labelsize=SIZE)  # Make the tick labels big enough to read\n",
        "\n",
        "# Create a legend and make it big enough to read\n",
        "ax.legend(fontsize=SIZE, loc='best')\n",
        "print(f\"Mean-squared error is {np.mean((yout-f)**2):0.5f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "684a3457-6408-45d6-8d43-f857b8c6c89f",
      "metadata": {
        "id": "684a3457-6408-45d6-8d43-f857b8c6c89f"
      },
      "source": [
        "Very cool! The two nodes interact with each other to produce a pretty complicated-looking function. It still doesn't match the true function, but now we have some hope. In fact, it's starting to look a little bit like a Gaussian!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c931448c",
      "metadata": {
        "id": "c931448c"
      },
      "source": [
        "### Adding some learning capabilities\n",
        "\n",
        "We know that changing the weights and biases can get us a better approximation. Instead of trying to fix these values by hand, let us launch an iterative **learning procedure** using the **perceptron learning rule**. For a certain input sample $x_n$ the incurring update is:\n",
        "$$ \\Delta_n {w} = -\\rho (\\hat{y}_{n} - y_n) x_n$$\n",
        "$$ \\Delta_n b = -\\rho (\\hat{y}_{n} - y_n)$$\n",
        "where $\\rho$ is the learning rate.\n",
        "\n",
        "\n",
        "Summing the updates for all samples we get:\n",
        "$$ \\Delta {w} = -\\rho \\vec{\\varepsilon} \\cdot \\vec{x} = -\\rho \\sum_{n=1}^{500} \\varepsilon_n x_n$$\n",
        "$$ \\Delta b = -\\rho \\sum_{n=1}^{500} \\varepsilon_n$$\n",
        "where $\\rho$ is the learning rate,\n",
        "$\\varepsilon_n = \\hat{y}_{n} - y_n$ and $\\vec{\\varepsilon} = \\vec{\\hat{y}} - \\vec{y}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c473548",
      "metadata": {
        "id": "2c473548"
      },
      "source": [
        "The following code applies these update rules in a loop. Launch the code and see after how many iterations we can achieve a mean-squared-error (MSE) lower than `1e-4`\n",
        "\n",
        "**TODO** Complete the code bellow to include the perceptron updates. In the code, our $\\vec{\\hat{y}}$ is given by `yout`. The target variable $\\vec{y}$ is given by `f`. The error term $\\vec\\varepsilon$ is kept in the variable `error`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad052708",
      "metadata": {},
      "source": [
        "**NOTE:** \n",
        "We are trying to learn the weights $w_{out} = [w_{{out}_1} ~ w_{{out}_2}]$ and bias $b_{out}$ that combine $\\vec{z}_1$ and $\\vec{z}_2$. The weights before each neuron will not be changed. This means the input samples for our case are in $\\vec{z}_1$ and $\\vec{z}_2$. Therefore we have the following updates:\n",
        "- $\\Delta w_{{out}_1} = - \\rho \\vec{\\epsilon} \\cdot \\vec{z}_1 = -\\rho \\sum_{n=1}^{500} \\varepsilon_n {z_1}_n$\n",
        "- $\\Delta w_{{out}_2} = - \\rho \\vec{\\epsilon} \\cdot \\vec{z}_2 = -\\rho \\sum_{n=1}^{500} \\varepsilon_n {z_2}_n$\n",
        "- $\\Delta b_{out} = - \\rho \\sum_{n=1}^{500} \\varepsilon_n$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc94c7ae",
      "metadata": {
        "id": "cc94c7ae"
      },
      "outputs": [],
      "source": [
        "# initializing output neuron parameters\n",
        "wout = np.array([1.2, 1.2])\n",
        "bout = -1.2\n",
        "\n",
        "# learning hyperparameters\n",
        "learning_rate = 1e-3\n",
        "max_iter = 500\n",
        "\n",
        "# intializing loop variables\n",
        "error = 1\n",
        "i = 0\n",
        "learning_curve = []\n",
        "# learning loop\n",
        "# while not converged:\n",
        "while np.mean(error**2) > 1e-4 and i < max_iter:\n",
        "    # compute output\n",
        "    yout = wout[0] * z1 + wout[1] * z2 + bout\n",
        "    # compte error and save it in list to visualize leraning curve later\n",
        "    # TODO your code here\n",
        "    error = ...\n",
        "    learning_curve.append(np.mean(error**2))\n",
        "    # apply the perceptron learning rule to weights and bias\n",
        "    # TODO your code here\n",
        "    wout = ...\n",
        "    bout = ...\n",
        "    # increment iteration counter\n",
        "    i += 1\n",
        "print(\n",
        "    f\"Learning stopped with MSE={learning_curve[-1]:0.5f} after {i} iterations\")\n",
        "print(f\"weights={wout.round(5)} bias={bout.round(5)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01932ed5",
      "metadata": {
        "id": "01932ed5"
      },
      "source": [
        "Plot again the function graphs so you can see how the new predicted curve is a better fit. Note the mean squared error has been reduced 10x by the learning procedure. To get better results, the best would be to learn not only these output parameters, but also the parameters from the previous 2 neurons. It could also help to include more neurons in the previous layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f359f86f",
      "metadata": {
        "id": "f359f86f"
      },
      "source": [
        "**Note**: the graph to the right shows the evolution of the mean-squared error across iterations and it is called a **learning curve** (same as when doing SGD).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46dbc1fc",
      "metadata": {
        "id": "46dbc1fc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# compute output again prior to plotting\n",
        "yout = wout @ np.array([z1, z2]) + bout\n",
        "\n",
        "fig, (ax, lc) = plt.subplots(1, 2, figsize=(20, 5))\n",
        "\n",
        "ax.plot(x, f, lw=4, ls='-.', label='True function f(x)')\n",
        "ax.plot(x, yout, lw=4, label=r'$y_{out} = w_{1}z_{1} + w_{2}z_{2} + b$')\n",
        "ax.plot(x, yout-f, label='$error = y_{out}-f$')\n",
        "\n",
        "# Create labels (very important!)\n",
        "# Notice we make the labels big enough to read\n",
        "ax.set_xlabel('$x$', fontsize=SIZE)\n",
        "ax.set_ylabel('$y$', fontsize=SIZE)\n",
        "\n",
        "ax.tick_params(labelsize=SIZE)  # Make the tick labels big enough to read\n",
        "\n",
        "# Create a legend and make it big enough to read\n",
        "ax.legend(fontsize=SIZE, loc=1)\n",
        "\n",
        "lc.plot(np.arange(len(learning_curve)), learning_curve, 'o:')\n",
        "lc.set_xlabel(\"iterations\", fontsize=SIZE)\n",
        "lc.set_ylabel(\"mean squared error\", fontsize=SIZE)\n",
        "lc.set_title('Learning curve', fontsize=SIZE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01ec2013-ed58-46c6-99c4-70169b985ef1",
      "metadata": {
        "id": "01ec2013-ed58-46c6-99c4-70169b985ef1"
      },
      "source": [
        "## Part 3: Multi-layer perceptron from scratch\n",
        "\n",
        "In this section, you will be implementing a full multi-layer perceptron in Numpy. You will need to code all the vectorized equations by yourself, both forward and backward.\n",
        "In the following assignments (TP3 and on), we will be working with Tensorflow and Keras, which do much of this work for you.\n",
        "\n",
        "We will focus on the following 4-layer neural network, with fully connected layers in this notebook.\n",
        "\n",
        "<img alt=\"NN diagram with 784 inputs, followed by 3 layers with 128, 64 and 10 nodes, respectively\" src=\"https://raw.githubusercontent.com/lionelmessi6410/Neural-Networks-from-Scratch/0145471d6124831c071b3a772c943b9a57128984//figs/deep_nn-1.png\" width=\"500\"></img>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1ef5835",
      "metadata": {
        "id": "d1ef5835"
      },
      "source": [
        "Here some extra imports necessary for this part:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feb3c9ea-266c-44b9-88d6-c3393ab74e6a",
      "metadata": {
        "id": "feb3c9ea-266c-44b9-88d6-c3393ab74e6a"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "from numpy.typing import ArrayLike\n",
        "from typing import List, Dict, Tuple \n",
        "\n",
        "try:\n",
        "    import tqdm\n",
        "except ModuleNotFoundError:\n",
        "    !pip install tqdm\n",
        "from tqdm.notebook import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcd619c5-0f79-42ce-91ed-0e26f674410f",
      "metadata": {
        "id": "fcd619c5-0f79-42ce-91ed-0e26f674410f"
      },
      "source": [
        "### Choosing a Dataset\n",
        "\n",
        "For this walkthrough, we will focus on importing the MNIST dataset and using that as the input to our deep neural networks. Note that this is purely a demonstration of how to make a neural network from scratch, and it is NOT the recommended architecture for solving the MNIST problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9199b45b",
      "metadata": {
        "id": "9199b45b"
      },
      "source": [
        "First we download the data from OpenML:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b3b1df0",
      "metadata": {
        "id": "6b3b1df0"
      },
      "outputs": [],
      "source": [
        "data_bunch = fetch_openml('mnist_784', as_frame=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a7f2c3b",
      "metadata": {
        "id": "7a7f2c3b"
      },
      "source": [
        "Image pixels have values from 0 to 255. We will divide all of them by this value so the range of variation stays between 0 and 1, so features have a similar scale (remember this favours SGD convergence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d474c0e",
      "metadata": {
        "id": "2d474c0e"
      },
      "outputs": [],
      "source": [
        "X = (data_bunch.data/255).astype('float32')\n",
        "y = data_bunch.target.astype('uint32')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3ba90ca",
      "metadata": {
        "id": "c3ba90ca"
      },
      "source": [
        "Then we split data in training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3a13c78",
      "metadata": {
        "id": "c3a13c78"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.15, stratify=y, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85d2ef17",
      "metadata": {
        "id": "85d2ef17"
      },
      "source": [
        "### Encoding targets\n",
        "Our targets `y` have integer values at this point. When training our classifier network, we need to compare the 10 predicted class probabilities to the ground thruth. For that, we will encode out targets `y` using one-hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c103810b",
      "metadata": {
        "id": "c103810b"
      },
      "outputs": [],
      "source": [
        "def one_hot(a):\n",
        "    num_classes = np.max(a) + 1\n",
        "    return np.eye(num_classes, dtype=\"uint32\")[a.reshape(-1)]\n",
        "\n",
        "# small y represents the target wiht class ids\n",
        "# capital Y stores the one-hot representation\n",
        "Y_train = one_hot(y_train)\n",
        "Y_val = one_hot(y_val)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "512a7235",
      "metadata": {
        "id": "512a7235"
      },
      "source": [
        "### Equations from theory\n",
        "#### Forward pass\n",
        "From the theory we know we need to implement the following forward equations:\n",
        "$$ \\vec{a}_1 := W_1\\vec{x} + \\vec{b}_1$$\n",
        "$$ \\vec{z}_1 := \\sigma(\\vec{a}_1)$$\n",
        "$$ \\vec{a}_2 := W_2\\vec{z}_1 + \\vec{b}_2$$\n",
        "$$ \\vec{z}_2 := \\sigma(\\vec{a}_2)$$\n",
        "$$ \\vec{a}_3 := W_3\\vec{z}_2 + \\vec{b}_3$$\n",
        "$$ \\vec{\\hat{y}} := \\vec{z}_3 = \\mathcal{S}(\\vec{a}_3)$$\n",
        "\n",
        "Here $\\mathcal{S}:\\mathbb{R}^n \\to \\mathbb{R}^n $ is the softmax function.\n",
        "#### Backward pass\n",
        "For the backward equations, we have for $l$ in $\\{3,2,1\\}$ and initializing $ \\vec{g}_3 = \\frac{\\partial J}{\\partial \\vec{\\hat{y}}} $:\n",
        "$$ \\vec{g}_{l} := \\vec{g}_l \\odot \\varphi_l\\prime(\\vec{a}_l)$$\n",
        "$$ \\Delta W_l := \\vec{g}_l \\otimes \\vec{z}_{l-1} $$\n",
        "$$ \\Delta \\vec{b}_l := \\vec{g}_l  $$\n",
        "$$ \\vec{g}_{l-1} := W_l^T \\vec{g}_l $$\n",
        "Here $\\otimes$ is the outter product between two vectors ($\\vec{x} \\otimes \\vec{y} = \\vec{x}\\vec{y}^T$), and $\\odot$ is the element-wise product.\n",
        "\n",
        "Using the cross entropy loss, our gradient with respect to the last layer will be proportional to the prediction error:\n",
        "$$ \\frac{\\partial J}{\\partial \\vec{\\hat{y}}} =\\vec{\\hat{y}}-\\vec{y}$$\n",
        "\n",
        "Expanding the equations for all layers in our network we have the following:\n",
        "$$\\vec{g}_3 := (\\vec{\\hat{y}}-\\vec{y})\\odot \\mathcal{S}\\prime(\\vec{a}_3) $$\n",
        "$$ \\Delta W_3 := \\vec{g}_3 \\otimes \\vec{z}_2 $$\n",
        "$$ \\Delta \\vec{b}_3 := \\vec{g}_3 $$\n",
        "$$ \\vec{g}_2 := (W_3^T \\vec{g}_3) \\odot \\sigma\\prime(\\vec{a}_2)$$\n",
        "$$ \\Delta W_2 := \\vec{g}_2 \\otimes \\vec{z}_1 $$\n",
        "$$ \\Delta \\vec{b}_2 := \\vec{g}_2 $$\n",
        "$$ \\vec{g}_1 := (W_2^T \\vec{g}_2) \\odot \\sigma\\prime(\\vec{a}_1)$$\n",
        "$$ \\Delta W_1 := \\vec{g}_1 \\otimes \\vec{x} $$\n",
        "$$ \\Delta \\vec{b}_1 := \\vec{g}_1 $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efe8c600-e416-489d-b312-b5c30385d2b1",
      "metadata": {
        "id": "efe8c600-e416-489d-b312-b5c30385d2b1"
      },
      "source": [
        "### Implementation with NumPy\n",
        "\n",
        "You will complete the following code with the pertinent equations. For now, the code does not include biases for the neurons, only weight matrices\n",
        "\n",
        "1. **Initialization** Complete the method `initialization` with the shapes for each of the network weight matrices.\n",
        "1. **Forward pass** Complete the method `forward_pass` with the expressions computing the forward equations. The method should return the final predicted `output` $\\vec{\\hat{y}}$.\n",
        "1. **Backward pass** Complete the method `backward_pass` with the expressions to compute the backward equations, that is, the weight updates. You may use [`numpy`'s `np.outer`](https://numpy.org/doc/stable/reference/generated/numpy.outer.html) to compute outer products.\n",
        "1. **Update parameters** Complete the method `update_network_parameters` so that the updates computed during the forward pass are applied to the parameters.\n",
        "\n",
        "Once you are done with these steps, your model will be functional, and you will be able to fit it to the training data.\n",
        "\n",
        "There are further additions you can do, which are the subject of the exercises comming after the code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "415d52a2",
      "metadata": {
        "id": "415d52a2"
      },
      "source": [
        "#### Activation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a40a775-d11d-4ee7-ac45-36a7d019cb32",
      "metadata": {
        "id": "7a40a775-d11d-4ee7-ac45-36a7d019cb32"
      },
      "outputs": [],
      "source": [
        "# Activation functions to be used in the network\n",
        "# define the sigmoind function and its derivative\n",
        "def sigmoid(x, derivative=False):\n",
        "    if derivative:\n",
        "        return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "# define the softmax function and its derivative\n",
        "def softmax(x, derivative=False):\n",
        "    # Numerically stable with large exponentials\n",
        "    exps = np.exp(x - x.max())\n",
        "    if derivative:\n",
        "        return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
        "    return exps / np.sum(exps, axis=0)\n",
        "\n",
        "# define the relu activaiton and its derivative\n",
        "def relu(x, derivative=False):\n",
        "    # TODO EXERCISE 1\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec54ea70",
      "metadata": {
        "id": "ec54ea70"
      },
      "source": [
        "#### Neural network class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d03165c7",
      "metadata": {
        "id": "d03165c7"
      },
      "outputs": [],
      "source": [
        "class DeepNeuralNetwork():\n",
        "    \"\"\"\n",
        "    This class implments a neural network with 4 layers \n",
        "    (one input layer + 2 hidden layers + one output layer).\n",
        "    # TODO EXTRA Exercise 3 change the code so it supports an arbitrary number of layers\n",
        "    \"\"\"\n",
        "    def __init__(self, n_neurons:list):\n",
        "        \"\"\"AI is creating summary for __init__\n",
        "            The class constructor has only one argument, that determines the \n",
        "            size of each layer.\n",
        "            It initializes network parameters according to these sizes\n",
        "        Args:\n",
        "            n_neurons (list): a list of integers containing the quantity of \n",
        "            neurons in each layer. The lenght of the list corresponds to the number of layers (including the input layer).\n",
        "        \"\"\"        \n",
        "        self.n_neurons = n_neurons\n",
        "\n",
        "        # we save all parameters in the neural network in this dictionary\n",
        "        self.params = self._initialization()\n",
        "\n",
        "    def _initialization(self) -> Dict:\n",
        "        \"\"\"Initializes parameters for the network and gathers them \n",
        "        in a dictionnary. This method is called in the class constructor.\n",
        "\n",
        "        Weights for layer i are named Wi \n",
        "        biasfor layer i are named bi\n",
        "\n",
        "        Returns:\n",
        "            dict: dictionnary of parameters in the network. Weights for layer i\n",
        "            are named Wi (W1, W2, etc). Bias for layer i is named bi (b1, b2, \n",
        "            etc)\n",
        "        \"\"\"\n",
        "        # number of nodes in each layer\n",
        "        input_layer = self.n_neurons[0]\n",
        "        hidden_1 = self.n_neurons[1]\n",
        "        hidden_2 = self.n_neurons[2]\n",
        "        output_layer = self.n_neurons[3]\n",
        "\n",
        "        # TODO complete with the shape of each weight matrix\n",
        "        W1_shape = (0, 0)  # TODO code here\n",
        "        W2_shape = (0, 0)  # TODO code here\n",
        "        W3_shape = (0, 0)  # TODO code here\n",
        "\n",
        "        params = {\n",
        "            # intialize weights with small random values\n",
        "            'W1': np.random.randn(*W1_shape) * np.sqrt(1. / W1_shape[0]),\n",
        "            'W2': np.random.randn(*W2_shape) * np.sqrt(1. / W2_shape[0]),\n",
        "            'W3': np.random.randn(*W3_shape) * np.sqrt(1. / W3_shape[0])\n",
        "            # TODO EXTRA EXERCISE 2 add network bias here\n",
        "        }\n",
        "\n",
        "        return params\n",
        "\n",
        "    def forward_pass(self, x) -> ArrayLike:\n",
        "        \"\"\"\n",
        "        This functions computes the forward pass for a single sample x\n",
        "        \"\"\"\n",
        "        params = self.params\n",
        "\n",
        "        # input layer activations becomes sample\n",
        "        params['Z0'] =  # TODO code here\n",
        "\n",
        "        # input layer to hidden layer 1\n",
        "        params['A1'] =   # TODO code here\n",
        "        params['Z1'] =   # TODO code here\n",
        "\n",
        "        # hidden layer 1 to hidden layer 2\n",
        "        params['A2'] =  # TODO code here\n",
        "        params['Z2'] =  # TODO code here\n",
        "\n",
        "        # hidden layer 2 to output layer\n",
        "        params['A3'] =   # TODO code here\n",
        "        params['Z3'] =   # TODO code here\n",
        "\n",
        "        output =   # TODO code here\n",
        "        return output\n",
        "\n",
        "    def backward_pass(self, y, output) -> Dict:\n",
        "        '''\n",
        "        This is the backpropagation algorithm, for calculating the updates\n",
        "        of the neural network's parameters.\n",
        "\n",
        "        Note: There is a stability issue that causes warnings. This is \n",
        "                caused  by the dot and multiply operations on the huge arrays.\n",
        "\n",
        "                RuntimeWarning: invalid value encountered in true_divide\n",
        "                RuntimeWarning: overflow encountered in exp\n",
        "                RuntimeWarning: overflow encountered in square\n",
        "        '''\n",
        "        params = self.params\n",
        "        change_w = {}\n",
        "\n",
        "        # Calculate W3 update\n",
        "        grad =   # TODO code here\n",
        "        change_w['W3'] =   # TODO code here\n",
        "        # TODO EXTRA EXERCISE 2 compute bias updates\n",
        "\n",
        "        # Calculate W2 update\n",
        "        grad =   # TODO code here\n",
        "        change_w['W2'] =   # TODO code here\n",
        "        # TODO EXTRA EXERCISE 2 compute bias updates\n",
        "\n",
        "        # Calculate W1 update\n",
        "        grad =   # TODO code here\n",
        "        change_w['W1'] =  # TODO code here\n",
        "        # TODO EXTRA EXERCISE 2 compute bias updates\n",
        "\n",
        "        return change_w\n",
        "\n",
        "    def update_network_parameters(self, changes_to_w, l_rate):\n",
        "        '''\n",
        "        Update network parameters according to update rule from\n",
        "        Stochastic Gradient Descent.\n",
        "\n",
        "        θ = θ - η * ∇J(x, y), \n",
        "            theta θ:            a network parameter (e.g. a weight w)\n",
        "            eta η:              the learning rate\n",
        "            gradient ∇J(x, y):  the gradient of the cost function,\n",
        "                                i.e. the change for a specific theta θ\n",
        "        '''\n",
        "        for key, value in changes_to_w.items():\n",
        "            # TODO code here\n",
        "            self.params[key] = ...\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        This function computes predictions for a maxtrix of samples X\n",
        "        by calling the forward_pass on each row. \n",
        "        Returns a class distribution vector per sample\n",
        "        \"\"\"\n",
        "        # computes preductions by calling the forward pass on all samples\n",
        "        return np.array([self.forward_pass(x_i) for x_i in X])\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        This function computes predictions for a maxtrix of samples X\n",
        "        by calling the forward_pass on each row\n",
        "        Returns a class id per sample\n",
        "        \"\"\"\n",
        "        # computes preductions by calling the forward pass on all samples\n",
        "        return np.array([np.argmax(self.forward_pass(x_i)) for x_i in X],\n",
        "                        dtype='uint32')\n",
        "\n",
        "    def score(self, X, y):\n",
        "        '''\n",
        "        This function computes predictions for a maxtrix of samples X,\n",
        "        then checks if the index of the maximum value \n",
        "        in the output equals the index in the label y.\n",
        "        This indicator vector of correct/incorrect predictions is \n",
        "        averaged to obtain the accuracy\n",
        "\n",
        "        y may be a 1d array of labels or a one-hot encoded matrix of label indicators\n",
        "        '''\n",
        "        # computes the accuracy score by calling predict on all samples\n",
        "        if y.ndim == 1:\n",
        "            y_true = y\n",
        "        elif y.ndim == 2:\n",
        "            y_true = np.argmax(y, axis=1)\n",
        "\n",
        "        return np.mean(y_true == self.predict(X))\n",
        "\n",
        "    def fit(self, X_train:ArrayLike, Y_train:ArrayLike,\n",
        "            X_val:ArrayLike, Y_val:ArrayLike,\n",
        "            epochs=10, batch_size=16, l_rate=0.001) -> Dict:\n",
        "        \"\"\"This function fits the model parameters to the training data, \n",
        "        while monitoring the accuracy score on the validation data.\n",
        "\n",
        "        Y arrays should be one-hot encoded\n",
        "\n",
        "        Args:\n",
        "            X_train (ArrayLike): Array with training input samples\n",
        "            Y_train (ArrayLike): One-hot encoded array of training set targets\n",
        "            X_val (ArrayLike): Array with validation samples\n",
        "            Y_val (ArrayLike): One-hot encoded array of validation set targets\n",
        "            epochs (int, optional): number of epochs for SGD. Defaults to 10.\n",
        "            l_rate (float, optional): learning rate for SGD. Defaults to 0.001.\n",
        "\n",
        "        Returns:\n",
        "            Dict: dict containing the history of accuracy and loss metrics \n",
        "            across training epochs\n",
        "        \"\"\"\n",
        "        history = defaultdict(list)\n",
        "        for iteration in range(epochs):\n",
        "            start_time = time.time()\n",
        "            # split sample indexes in batches\n",
        "            batch_indexes = np.array_split(np.arange(X_train.shape[0]), batch_size) \n",
        "            # Loop over batches of trainng samples\n",
        "            # tqdm allows us to show a progress bar\n",
        "            for batch_i, idx in tqdm(enumerate(batch_indexes), \n",
        "                                    desc=f\"epoch {iteration+1} progress\", \n",
        "                                    total=len(batch_indexes)):\n",
        "                # this dictionnary will store the updates computed \n",
        "                # over all samples in the batch\n",
        "                # values get updated after each sample is processed\n",
        "                batch_changes = defaultdict(int)\n",
        "                # loop over batch samples\n",
        "                for x, y in zip(X_train[idx], Y_train[idx]):\n",
        "                    # compute forward pass, backward pass, \n",
        "                    output = self.forward_pass(x)\n",
        "                    changes_to_w = self.backward_pass(y, output)\n",
        "                    # sum updates in batch_changes dictionary\n",
        "                    for key, value in changes_to_w.items():\n",
        "                         batch_changes[key] += value/batch_size\n",
        "\n",
        "                # apply batched GD updates\n",
        "                self.update_network_parameters(batch_changes, l_rate)\n",
        "\n",
        "            # store performances on train and validation \n",
        "            # for learning curves later\n",
        "            for split, X, Y in zip(['train', 'val'],\n",
        "                                [X_train, X_val],\n",
        "                                [Y_train, Y_val]):\n",
        "                y_pred_proba = self.predict_proba(X)\n",
        "                y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "                loss = log_loss(Y, y_pred_proba)\n",
        "                y_true = np.argmax(Y, axis=1)\n",
        "                accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "                history[split+'_loss'].append(loss)\n",
        "                history[split+'_accuracy'].append(accuracy)\n",
        "            # Print stats about this epoch\n",
        "            acc = history['val_accuracy'][-1]\n",
        "            print(\n",
        "                f'Epoch: {iteration + 1}, Time Spent: {time.time() - start_time:.2f}s, Val Accuracy: { acc * 100:.2f}%')\n",
        "\n",
        "        return history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "991e0dcd",
      "metadata": {
        "id": "991e0dcd"
      },
      "source": [
        "### Testing your implementation\n",
        "#### Creating a network\n",
        "\n",
        "**Note**: depending on the random initialization of the weights, you may get different results each time you initialize and fit a model. If you want to have a reproducible experiment, try setting a seed for numpy by `np.random.seed(42)` (or any other number), **prior to initializing your model**. Then you should get the same results each time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62149df5-47b6-4dcd-9cfe-ea3c988ae715",
      "metadata": {
        "id": "62149df5-47b6-4dcd-9cfe-ea3c988ae715"
      },
      "outputs": [],
      "source": [
        "# TODO set a seed for reproducibility\n",
        "np.random.seed(42)\n",
        "dnn = DeepNeuralNetwork(n_neurons=[784, 128, 64, 10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4e7a588",
      "metadata": {
        "id": "c4e7a588"
      },
      "source": [
        "#### Some basic tests prior to training\n",
        "Let's verify that the base functions return appropriate arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de151915",
      "metadata": {
        "id": "de151915"
      },
      "outputs": [],
      "source": [
        "# testing the forward pass\n",
        "output = dnn.forward_pass(X_train[1])\n",
        "assert output.shape == Y_train[1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Gm7d9o8YIpK8",
      "metadata": {
        "id": "Gm7d9o8YIpK8"
      },
      "outputs": [],
      "source": [
        "# testing the backward pass\n",
        "updates = dnn.backward_pass(Y_train[1], output)\n",
        "for key, value in updates.items():\n",
        "    if not np.isscalar(value):\n",
        "        assert value.shape == dnn.params[key].shape, f\"param {key} shape: {dnn.params[key].shape} != Update shape: {value.shape} \"\n",
        "    else:\n",
        "        assert  np.isscalar(dnn.params[key]), f\"param {key} has shape {dnn.params[key].shape} but update is scalar={value} \"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54ce5bc8",
      "metadata": {
        "id": "54ce5bc8"
      },
      "source": [
        "Let's see how long it takes to predict outputs for the entire training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc023755",
      "metadata": {
        "id": "dc023755"
      },
      "outputs": [],
      "source": [
        "%timeit -n1 -r1 dnn.predict(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0baf488",
      "metadata": {
        "id": "c0baf488"
      },
      "source": [
        "That is quite a while! Let's now see how our network is perfoming so far:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "250EWcqMIpK-",
      "metadata": {
        "id": "250EWcqMIpK-"
      },
      "outputs": [],
      "source": [
        "pred_proba = dnn.predict_proba(X_train)\n",
        "y_pred = np.argmax(pred_proba, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fce80a64",
      "metadata": {
        "id": "fce80a64"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    f\"Training accuracy: {accuracy_score(y_train, y_pred):.2f}%, training loss:{log_loss(Y_train, pred_proba):.5f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaa27c96",
      "metadata": {
        "id": "aaa27c96"
      },
      "source": [
        "Not a great begining, but that should improve with training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b14304e3",
      "metadata": {
        "id": "b14304e3"
      },
      "source": [
        "#### Training the network\n",
        "While testing your code, you may train on a small subset of the training set, so each iteration runs faster. Once your code is finished, you may train the model on the full dataset, and for a larger number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86e851b1",
      "metadata": {
        "id": "86e851b1"
      },
      "outputs": [],
      "source": [
        "history = dnn.fit(X_train[:10000,:], Y_train[:10000], X_val, Y_val,\n",
        "                  epochs=5, batch_size=16, l_rate=1e-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dcb6e35",
      "metadata": {
        "id": "3dcb6e35"
      },
      "source": [
        "### Plotting the learning curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9445aecf",
      "metadata": {
        "id": "9445aecf"
      },
      "outputs": [],
      "source": [
        "l_curves = pd.DataFrame(history)\n",
        "fig, (ax1,ax2) = plt.subplots(2,1, sharex=True, figsize=(8,8))\n",
        "fig.suptitle(\"learning curves\")\n",
        "l_curves.filter(regex='loss').plot(ax=ax1)\n",
        "l_curves.filter(regex='accuracy').plot(ax=ax2)\n",
        "ax2.set_xlabel('iterations')\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "975ad917-5597-4e49-ba41-175b86aa8a64",
      "metadata": {
        "id": "975ad917-5597-4e49-ba41-175b86aa8a64"
      },
      "source": [
        "### Extra Exercises\n",
        "\n",
        "These exercises could improve the network's performance and/or SW quality of this impelementation. It's good to try and solve them to get better knowledge of how neural networks work and what efficient implementations look like.\n",
        "\n",
        "1. **Easy**: Implement the ReLU activation function. Check how the sigmoid functions are implemented for reference, and remember to implement the derivative as well. Use the ReLU activation function in place of the sigmoid function.\n",
        "\n",
        "2. **Easy**: Initialize biases and add them to A before the activation function in the forward pass, and update the biases in the backward pass.\n",
        "\n",
        "3. **Medium**: Change the class implementation so that it supports an arbitrary number of layers via the `sizes=[]` argument.\n",
        "   - Optimize the initialization function that makes weights for the neural network, such that you can modify the `sizes=[]` argument without the neural network failing.\n",
        "   - Optimize the forward and backward pass, such that they run in a for-loop in each function. This makes the code easier to modify and possibly easier to maintain.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('ml-latest')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "1bede9cc09fadb76754e231ea17b3d1b4d36d88785eed308e26382b97c73c356"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
