{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LabAssignment3B.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO53EDWeWHv/Lla2lQ6n54H"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdFq6io3oWcH"
      },
      "source": [
        "# Processing Sequences Using RNNs (and CNNs)\n",
        "\n",
        "*Credits:* Based on code written by [A. Géron](https://colab.research.google.com/github/ageron/handson-ml2/blob/master/15_processing_sequences_using_rnns_and_cnns.ipynb#scrollTo=AiINDLJHVNep) for his \"Hands-on ML\" book. Code realeased under MIT license."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbgX6-aMHanl"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyooZ0K4pCpC"
      },
      "source": [
        "In this activity we are going to explore different ways of modelling sequential data. We will try out simple baseline models as well as 1D CNNs and different RNNs to **forecast the next step in a time series**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR4QqhoDihkx"
      },
      "source": [
        "## Generating sample data\n",
        "\n",
        "To experiment with different models, we'll use some randomly generated univariate time series.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_bQfrh_PGdz"
      },
      "source": [
        "def generate_time_series(batch_size, n_steps):\n",
        "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
        "    time = np.linspace(0, 1, n_steps)\n",
        "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  #   wave 1\n",
        "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2\n",
        "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # + noise\n",
        "    return series[..., np.newaxis].astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSZCqVimq9Sz"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jALDZroTkupK"
      },
      "source": [
        "Notice the shape of the series array:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj6Fw0yjrJ4_"
      },
      "source": [
        "series.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd9b9zf7p9Xt"
      },
      "source": [
        "The meaning of each dimension is `[samples, time steps, sequence element size]`. \n",
        "Here each series element is a single scalar because we are making a univariate prediction.\n",
        "If we were modeling mutiple time series at once (eg. temperature and humidity) this would be a multi-variate prediction and our sequence element size would be 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWUj5-Zaqi1D"
      },
      "source": [
        "### Creating predicion targets\n",
        "We want to predict one step further into the series. To do so , we can take the last time step of a series as the regression target, while using all the previous time steps as inputs.\n",
        "\n",
        "We also want to split our data into train, validation and test sets as usual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjSOnWaRVNer"
      },
      "source": [
        "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
        "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
        "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YItC9egeVNer"
      },
      "source": [
        "X_train.shape, y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCbCeJ5-3Kro"
      },
      "source": [
        "np.random.seed(43) # not 42, as it would give the first series in the train set\n",
        "\n",
        "series = generate_time_series(1, n_steps + 10)\n",
        "X_new, Y_new = series[:, :n_steps], series[:, n_steps:]\n",
        "X = X_new\n",
        "for step_ahead in range(10):\n",
        "    y_pred_one = model.predict(X[:, step_ahead:])[:, np.newaxis, :]\n",
        "    X = np.concatenate([X, y_pred_one], axis=1)\n",
        "\n",
        "Y_pred = X[:, n_steps:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04AEkgT93Krp"
      },
      "source": [
        "Y_pred.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjIjQ4W23MZ-"
      },
      "source": [
        "### Visualizing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y1Wg-eXVNer"
      },
      "source": [
        "def plot_series(series, y=None, y_pred=None, x_label=\"$t$\", y_label=\"$x(t)$\", legend=True):\n",
        "    plt.plot(series, \".-\")\n",
        "    if y is not None:\n",
        "        plt.plot(n_steps, y, \"bo\", label=\"Target\")\n",
        "    if y_pred is not None:\n",
        "        plt.plot(n_steps, y_pred, \"rx\", markersize=10, label=\"Prediction\")\n",
        "    plt.grid(True)\n",
        "    if x_label:\n",
        "        plt.xlabel(x_label, fontsize=16)\n",
        "    if y_label:\n",
        "        plt.ylabel(y_label, fontsize=16, rotation=0)\n",
        "    plt.hlines(0, 0, 100, linewidth=1)\n",
        "    plt.axis([0, n_steps + 1, -1, 1])\n",
        "    if legend and (y or y_pred):\n",
        "        plt.legend(fontsize=14, loc=\"upper left\")\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3, sharey=True, figsize=(12, 4))\n",
        "for col in range(3):\n",
        "    plt.sca(axes[col])\n",
        "    plot_series(X_valid[col, :, 0], y_valid[col, 0],\n",
        "                y_label=(\"$x(t)$\" if col==0 else None),\n",
        "                legend=(col == 0))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78-2AbhOsvrX"
      },
      "source": [
        "## Prediction task 1 : forecast one time step\n",
        "\n",
        "We want to predict a single continuous value for each input sequence. This can be seen as a sequence-to-vector regression task. As seen in class, a suitable loss function is the `keras.losses.mean_squared_error` (as derived by MLE under a Gaussian assumption on the prediciton targets).\n",
        "\n",
        "\n",
        "As we try out different models, keep the models and their validation performances. In the end you will compare their performances in validation to pick the best model (which will be applied to the test set)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq8h_fRJu2cb"
      },
      "source": [
        "\n",
        "### Simple baseline models\n",
        "\n",
        "Before trying complex models, it is important to set up a baseline performance obtained with a simple model. This way we will only be interest in the mode complex models if they do better than our baseline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH8ELltMrrmg"
      },
      "source": [
        "#### Baseline 1: Naive baseline\n",
        "\n",
        "Our first baseline will follow a very simple rule: predict the series will not change. That means the output will be a repetation of the series value at the last observed time step.\n",
        "\n",
        "This is very simple to implement: our predictions `y_pred` will simply be equal to the last time step in the `X` array.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRxCh_Cfwf9K"
      },
      "source": [
        "\n",
        "##### TODO: write the naive baseline code bellow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0F_BH9uqUTL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7EtFfee3k3H"
      },
      "source": [
        "Here is the first series in the validation set with its predicted value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqrIG3bXVNet"
      },
      "source": [
        "plot_series(X_valid[0, :, 0], y_valid[0, 0], y_pred[0, 0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJJoJTwqsiCr"
      },
      "source": [
        "#### Baseline 2: linear baseline\n",
        "\n",
        "Our second basline model will be a linear regression. We can implement it using keras `Sequential` API by simply using a `Dense` layer with no activation functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO4mpDFUwhiw"
      },
      "source": [
        "\n",
        "##### TODO: write a linear regression model using a Dense layer\n",
        "- Don't forget to use `Flatten` and define the expected input shape using the keyword argument `input_shape`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UoHY76NuIps"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c5GO6XVuJR_"
      },
      "source": [
        "This model has parameters to train: we will call `fit` to learn them.\n",
        "\n",
        "##### TODO: train the linear regression model\n",
        " - Use the Adam optimizer with the default learning rate\n",
        " - Train it it for 20 epochs\n",
        " - Don't forget to include your validation data\n",
        " - Check the learning curves and the performance on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIps0Re0uxeL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUtD75qcxWNj"
      },
      "source": [
        "### Recurrent models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7lXYg0luxzb"
      },
      "source": [
        "#### Model 1: Simple RNN\n",
        "Our first recurrent model will use simple recurrent units. You can implement it using the built-in `keras.layers.SimpleRNN`.\n",
        "\n",
        "Here again we will specify the expected input shape (ignoring the batch dimension). We want the time-steps dimension to be variable: thus we set it to `None`. The sequence element dimension continues to be 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD0qhLwLvfAL"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQrlBTNQvnnj"
      },
      "source": [
        "##### TODO: train the simple RNN model\n",
        " - Use the Adam optimizer with the a learning rate of 1e-5\n",
        " - Train it it for 20 epochs\n",
        " - Don't forget to include your validation data\n",
        " - Check the learning curves and the performance on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNEebRjExJbL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EEjkFk-yKGK"
      },
      "source": [
        "#### Model 2: Deep simple RNN\n",
        "This time we will use multiple recurrent layers in our model. The final output will be computed with a dense layer.\n",
        "\n",
        "##### Chaining two RNN layers\n",
        "Notice that while recurrent layers expect sequence inputs with 3 dimensions (samples, time, element size), they output by default 2 D data as in `[samples, predicted element size]`. \n",
        "\n",
        "If you chain two RNN layers, the first one needs yield sequence-shaped outputs (with the 3 dimensions) so that it is compatible with the second layer. In this case, the layer needs to be declared with the keywork argument `return_sequences=True`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doNNziYp0F6F"
      },
      "source": [
        "##### TODO: Create a Deep RNN with 2 hidden recurrent layers\n",
        "- Use two simple RNN layers with 20 units\n",
        "- Compute the final output doing a linear read-out (use a Dense layer with no activation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32b1oHTQytQ1"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuJzOArd0dzQ"
      },
      "source": [
        "##### TODO: train the Deep RNN model\n",
        " - Use the Adam optimizer with the default learning rate\n",
        " - Train it it for 20 epochs\n",
        " - Don't forget to include your validation data\n",
        " - Check the learning curves and the performance on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RxDc74T0mRL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPpWzXdw1OuQ"
      },
      "source": [
        "### TODO: Final comparison and model choice\n",
        "Compare the compare performances of different models in validation to pick the best model. \n",
        "Apply it to the test set and evaluate it: is the performance close to what you got in the validation set?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0kJDf-a1fNf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7_2OCdW2HHR"
      },
      "source": [
        "## Prediction task 2: forecast multiple time steps\n",
        "\n",
        "This task can be formulated as a sequence-to-sequence task if we predict one output timstep at a time. But we can also formulate it as a sequence-to-vector task, in which we try to predict all time steps at once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0Rpk0d023_f"
      },
      "source": [
        "### Method 1: Predicting 1 step at a time (seq2vec)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfNZx94JFCjK"
      },
      "source": [
        "We'll use the previous model to predict the next 10 values. We first need to regenerate the sequences with 9 more time steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdEYgOTyFCjL"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 10)\n",
        "X_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\n",
        "X_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]\n",
        "X_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPa-XFggVNey"
      },
      "source": [
        "Now let's predict the next 10 values one by one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOYi2JpgVNey"
      },
      "source": [
        "X = X_valid\n",
        "for step_ahead in range(10):\n",
        "    y_pred_one = model.predict(X)[:, np.newaxis, :]\n",
        "    X = np.concatenate([X, y_pred_one], axis=1)\n",
        "\n",
        "Y_pred = X[:, n_steps:, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfaioULUVNez"
      },
      "source": [
        "Y_pred.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxBxROh4VNez"
      },
      "source": [
        "np.mean(keras.metrics.mean_squared_error(Y_valid, Y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98U87VyPETqC"
      },
      "source": [
        "Use this plotting function to visualize your input series and the output predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4omE9niVVNey"
      },
      "source": [
        "def plot_multiple_forecasts(X, Y, Y_pred):\n",
        "    n_steps = X.shape[1]\n",
        "    ahead = Y.shape[1]\n",
        "    plot_series(X[0, :, 0])\n",
        "    plt.plot(np.arange(n_steps, n_steps + ahead), Y[0, :, 0], \"bo-\", label=\"Actual\")\n",
        "    plt.plot(np.arange(n_steps, n_steps + ahead), Y_pred[0, :, 0], \"rx-\", label=\"Forecast\", markersize=10)\n",
        "    plt.axis([0, n_steps + ahead, -1, 1])\n",
        "    plt.legend(fontsize=14)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2H51keT2osf"
      },
      "source": [
        "#### TODO: Baselines: naive and linear prediction\n",
        "\n",
        "Implement the naive and the linear baselines. Compare their performances to the previous RNN model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyuiOUFx4gvf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzFdKKHG_2fn"
      },
      "source": [
        "### Method 2A: Predicting 10 steps at once at the end (seq2vec)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rdtEMSv4y17"
      },
      "source": [
        "#### Model 1: Deep simple RNN with output prediction at the end\n",
        "\n",
        "Now let's create an RNN that predicts all 10 next values at once. To do that, all you need is to change the size of the final `Dense` layer to 10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWRGy4zhVNez"
      },
      "source": [
        "##### TODO: build the new RNN model with output size = 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYn3ggzg41dV"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63luki0pBm6g"
      },
      "source": [
        "##### TODO: train the Deep RNN model\n",
        " - Use the Adam optimizer with the default learning rate\n",
        " - Train it it for 20 epochs\n",
        " - Don't forget to include your validation data\n",
        " - Check the learning curves and the performance on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJgMX2LE9va9"
      },
      "source": [
        "# Training the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-QP6QRpAOTx"
      },
      "source": [
        "### Method 2B: Predicting 10 steps at once at every time step (seq2seq)\n",
        "\n",
        "Now let's create RNN models that predict the next 10 steps at each time step. \n",
        "\n",
        "That is, instead of just forecasting time steps 50 to 59 based on time steps 0 to 49, a model will forecast time steps 1 to 10 at time step 0, then time steps 2 to 11 at time step 1, and so on, and finally it will forecast time steps 50 to 59 at the last time step.\n",
        "\n",
        " Notice that the model is causal: when it makes predictions at any time step, it can only see past time steps.\n",
        "\n",
        "The advantage of this method is that the loss will contain a term for the output of the RNN at each and every time step, not just the output at the last time step. This means there will be many more error gradients flowing through the model, and they won’t have to flow only through time; they will also flow from the output of each time step. This will both stabilize and speed up training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxWM2UTTGnQb"
      },
      "source": [
        "#### Generating appropriate data and prediciton targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jqkemjPVNe0"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 10)\n",
        "X_train = series[:7000, :n_steps]\n",
        "X_valid = series[7000:9000, :n_steps]\n",
        "X_test = series[9000:, :n_steps]\n",
        "Y = np.empty((10000, n_steps, 10))\n",
        "for step_ahead in range(1, 10 + 1):\n",
        "    Y[..., step_ahead - 1] = series[..., step_ahead:step_ahead + n_steps, 0]\n",
        "Y_train = Y[:7000]\n",
        "Y_valid = Y[7000:9000]\n",
        "Y_test = Y[9000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bfUkIg7VNe0"
      },
      "source": [
        "X_train.shape, Y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EKy30KcChpp"
      },
      "source": [
        "#### Custom loss function\n",
        "\n",
        "All outputs are needed during training, but only the output at the last time step is useful for predictions and for evaluation. So although we will rely on the MSE over all the outputs for training, we will use a custom metric for evaluation, to only compute the MSE over the output at the last time step:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5xQ6L8yDFIz"
      },
      "source": [
        "def last_time_step_mse(Y_true, Y_pred):\n",
        "    return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVqbmKMrAOTy"
      },
      "source": [
        "#### Model 2: Deep simple RNN with output prediction at every timestep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbzEv7m6DHiW"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdyAq1bSKLKX"
      },
      "source": [
        "##### TODO: train the Deep RNN model\n",
        " - Use the Adam optimizer with the default learning rate\n",
        " - Train it it for 20 epochs\n",
        " - Don't forget to include your validation data\n",
        " - Check the learning curves and the performance on the validation set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psYQ8rPe43_s"
      },
      "source": [
        "#### Model 3: Deep LSTM with output prediction at every timestep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUjDSmje5ktJ"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KZhGpAjBo5F"
      },
      "source": [
        "##### TODO: train the Deep RNN model\n",
        " - Use the Adam optimizer with the default learning rate\n",
        " - Train it it for 20 epochs\n",
        " - Don't forget to include your validation data\n",
        " - Check the learning curves and the performance on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd_1bYjZ9zns"
      },
      "source": [
        "# Training the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cY2whzZD5l47"
      },
      "source": [
        "#### Model 4: Deep GRU with output prediction at every timestep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b6nqVxO5lX1"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2PXnUNeBptl"
      },
      "source": [
        "##### TODO: train the Deep RNN model\n",
        " - Use the Adam optimizer with the default learning rate\n",
        " - Train it it for 20 epochs\n",
        " - Don't forget to include your validation data\n",
        " - Check the learning curves and the performance on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezffRGX290Qr"
      },
      "source": [
        "# Training the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUm5-9itHWX0"
      },
      "source": [
        "### TODO: Model comparison for prediction task 2\n",
        "\n",
        "Create a summary table with the validation performaces of all models used in this secont task. Choose the best of them and evaluate it on the test set. Did it generalize as well as predicted by the validation score?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UC2w7btHTep"
      },
      "source": [
        "## Extra models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bb3-HpgkIRBJ"
      },
      "source": [
        "#### Generating appropriate data and prediciton targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb_8S82_IRBK"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 10)\n",
        "X_train = series[:7000, :n_steps]\n",
        "X_valid = series[7000:9000, :n_steps]\n",
        "X_test = series[9000:, :n_steps]\n",
        "Y = np.empty((10000, n_steps, 10))\n",
        "for step_ahead in range(1, 10 + 1):\n",
        "    Y[..., step_ahead - 1] = series[..., step_ahead:step_ahead + n_steps, 0]\n",
        "Y_train = Y[:7000]\n",
        "Y_valid = Y[7000:9000]\n",
        "Y_test = Y[9000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUviOPLDIRBL"
      },
      "source": [
        "X_train.shape, Y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfVfbrac8N-F"
      },
      "source": [
        "#### Model 5: 1D convolution + GRUs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ9VYQJ28Nl0"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv1D(filters=20, kernel_size=4, strides=2, padding=\"valid\",\n",
        "                        input_shape=[None, 1]),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3JoEZ5DBrtO"
      },
      "source": [
        "##### TODO: train the Deep RNN model\n",
        " - Use the Adam optimizer with the default learning rate\n",
        " - Train it it for 20 epochs\n",
        " - Don't forget to include your validation data\n",
        " - Check the learning curves and the performance on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dcu6mqzBsfT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Tiilsvt8UD8"
      },
      "source": [
        "#### Model 6: WaveNet-like model\n",
        "![Wavenet diagram](http://benanne.github.io/images/wavenet.png)\n",
        "[From Oord et al., 2016](https://arxiv.org/abs/1609.03499)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_ZlhU-mVNe5"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.InputLayer(input_shape=[None, 1]))\n",
        "for rate in (1, 2, 4, 8) * 2:\n",
        "    model.add(keras.layers.Conv1D(filters=20, kernel_size=2, padding=\"causal\",\n",
        "                                  activation=\"relu\", dilation_rate=rate))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUAOLoDdIIxt"
      },
      "source": [
        "##### TODO: train the WaveNet model\n",
        " - Use the Adam optimizer with the default learning rate\n",
        " - Train it it for 20 epochs\n",
        " - Don't forget to include your validation data\n",
        " - Check the learning curves and the performance on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox-NYduk8Ul2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}