{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LabAssignment3A.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMCh0WL6PXHrl/LKuXjDTPC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dkuPNxcnkrX"
      },
      "source": [
        "# RNN models and advanced keras APIs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbgX6-aMHanl"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUQ1gw_pHyoJ"
      },
      "source": [
        "## Keras functional API\n",
        "*Credts*: This is a section of F. Chollet's tutorial for the Keras API documentation (you can see the full tutorial [here](https://keras.io/guides/functional_api/)).\n",
        "\n",
        "The Keras *functional API* is a way to create models that are more flexible\n",
        "than the `tf.keras.Sequential` API. The functional API can handle models\n",
        "with non-linear topology, shared layers, and even multiple inputs or outputs.\n",
        "\n",
        "The main idea is that a deep learning model is usually\n",
        "a directed acyclic graph (DAG) of layers.\n",
        "So the functional API is a way to build *graphs of layers*.\n",
        "\n",
        "Consider the following model:\n",
        "\n",
        "\n",
        "```\n",
        "(input: 784-dimensional vectors)\n",
        "       ↧\n",
        "[Dense (64 units, relu activation)]\n",
        "       ↧\n",
        "[Dense (64 units, relu activation)]\n",
        "       ↧\n",
        "[Dense (10 units, softmax activation)]\n",
        "       ↧\n",
        "(output: logits of a probability distribution over 10 classes)\n",
        "```\n",
        "\n",
        "This is a basic graph with three layers.\n",
        "To build this model using the functional API, start by creating an input node:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0s_ZRpGHuJo"
      },
      "source": [
        "inputs = keras.Input(shape=(784,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Brk-63qHiLK"
      },
      "source": [
        "The shape of the data is set as a 784-dimensional vector.\n",
        "The batch size is always omitted since only the shape of each sample is specified.\n",
        "\n",
        "If, for example, you have an image input with a shape of `(32, 32, 3)`,\n",
        "you would use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4kMzfEsHiLK"
      },
      "source": [
        "# Just for demonstration purposes.\n",
        "img_inputs = keras.Input(shape=(32, 32, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMCUpLN_HiLL"
      },
      "source": [
        "The `inputs` that is returned contains information about the shape and `dtype`\n",
        "of the input data that you feed to your model.\n",
        "Here's the shape:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2DK-Iz2HiLL"
      },
      "source": [
        "inputs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiQajWwgHiLM"
      },
      "source": [
        "Here's the dtype:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM9IcM3UHiLM"
      },
      "source": [
        "inputs.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jctE3mnHiLM"
      },
      "source": [
        "You create a new node in the graph of layers by calling a layer on this `inputs`\n",
        "object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDPepInUHiLN"
      },
      "source": [
        "dense = layers.Dense(64, activation=\"relu\")\n",
        "x = dense(inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT7EjX1OHiLN"
      },
      "source": [
        "The \"layer call\" action is like drawing an arrow from \"inputs\" to this layer\n",
        "you created.\n",
        "You're \"passing\" the inputs to the `dense` layer, and you get `x` as the output.\n",
        "\n",
        "Let's add a few more layers to the graph of layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iaUMYScHiLN"
      },
      "source": [
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "outputs = layers.Dense(10)(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8UPiAeVHiLN"
      },
      "source": [
        "At this point, you can create a `Model` by specifying its inputs and outputs\n",
        "in the graph of layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd0lPEXCHiLO"
      },
      "source": [
        "model = keras.Model(inputs=inputs, outputs=outputs, name=\"mnist_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKu6AfteHiLO"
      },
      "source": [
        "Let's check out what the model summary looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5SOA3U6HiLO"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaeJFelKHiLO"
      },
      "source": [
        "You can also plot the model as a graph:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkaHwGOWHiLP"
      },
      "source": [
        "keras.utils.plot_model(model, \"my_first_model.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6FTznHxHiLP"
      },
      "source": [
        "And, optionally, display the input and output shapes of each layer\n",
        "in the plotted graph:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A6ZkKnwHiLP"
      },
      "source": [
        "keras.utils.plot_model(model, \"my_first_model_with_shape_info.png\", show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPhba208HiLQ"
      },
      "source": [
        "This figure and the code are almost identical. In the code version,\n",
        "the connection arrows are replaced by the call operation.\n",
        "\n",
        "A \"graph of layers\" is an intuitive mental image for a deep learning model,\n",
        "and the functional API is a way to create models that closely mirrors this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDepGTZZIpFc"
      },
      "source": [
        "## Making new layers and models via subclassing\n",
        "\n",
        "*Credts*: This is a section of F. Chollet's tutorial for the Keras API documentation (you can see the full tutorial [here](https://keras.io/guides/making_new_layers_and_models_via_subclassing/))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbIZ12LUIm4n"
      },
      "source": [
        "### The `Layer` class: the combination of state (weights) and some computation\n",
        "\n",
        "One of the central abstraction in Keras is the `Layer` class. A layer\n",
        "encapsulates both a state (the layer's \"weights\") and a transformation from\n",
        "inputs to outputs (a \"call\", the layer's forward pass).\n",
        "\n",
        "Here's a densely-connected layer. It has a state: the variables `w` and `b`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIRBZaNDIm4n"
      },
      "source": [
        "\n",
        "class Linear(keras.layers.Layer):\n",
        "    def __init__(self, units=32, input_dim=32):\n",
        "        super(Linear, self).__init__()\n",
        "        w_init = tf.random_normal_initializer()\n",
        "        self.w = tf.Variable(\n",
        "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
        "            trainable=True,\n",
        "        )\n",
        "        b_init = tf.zeros_initializer()\n",
        "        self.b = tf.Variable(\n",
        "            initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2mLrLmkIm4o"
      },
      "source": [
        "You would use a layer by calling it on some tensor input(s), much like a Python\n",
        "function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPHWUvQWIm4o"
      },
      "source": [
        "x = tf.ones((2, 2))\n",
        "linear_layer = Linear(4, 2)\n",
        "y = linear_layer(x)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_84ZxO5SIm4q"
      },
      "source": [
        "Note that the weights `w` and `b` are automatically tracked by the layer upon\n",
        "being set as layer attributes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5ubTXjUIm4q"
      },
      "source": [
        "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy5rJ3BtIm4r"
      },
      "source": [
        "Note you also have access to a quicker shortcut for adding weight to a layer:\n",
        "the `add_weight()` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sD-s4csIm4r"
      },
      "source": [
        "\n",
        "class Linear(keras.layers.Layer):\n",
        "    def __init__(self, units=32, input_dim=32):\n",
        "        super(Linear, self).__init__()\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_dim, units), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "        self.b = self.add_weight(shape=(units,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "\n",
        "x = tf.ones((2, 2))\n",
        "linear_layer = Linear(4, 2)\n",
        "y = linear_layer(x)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltjgSfPWIm4z"
      },
      "source": [
        "### The `Model` class\n",
        "\n",
        "In general, you will use the `Layer` class to define inner computation blocks,\n",
        "and will use the `Model` class to define the outer model -- the object you\n",
        "will train.\n",
        "\n",
        "For instance, in a ResNet50 model, you would have several ResNet blocks\n",
        "subclassing `Layer`, and a single `Model` encompassing the entire ResNet50\n",
        "network.\n",
        "\n",
        "The `Model` class has the same API as `Layer`, with the following differences:\n",
        "\n",
        "- It exposes built-in training, evaluation, and prediction loops\n",
        "(`model.fit()`, `model.evaluate()`, `model.predict()`).\n",
        "- It exposes the list of its inner layers, via the `model.layers` property.\n",
        "- It exposes saving and serialization APIs (`save()`, `save_weights()`...)\n",
        "\n",
        "Effectively, the `Layer` class corresponds to what we refer to in the\n",
        "literature as a \"layer\" (as in \"convolution layer\" or \"recurrent layer\") or as\n",
        "a \"block\" (as in \"ResNet block\" or \"Inception block\").\n",
        "\n",
        "Meanwhile, the `Model` class corresponds to what is referred to in the\n",
        "literature as a \"model\" (as in \"deep learning model\") or as a \"network\" (as in\n",
        "\"deep neural network\").\n",
        "\n",
        "So if you're wondering, \"should I use the `Layer` class or the `Model` class?\",\n",
        "ask yourself: will I need to call `fit()` on it? Will I need to call `save()`\n",
        "on it? If so, go with `Model`. If not (either because your class is just a block\n",
        "in a bigger system, or because you are writing training & saving code yourself),\n",
        "use `Layer`.\n",
        "\n",
        "For instance, we could take our mini-resnet example above, and use it to build\n",
        "a `Model` that we could train with `fit()`, and that we could save with\n",
        "`save_weights()`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIjbsIisIm4z"
      },
      "source": [
        "```python\n",
        "class ResNet(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.block_1 = ResNetBlock()\n",
        "        self.block_2 = ResNetBlock()\n",
        "        self.global_pool = layers.GlobalAveragePooling2D()\n",
        "        self.classifier = Dense(num_classes)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.block_1(inputs)\n",
        "        x = self.block_2(x)\n",
        "        x = self.global_pool(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "resnet = ResNet()\n",
        "dataset = ...\n",
        "resnet.fit(dataset, epochs=10)\n",
        "resnet.save(filepath)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akGNd6waKTlp"
      },
      "source": [
        "## Exercise: coding a recurrent model\n",
        "\n",
        "Using the functional and sub-classing APIs, code an LSTM or a GRU based on the equations seen in class.\n",
        "\n",
        "As an example, here is such code for a simple recurrent unit.\n",
        "\n",
        "![Simple Recurrent Network unfolded](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Recurrent_neural_network_unfold.svg/800px-Recurrent_neural_network_unfold.svg.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvOebPPjKc6V"
      },
      "source": [
        "class SimpleRNN(tf.keras.Model):\n",
        "    def __init__(self, n_units, n_outputs):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.layer_U = keras.layers.Dense(n_units)\n",
        "        self.layer_V = keras.layers.Dense(n_units)\n",
        "        self.layer_W = keras.layers.Dense(n_outputs, activation='tanh')\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        # Inputs are expected to have shape\n",
        "        # [batch size, time steps, sequence element size]\n",
        "\n",
        "        # First we compute the initial hidden state\n",
        "        h = x = self.layer_U(inputs[:, 0, :])\n",
        "        # Then we iterate over the time steps dimension\n",
        "        for i in range(1, inputs.shape[1]):\n",
        "            x = self.layer_U(inputs[:, i, :])\n",
        "            h = self.layer_V(h) + x\n",
        "        # Now we finish applying the output layer\n",
        "        return self.layer_W(h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR4QqhoDihkx"
      },
      "source": [
        "### Generating sample data\n",
        "\n",
        "To test our new model we'll use some randomly generated univariate time series that we'll try to model.\n",
        "\n",
        "*Credits:* Data generation code by [A. Géron](https://colab.research.google.com/github/ageron/handson-ml2/blob/master/15_processing_sequences_using_rnns_and_cnns.ipynb#scrollTo=AiINDLJHVNep)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_bQfrh_PGdz"
      },
      "source": [
        "def generate_time_series(batch_size, n_steps):\n",
        "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
        "    time = np.linspace(0, 1, n_steps)\n",
        "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  #   wave 1\n",
        "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2\n",
        "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # + noise\n",
        "    return series[..., np.newaxis].astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSZCqVimq9Sz"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhwaZ-Suv8Wj"
      },
      "source": [
        "Notice the shape of the series array:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj6Fw0yjrJ4_"
      },
      "source": [
        "series.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd9b9zf7p9Xt"
      },
      "source": [
        "The meaning of each dimension is `[samples, time steps, sequence element size]`. \n",
        "Here each series element is a single scalar because we are making a univariate prediction.\n",
        "If we were modeling mutiple time series at once (eg. temperature and humidity) this would be a multi-variate prediction and our sequence element size would be 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWUj5-Zaqi1D"
      },
      "source": [
        "#### Creating predicion targets\n",
        "We want to predict one step further into the series. To do so , we can take the last time step of a series as the regression target, while using all the previous time steps as inputs.\n",
        "\n",
        "We also want to split our data into train, validation and test sets as usual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjSOnWaRVNer"
      },
      "source": [
        "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
        "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
        "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YItC9egeVNer"
      },
      "source": [
        "X_train.shape, y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0miv2Q44lGiq"
      },
      "source": [
        "### Checking the model runs appropriately\n",
        "Here we instantiate a model from our custom class. We will use a single hidden unit, and the predict a single output.\n",
        "\n",
        "Call `build` to inform keras of the expected input shape: it will allow the initialization of the parameters that depend on it.\n",
        "\n",
        "After model build, it is possible to visualize the model with `summary` and `plot model`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY0-0m6eQ-eF"
      },
      "source": [
        "model = SimpleRNN(n_units=1, n_outputs=1)\n",
        "model.build(input_shape=(X_train.shape))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WYOpeoZlrSC"
      },
      "source": [
        "Let's run a single training epoch just to make sure everything is running as it should."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVokfM27UMEO"
      },
      "source": [
        "model.compile(loss='mse')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryP0m9HHTOs9"
      },
      "source": [
        "model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}